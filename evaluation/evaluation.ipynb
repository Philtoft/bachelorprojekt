{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import QG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get parent directory\n",
    "notebook_path = os.path.abspath(\"evaluation.ipynb\")\n",
    "parent_dir = os.path.dirname(os.path.dirname(notebook_path))\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': \"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\", 'questions': ['Who created Python?', 'When was Python first released?', \"What is Python's design philosophy?\"]}\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from models.qg import QG\n",
    "\n",
    "context = \"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\"\n",
    "\n",
    "qg = QG(model=\"valhalla/t5-base-e2e-qg\", tokenizer=\"t5-base\")\n",
    "\n",
    "contexts_questions = []\n",
    "\n",
    "result = qg(context)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Squad V2 and compare the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets -q\n",
    "%pip install pandas -q\n",
    "%pip install evaluate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad_v2 (/Users/philiphyltoft/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n",
      "100%|██████████| 2/2 [00:00<00:00, 290.48it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'qg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mreferences\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mreferences\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: [question \u001b[39mfor\u001b[39;00m question \u001b[39min\u001b[39;00m x])\n\u001b[1;32m     25\u001b[0m \u001b[39m# Run QG on each context and insert into seperate column\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m# df['generated_questions'] = df['context'].apply(lambda x: qg(x)['questions'])\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mpredictions\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mcontext\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: [question \u001b[39mfor\u001b[39;49;00m question \u001b[39min\u001b[39;49;00m qg(x)[\u001b[39m'\u001b[39;49m\u001b[39mquestions\u001b[39;49m\u001b[39m'\u001b[39;49m]])\n\u001b[1;32m     29\u001b[0m \u001b[39m# Drop context column\u001b[39;00m\n\u001b[1;32m     30\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/hg_training/lib/python3.10/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/miniconda3/envs/hg_training/lib/python3.10/site-packages/pandas/core/apply.py:1105\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1104\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1105\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/miniconda3/envs/hg_training/lib/python3.10/site-packages/pandas/core/apply.py:1156\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1155\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1156\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1157\u001b[0m             values,\n\u001b[1;32m   1158\u001b[0m             f,\n\u001b[1;32m   1159\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1160\u001b[0m         )\n\u001b[1;32m   1162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1163\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1164\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/miniconda3/envs/hg_training/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2918\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     23\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mreferences\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mreferences\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: [question \u001b[39mfor\u001b[39;00m question \u001b[39min\u001b[39;00m x])\n\u001b[1;32m     25\u001b[0m \u001b[39m# Run QG on each context and insert into seperate column\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m# df['generated_questions'] = df['context'].apply(lambda x: qg(x)['questions'])\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mpredictions\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: [question \u001b[39mfor\u001b[39;00m question \u001b[39min\u001b[39;00m qg(x)[\u001b[39m'\u001b[39m\u001b[39mquestions\u001b[39m\u001b[39m'\u001b[39m]])\n\u001b[1;32m     29\u001b[0m \u001b[39m# Drop context column\u001b[39;00m\n\u001b[1;32m     30\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qg' is not defined"
     ]
    }
   ],
   "source": [
    "# It needs \n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "dataset = load_dataset(\"squad_v2\")[\"train\"].select(range(10))\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "\n",
    "df = df[[\"context\", \"question\"]]\n",
    "\n",
    "df = df.groupby(\"context\")['question'].apply(list).reset_index()\n",
    "\n",
    "# remove '\\n' and spaces from the context column\n",
    "# df['context'] = df['context'].apply(lambda x: x.replace('\\n', ' ').strip())\n",
    "df['context'] = df['context'].apply(lambda x: x.replace('\\n', ' ').strip())\n",
    "\n",
    "# Change column name from question to questions\n",
    "df.rename(columns={'question': 'references'}, inplace=True)\n",
    "\n",
    "# Split questions in validation questions column\n",
    "df['references'] = df['references'].apply(lambda x: [question for question in x])\n",
    "\n",
    "# Run QG on each context and insert into seperate column\n",
    "# df['generated_questions'] = df['context'].apply(lambda x: qg(x)['questions'])\n",
    "df['predictions'] = df['context'].apply(lambda x: [question for question in qg(x)['questions']])\n",
    "\n",
    "# Drop context column\n",
    "df = df.drop(columns=[\"context\"])\n",
    "\n",
    "result = df.to_dict(orient=\"records\")\n",
    "\n",
    "with open(\"output.json\", \"w\") as outfile:\n",
    "    json.dump(result, outfile, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run QGAR through each context and make it output questions.\n",
    "2. Compare the questions QGAR generates to the actually created questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Run BLEU on generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk -q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources of Inspiration:\n",
    "- [BLEU score in Python - Beginners Overview](https://www.askpython.com/python/bleu-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is', 'a', 'dog'], ['it', 'is', 'dog'], ['dog', 'it', 'is'], ['a', 'dog,', 'it', 'is']]\n",
      "BLEU score for test -> 1.2213386697554703e-77\n",
      "BLUE score for test2 -> 1.384292958842266e-231\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ref = [\n",
    "    'this is a dog'.split(),\n",
    "    'it is dog'.split(),\n",
    "    'dog it is'.split(),\n",
    "    'a dog, it is'.split() \n",
    "]\n",
    "\n",
    "print(ref)\n",
    "\n",
    "candidate = 'it is dog'.split()\n",
    "print('BLEU score for test -> {}'.format(sentence_bleu(ref, candidate)))\n",
    "test2 = 'what is dog?'.split()\n",
    "print('BLUE score for test2 -> {}'.format(sentence_bleu(ref, test2)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU implementation on QG's output\n",
    "It must for each context loop through each question generated and compare it to the expected output. But it has to split each sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLUE score is: 0.4770199193643617\n",
      "BLUE score is: 0.5630512950672975\n",
      "BLUE score is: 0.6208612150735205\n",
      "BLUE score is: 0.5849178413607281\n",
      "BLUE score is: 0.5875203180024213\n",
      "BLUE score is: 0.6272931243309239\n",
      "BLUE score is: 0.4659999870476353\n",
      "BLUE score is: 0.6650520588666322\n",
      "BLUE score is: 0.8433024381672091\n",
      "BLUE score is: 0.7319448444256154\n",
      "Average BLUE score is: 0.6166963041706344\n"
     ]
    }
   ],
   "source": [
    "# Loop through each context\n",
    "with open(\"output.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for context in data:\n",
    "    avg = 0\n",
    "    for validation_question in context['references']:\n",
    "        print('BLUE score is: {}'.format(sentence_bleu(context['predictions'], validation_question)))\n",
    "        avg += sentence_bleu(context['predictions'], validation_question)\n",
    "\n",
    "    print('Average BLUE score is: {}'.format(avg / len(context['references'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Beyonce was born on what date?', 'What is the name of the singer, songwriter, record producer, and actress?', 'Where was Beyoncé born and raised?', \"In what decade did she rise to fame as the lead singer of Destiny's Child?\", 'How many Grammy Awards did her debut album earn?']\n",
      "['When did Beyonce start becoming popular?', 'What areas did Beyonce compete in when she was growing up?', \"When did Beyonce leave Destiny's Child and become a solo singer?\", 'In what city and state did Beyonce  grow up? ', 'In which decade did Beyonce become famous?', 'In what R&B group was she the lead singer?', 'What album made her a worldwide known artist?', \"Who managed the Destiny's Child group?\", 'When did Beyoncé rise to fame?', \"What role did Beyoncé have in Destiny's Child?\"]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Predictions and/or references don't match the expected format.\nExpected format:\nFeature option 0: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}\nFeature option 1: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')},\nInput predictions: [['Beyonce was born on what date?', 'What is the name of the singer, songwriter, record producer, and actress?', 'Where was Beyoncé born and raised?', \"In what decade did she rise to fame as the lead singer of Destiny's Child?\", 'How many Grammy Awards did her debut album earn?']],\nInput references: [['When did Beyonce start becoming popular?', 'What areas did Beyonce compete in when she was growing up?', \"When did Beyonce leave Destiny's Child and become a solo singer?\", 'In what city and state did Beyonce  grow up? ', 'In which decade did Beyonce become famous?', 'In what R&B group was she the lead singer?', 'What album made her a worldwide known artist?', \"Who managed the Destiny's Child group?\", 'When did Beyoncé rise to fame?', \"What role did Beyoncé have in Destiny's Child?\"]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m         references\u001b[39m.\u001b[39mappend(output[\u001b[39m'\u001b[39m\u001b[39mreferences\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     16\u001b[0m bleu \u001b[39m=\u001b[39m evaluate\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mbleu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m results \u001b[39m=\u001b[39m bleu\u001b[39m.\u001b[39;49mcompute(predictions\u001b[39m=\u001b[39;49m[predictions], references\u001b[39m=\u001b[39;49m[references])\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(results)\n",
      "File \u001b[0;32m~/miniconda3/envs/hg_training/lib/python3.10/site-packages/evaluate/module.py:432\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m compute_kwargs \u001b[39m=\u001b[39m {k: kwargs[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m kwargs \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feature_names()}\n\u001b[1;32m    431\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m--> 432\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_batch(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m    433\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_finalize()\n\u001b[1;32m    435\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_file_name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hg_training/lib/python3.10/site-packages/evaluate/module.py:480\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m batch \u001b[39m=\u001b[39m {input_name: batch[input_name] \u001b[39mfor\u001b[39;00m input_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feature_names()}\n\u001b[1;32m    479\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 480\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselected_feature_format \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_infer_feature_from_batch(batch)\n\u001b[1;32m    481\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_writer()\n\u001b[1;32m    482\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/hg_training/lib/python3.10/site-packages/evaluate/module.py:552\u001b[0m, in \u001b[0;36mEvaluationModule._infer_feature_from_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    551\u001b[0m     example \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m([(k, v[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems()])\n\u001b[0;32m--> 552\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_infer_feature_from_example(example)\n",
      "File \u001b[0;32m~/miniconda3/envs/hg_training/lib/python3.10/site-packages/evaluate/module.py:572\u001b[0m, in \u001b[0;36mEvaluationModule._infer_feature_from_example\u001b[0;34m(self, example)\u001b[0m\n\u001b[1;32m    565\u001b[0m feature_strings \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFeature option \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mfeature\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m i, feature \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures)])\n\u001b[1;32m    566\u001b[0m error_msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    567\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPredictions and/or references don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt match the expected format.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected format:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfeature_strings\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput predictions: \u001b[39m\u001b[39m{\u001b[39;00msummarize_if_long_list(example[\u001b[39m'\u001b[39m\u001b[39mpredictions\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput references: \u001b[39m\u001b[39m{\u001b[39;00msummarize_if_long_list(example[\u001b[39m'\u001b[39m\u001b[39mreferences\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m )\n\u001b[0;32m--> 572\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(error_msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Predictions and/or references don't match the expected format.\nExpected format:\nFeature option 0: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}\nFeature option 1: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')},\nInput predictions: [['Beyonce was born on what date?', 'What is the name of the singer, songwriter, record producer, and actress?', 'Where was Beyoncé born and raised?', \"In what decade did she rise to fame as the lead singer of Destiny's Child?\", 'How many Grammy Awards did her debut album earn?']],\nInput references: [['When did Beyonce start becoming popular?', 'What areas did Beyonce compete in when she was growing up?', \"When did Beyonce leave Destiny's Child and become a solo singer?\", 'In what city and state did Beyonce  grow up? ', 'In which decade did Beyonce become famous?', 'In what R&B group was she the lead singer?', 'What album made her a worldwide known artist?', \"Who managed the Destiny's Child group?\", 'When did Beyoncé rise to fame?', \"What role did Beyoncé have in Destiny's Child?\"]]"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import json\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "with open(\"output.json\", \"r\") as outputs:\n",
    "    outputs = json.load(outputs)\n",
    "    for output in outputs:\n",
    "        print(output['predictions'])\n",
    "        print(output['references'])\n",
    "        # for item in output['predictions']:\n",
    "        predictions.append(output['predictions'])\n",
    "        references.append(output['references'])\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. HuggingFace Evaluate library"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use example with 2 metrics: [example link](https://huggingface.co/spaces/evaluate-metric/bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.0, 'precisions': [0.6666666666666666, 0.5714285714285714, 0.4, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.5, 'translation_length': 9, 'reference_length': 6}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
    "references = [[\"hello there general kenobi\", \"hello there !\"],[\"foo bar foobar\"]]\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'validation_questions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39moutput.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m output:\n\u001b[1;32m      4\u001b[0m     output1 \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(output)[\u001b[39m0\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m     \u001b[39mprint\u001b[39m(output1[\u001b[39m'\u001b[39;49m\u001b[39mvalidation_questions\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'validation_questions'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"output.json\", \"r\") as output:\n",
    "    output1 = json.load(output)[0]\n",
    "    print(output1['references'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hg_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
