IAI notes. IAI notes. : 1. Introduction. : Summary. This chapter defines AI and establishes the cultural background against which it has developed. Some of the important points are as follows: Different people approach AI with different goals in mind. Two important questions to. ask are: Are you concerned with thinking or behavior? Do you want to model humans. or work from an ideal standard: In this book, we adopt the view that intelligence is concerned mainly with rational. action. Ideally, an intelligent agent takes the best possible action in a situation. We. study the problem of building agents that are intelligent in this sense. Philosophers (going back to 400 B.C.) made AI conceivable by considering the ideas. that the mind is in some ways like a machine, that it operates on knowledge encoded in. some internal language, and that thought can be used to choose what actions to take. Mathematicians provided the tools to manipulate statements of logical certainty as well. as uncertain, probabilistic statements. They also set the groundwork for understanding. computation and reasoning about algorithms. Economists formalized the problem of making decisions that maximize the expected. outcome to the decision maker. Neuroscientists discovered some facts about how the brain works and the ways in which it is similar to and different from computers. Psychologists adopted the idea that humans and animals can be considered information processing machines. Linguists showed that language use fits into this model. Computer engineers provided the ever-more-powerful machines that make AI applications possible. Control theory deals with designing devices that act optimally on the basis of feedback. from the environment. Initially, the mathematical tools of control theory were quite. different from AI, but the fields are coming closer together. The history of AI has had cycles of success, misplaced optimism, and resulting cutbacks. in enthusiasm and funding. There have also been cycles of introducing new creative. approaches and systematically refining the best ones. AI has advanced more rapidly in the past decade because of greater use of the scientific. method in experimenting with and comparing approaches. Recent progress in understanding the theoretical basis for intelligence has gone hand in. hand with improvements in the capabilities of real systems. The subfields of AI have. become more integrated, and AI has found common ground with other disciplines. 1.1 What is AI. 1.1.1 Acting humanly: The Turing Test approach. Requirements for passing the Turing test: natural language processing. to enable it to communicate successfully in English;. knowledge representation. to store what it knows or hears;. automated reasoning. to use the stored information to answer questions and to draw. new conclusions;. machine learning. to adapt to new circumstances and to detect and extrapolate patterns. The total Turing test: this form of the test also includes the subject to handle visual input and physical objects given by the interrogator. The computer would need: computer vision. to perceive objects. robotics. to manipulate objects and move about. 1.1.2 Thinking humanly: The cognitive modeling approach. 1.1.3 Thinking rationally: The “laws of thought” approach. 1.1.4 Acting rationally: The rational agent approach. A rational agent is one that acts so as to achieve the. best outcome or, when there is uncertainty, the best expected outcome. The rational-agent approach has two advantages over the other approaches. First, it. is more general than the “laws of thought” approach because correct inference is just one of several possible mechanisms for achieving rationality. Second, it is more amenable to scientific development than are approaches based on human behavior or human thought. 1.2 The foundations of AI. 1.2.1 Philosophy. The final element in the philosophical picture of the mind is the connection between. knowledge and action. This question is vital to AI because intelligence requires action as well as reasoning. Moreover, only by understanding how actions are justified can we understand how to build an agent whose actions are justifiable (or rational). 1.2.2 Mathematics. 1.2.3 Economics. 1.2.4 Neuroscience. 1.2.5 Psychology. 1.2.6 Computer engineering. 1.2.7 Control theory and cybernetics. 1.2.8 Linguistics. 1.3 The history of AI. 1.3.1 The gestation of artificial intelligence (1943–1955). 1.3.2 The birth of artificial intelligence (1956). 1.3.3 Early enthusiasm, great expectations (1952–1969). 1.3.4 A dose of reality (1966–1973). 1.3.5 Knowledge-based systems: The key to power? (1969–1979). 1.3.6 AI becomes an industry (1980–present). 1.3.7 The return of neural networks (1986–present). 1.3.8 AI adopts the scientific method (1987–present). 1.3.9 The emergence of intelligent agents (1995–present). 1.3.10 The availability of very large data sets (2001–present). 1.4 The state of the art. 2 Intelligent Agents. : Summary. An. agent. is something that perceives and acts in an environment. The agent function. for an agent specifies the action taken by the agent in response to any percept sequence. The. performance measure. evaluates the behavior of the agent in an environment. A. rational agent acts so as to maximize the expected value of the performance measure,. given the percept sequence it has seen so far. A. task environment specification. includes the performance measure, the external environment, the actuators, and the sensors. In designing an agent, the first step must. always be to specify the task environment as fully as possible. Task environments. vary along several significant dimensions. They can be fully or. partially observable, single-agent or multiagent, deterministic or stochastic, episodic or. sequential, static or dynamic, discrete or continuous, and known or unknown. The agent program. implements the agent function. There exists a variety of basic. agent-program designs reflecting the kind of information made explicit and used in the. decision process. The designs vary in efficiency, compactness, and flexibility. The. appropriate design of the agent program depends on the nature of the environment. Simple reflex agents. respond directly to percepts, whereas. model-based reflex agents. maintain internal state to track aspects of the world that are not evident in the current. percept. Goal-based agents. act to achieve their goals, and. utility-based agents. try to. maximize their own expected “happiness.”. All agents can improve their performance through learning. 2.1 Agents and Environments. An agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators. percept sequence. : the history of everything an agent has perceived(memories). agent function: An action mapped from a given input(percept) which result in a given action. 2.2 Good behaviour: the concept of rationality. A rational agent is one that does the right thing. Put in an environment an agent perceives it and creates a sequence of actions, if the actions are good as a whole then it has performed well. a performance measure: Evaluation of the desirability of a sequence of environment states. As a general rule, it is better to design performance measures according to what one actually wants in the environment, rather than according to how one thinks the agent should behave. 2.2.1 Rationality. What is rational at any given time depends on four things: • The performance measure that defines the criterion of success. • The agent’s prior knowledge of the environment. • The actions that the agent can perform. • The agent’s percept sequence to date. definition of a rational agent: For each possible percept sequence, a rational agent should select an action that is expected to maximize its performance measure, given the evidence provided by the percept sequence and whatever built-in knowledge the agent has. An agent can be called rational when its expected performance is at least as high as any other agent’s, meaning it already has optimal efficiency for its environment. 2.2.2 Omniscience, learning and autonomy. An omniscient agent knows the actual outcome of its actions and can act accordingly; but omniscience is impossible in reality. Rationality maximizes expected performance, while perfection maximizes actual performance. information gathering. : Doing actions in order to modify future percepts - essentially spending time to learn and improve its future performance. A priori: en environment which is completely known. Autonomous. : Relies on its own percepts rather than the prior knowledge it is given - it thinks and reacts on its own. 2.3.1 specifying the task environment. Task environment(aka. PEAS (Performance, Environment, Actuators, Sensors)): The agent’s actuators and sensors. PEAS description: a description of the environment, agent actuators, performance measurement and sensors of the agent and its acting environment: Performance measures: Criterias for a rational and well performing agent. Environment: the environment(s) in which the agent will operate. Actuators: Actions available to the agent. Sensors: Ways for the agent to perceive its environment. 2.3.2 Properties of task environments. Fully observable. : The agent’s sensors gives it knowledge of the whole environment at all times and detects all relevant aspects for the choice of action. Agents need not keep track of its perceived environment as memory, since it is always available. Partially observable. : The agent’s sensors gives it perception of a limited area at a time, fx. a roomba can only see right in front of it. Multiagent: Multiple agents affect each other in a given environment - often requires communication between agents for rational behaviour. Single agent: Only a single agent affects an environment, it is not affected by other agents. Agents can be competitive or cooperative, eg. chess AI or multiple Roombas in a room. Randomized behaviour is rational since is avoids predictability which can be negative. Deterministic: The next state of the environment is completely determined by the current state and the action executed by the agent. Stochastic: The next state of the environment is not determined by state and executed action of the agent. Uncertain environment: not fully observable or not deterministic environment. nondeterministic environment: Environments where actions are characterized by their possible outcomes, but no probabilities are attached to them. Episodic environment: Agents experience is divided into atomic episodes. In each episode the agent receives a percept and then performs a single action. the next episode does not depend on the actions taken in previous episodes. Sequential environment: the agent’s current decision could affect all future decisions. Static environment: The environment is stable and does not change while the agent is deliberating its action. Dynamic environment: The environment may change while the agent is deliberating. Discrete environment: The time is based upon steps, like a turn based game. Continuous environment: Time is continuous and not affected by amount of actions. Known/unknown environment: The agents knowledge of the laws of the environment such as physics. If it is unknown the agent must learn it by itself. 2.4.1 Agent programs. 2.4.2 Simple reflex agents. These agents select actions on the basis of the current percept, ignoring the rest of the percept history. This type of agent is ruled by. condition–action rules: essentially if statements - If X happens then do Y. Infinite loops are often unavoidable for simple reflex agents operating in partially observable environments. Escape from infinite loops is possible if the agent can randomize its actions. In single-agent environments, however, randomization is usually not rational. 2.4.3 Model-based reflex agents. A Model-based agent. : an agent that uses a model of the world(knowledge about how the environment works, such as traffic rules fx.). The most effective way to handle partial observability is for the agent to keep track of the part of the world it can’t see now. That is, the agent should maintain some sort of. internal state. that depends on the percept history and thereby reflects at least some of the unobserved aspects of the current state. 2.4.4 Goal-based agents. the agent needs some sort of goal information that describes. situations that are desirable - fx. getting to a destination or an environmental state. Sometimes goal-based action selection is straightforward—for example, when goal satisfaction results immediately from a single action. Sometimes it will be more tricky—for example, when the agent has to consider long sequences of twists and turns in order to find a way to achieve the goal. Search. and. planning. are subfields of AI devoted to finding action sequences that achieve the agent’s goals. Although the goal-based agent appears less efficient, it is more flexible because the. knowledge that supports its decisions is represented explicitly and can be modified. If it starts to rain, the agent can update its knowledge of how effectively its brakes will operate; this will automatically cause all of the relevant behaviors to be altered to suit the new conditions. 2.4.5 Utility-based agents. Utility: how efficient and well a goal is accomplished, eg. quickly, well, safely, cheap etc. utility function: An internalization of the performance measure. If the internal utility function and the external performance measure are in agreement, then an agent that chooses actions to maximize its utility will be rational according to the external performance measure. utility provides a way in which the likelihood of success can be weighed against the importance of the goals. a rational utility-based agent chooses the action that maximizes the expected utility of the action outcomes - that is, the utility the agent expects to derive, on average, given the probabilities and utilities of each outcome. any rational agent must behave as if it possesses a utility function whose expected value it tries to maximize. 2.4.6 Learning agents. Learning allows the agent to operate in initially unknown environments and to become more competent than its initial knowledge alone might allow. learning element: responsible for making improvements. uses feedback from the. critic. on how the agent is doing and determines how the performance element should be modified to do better in the future. performance element: responsible for selecting external actions. The critic: tells the learning element how well the agent is doing with respect to a fixed. performance standard. The critic is necessary because the percepts themselves provide no indication of the agent’s success. problem generator: responsible for suggesting actions that will lead to new and informative experiences - if the agent is willing to explore a little and do some perhaps suboptimal actions in the short run, it might discover much better actions for the long run. 2.4.7 How the components of agent programs work. the axis along which atomic, factored, and structured representations lie is the axis of increasing expressiveness. A more expressive representation can capture, at least as concisely, everything a less expressive one can capture, plus some more. atomic representation: each state of the world is indivisible with no internal structure. factored representation: splits up each state into a fixed set of variables or attributes each of which can have a value. two different factored states can share some attributes - like being in the same GPS location - and not others states - such as fuel level. this makes it much easier to work out how to turn one state into another. With factored representations, we can also represent uncertainty. structured representation: Objects and their various and varying relationships can be described explicitly. 3 Solving problems by searching. : Summary. Search: Before an agent can start searching for solutions, a goal must be identified and a well defined problem must be formulated. A problem consists of five parts: the initial state. a set of actions. a transition model describing the results of those actions,. a goal test function,. a path cost function. The environment of the problem is represented by a state space. A path through the. state space from the initial state to a goal state is a solution. Search algorithms treat states and actions as atomic: they do not consider any internal. structure they might possess. A general TREE-SEARCH algorithm considers all possible paths to find a solution,. whereas a GRAPH-SEARCH algorithm avoids consideration of redundant paths. Search algorithms are judged on the basis of completeness, optimality, time complexity, and space complexity. Complexity depends on b, the branching factor in the state. space, and d, the depth of the shallowest solution. Uninformed search methods have access only to the problem definition. The basic. algorithms are as follows: Breadth-first search expands the shallowest nodes first; it is complete, optimal. for unit step costs, but has exponential space complexity. Uniform-cost search expands the node with lowest path cost, g(n), and is optimal. for general step costs. Depth-first search expands the deepest unexpanded node first. It is neither complete nor optimal, but has linear space complexity. Depth-limited search adds a. depth bound. Iterative deepening search calls depth-first search with increasing depth limits. until a goal is found. It is complete, optimal for unit step costs, has time complexity. comparable to breadth-first search, and has linear space complexity. Bidirectional search can enormously reduce time complexity, but it is not always. applicable and may require too much space. Informed search methods may have access to a heuristic function h(n) that estimates. the cost of a solution from n. The generic best-first search algorithm selects a node for expansion according to. an evaluation function. Greedy best-first search expands nodes with minimal h(n). It is not optimal but. is often efficient. A∗ search expands nodes with minimal f(n) = g(n) + h(n). A∗ is complete and. optimal, provided that h(n) is admissible (for TREE-SEARCH) or consistent (for. GRAPH-SEARCH). The space complexity of A∗ is still prohibitive. RBFS (recursive best-first search) and SMA∗ (simplified memory-bounded A∗). are robust, optimal search algorithms that use limited amounts of memory; given. enough time, they can solve problems that A∗ cannot solve because it runs out of. memory. The performance of heuristic search algorithms depends on the quality of the heuristic. function. One can sometimes construct good heuristics by relaxing the problem definition, by storing precomputed solution costs for subproblems in a pattern database, or by learning from experience with the problem class. Lecture Notes. Discrete states and actions: They are finite and countable, fx. 10 possible actions in a given state. State space: The initial state, a set of actions, and the transition model makes up the state space, essentially the route from a to b. A solution is a path fromt he initial state s0 to the goal state g. An optimum solution is the path with minimal path cost. Difference between graph and tree search: Graph search has a set of explored nodes. Nodes are added to the set right before they are expanded in the graph. If a any children of the node are not in the explored or frontier set, then ass it to nthe frontier. Big oh notation: Informed search: Best-first tree search: Maintain a queue in ascending f-value(lowest cost edge) for each node in the frontier. Best-first graph search: same as BFTS above + some more. Maintain a set of explored nodes and add as nodes are expanded. When expanding a node, add all children NOT IN Explored set, to the frontier set. Greedy best first search: Looks at the heuristic distance between the nodes in the frontier and the goal - aka. whatever is closest to the goal like A*. It then expands the closest node until the goal is reached. This is not optimal but it is quick. Also risks getting stuck on a dead end graph where there is a node close to the goal, but not connected to it. A*. Considers both heuristic distance from nodes to goal as well as the cost of reaching said nodes. 3.1 Problem-Solving Agents. Problem formulation. is the process of deciding what actions and states to consider, given a goal. In general, an agent with several immediate options of unknown value can decide what to do by first examining future actions that eventually lead to states of known value. A. search. algorithm takes a problem as input and returns a solution in the form of an action. sequence. Once a solution is found, the actions it recommends can be carried out. This is called the execution phase. While the agent is executing the solution sequence is in a. open-loop system. - meaning it ignores its percepts when choosing an action because it knows in advance what they will be and breaks the loop between agent and environment. 3.1.1 Well-defined problems and solutions. A problem can be defined formally by five components: The initial state. that the agent starts in. A. description of the possible actions available to the agent. Given a particular state s, ACTIONS(s) returns the set of actions that can be executed in s. We say that each of these actions is applicable in s. A. description of what each action does. ; the formal name for this is the transition. model, specified by a function RESULT(s, a) that returns the state that results from. doing action a in state s. We also use the term successor to refer to any state reachable from a given state by a single action. The. goal test. , which determines whether a given state is a goal state. Sometimes there is an explicit set of possible goal states, and the test simply checks whether the given state is one of them. A. path cost function. that assigns a numeric cost to each path. The problem-solving. agent chooses a cost function that reflects its own performance measure. state space. : defined by the initial state, actions, and transition model. A solution to a problem is an action sequence that leads from the initial state to a goal state. Solution quality is measured by the path cost function, and an optimal solution has the lowest path cost among all solutions. 3.1.2 Formulating problems. Abstraction: The process of removing detail from a representation. The choice of a good abstraction thus involves removing as much detail as possible while retaining validity and ensuring that the abstract actions are easy to carry out. Were it not for the ability to construct useful abstractions, intelligent agents would. be completely swamped by the real world. 3.2 Example Problems. toy problem: intended to illustrate or exercise various problem-solving methods. real-world problem: one whose solutions people care about. 3.2.1 Toy problems. 3.2.2 Real-World problems. 3.3 Searching for solutions. search algorithms work by considering various possible action sequences. The possible action sequences starting at the initial state form a. search tree. with the initial state at the root; the branches are actions and the nodes correspond to states in the state space of the problem. expanding the current state: applying each legal action to the current state, thereby generating a new set of states. Frontier: the set of available leaf nodes in the tree at a given time. 3.3.1 Infrastructure for search algorithms. Search algorithms require a data structure to keep track of the search tree that is being constructed. For each node n of the tree, we have a structure that contains four components: n.STATE: the state in the state space to which the node corresponds;. n.PARENT: the node in the search tree that generated this node;. n.ACTION: the action that was applied to the parent to generate the node;. n.PATH-COST: the cost, traditionally denoted by g(n), of the path from the initial state to the node, as indicated by the parent pointers. 3.3.2 Measuring problem-solving performance. We can evaluate an algorithm’s performance in four ways: Completeness. : Is the algorithm guaranteed to find a solution when there is one: Optimality. : Does the strategy find the optimal solution, as defined on page 68: Time complexity. : How long does it take to find a solution: Space complexity. : How much memory is needed to perform the search: complexity. is expressed in terms of three quantities: b, the branching factor or maximum number of successors of any node; d, the depth of the shallowest goal. node (i.e., the number of steps along the path from the root); and m, the maximum length of any path in the state space. Time. is often measured in terms of the number of nodes generated. during the search, and space in terms of the maximum number of nodes stored in memory. we describe. time and space complexity. for search on a tree; for a graph,. the answer depends on how “redundant” the paths in the state space are. To assess the effectiveness of a search algorithm, we can consider just the search cost,. which typically depends on the time complexity but can also include a term for memory usage or we can use the total cost, which combines the search cost and the path cost of the solution found. 3.4 Uninformed Search Strategies. exponential-complexity search problems cannot be solved by uninformed methods for any but the smallest instances. 3.4.1 Breadth-first search. the root node is expanded first, then all the successors of the root node are expanded next, then their successors, and so on. all the nodes are expanded at a given depth in the search tree before any nodes at the next level are expanded. Breadth-first search is an instance of the general graph-search algorithm in. which the shallowest unexpanded node is chosen for expansion. This is achieved very simply by using a FIFO queue for the frontier. breadth-first search always has the shallowest path to every node on the frontier. BFS evaluation: Complete. : if the shallowest goal node is at some finite depth. d, breadth-first search will eventually find it after generating all shallower nodes. Optimal: if the path cost is a nondecreasing function of the depth of the node. Poor time complexity. : it scales horribly as trees become deeper. Poor. space complexity: every node generated remains in memory. There will be O(b^d−1) nodes in the explored set and O(b^d) nodes in the frontier, so the space complexity is O(b^d). 3.4.2 Uniform-cost search. uniform-cost search expands the node n with the lowest path cost g(n). This is done by storing the frontier as a priority queue ordered by g. the goal test is applied to a node when it is selected for expansion. a test is added in case a better path is found to a node currently on the frontier - if there is a better path then the old path is discarded and the new is added instead. Evaluation. : Completeness. is guaranteed provided the cost of every step exceeds some small positive constant N. worst-case time and space complexity is: When all step costs are the same, uniform-cost search is similar to breadth-first search, except that the latter stops as soon as it generates a goal, whereas uniform-cost search examines all the nodes at the goal’s depth to see if one has a lower cost; thus uniform-cost search does strictly more work by expanding nodes at depth d unnecessarily. Uniform-cost search does not care about the number of steps a path has, but only about their total cost. Therefore, it will get stuck in an infinite loop if there is a path with an infinite sequence of zero-cost actions. 3.4.3 Depth-first search. Depth-first search always expands the deepest node in the current frontier of the search tree. The search proceeds immediately to the deepest level of the search tree, where the nodes have no successors. As those nodes are expanded, they are dropped from the frontier, so then the search “backs up” to the next deepest node that still has unexplored successors. depth-first search uses a LIFO queue. The most recently generated node is chosen for expansion. This must be the deepest unexpanded node because it is one deeper than its parent—which, in turn,. was the deepest unexpanded node when it was selected. Evaluation: graph-search version is. complete. , which avoids repeated states and redundant paths, is complete in finite state spaces because it will eventually expand every node. tree-search version is. not complete: In infinite state spaces, both versions fail if an infinite non-goal path is encountered. Does not stop when the goal node is found previously in a tree-search. The time complexity of depth-first graph search is bounded by the size of the state space. A depth-first tree search may generate all of the O(b^m) nodes in the search tree, where m is the maximum depth of any node;. Good space complexity. : a depth-first tree search needs to store only a single path from the root to a leaf node, along with the remaining unexpanded sibling nodes for each node on the path. Once a node has been expanded, it can be removed from memory as soon as all its descendants have been fully explored. An improved variant of depth-first search called. backtracking search. uses still less memory. 3.4.4 Depth-limited search. depth-first search in infinite state spaces can be improved by supplying depth-first search with a predetermined depth limit L. nodes at depth L are treated as if they have no successors. Unfortunately, it also introduces an additional. source of incompleteness if we choose L<d, that is, the shallowest goal is beyond the depth limit. Depth-limited search will also be nonoptimal if we choose L>d. 3.4.5 Iterative deepening depth-first search. Iterative deepening search is a general strategy, often used in combination with depth-first tree search, that finds the best depth limit. by gradually increasing the limit—first 0, then 1 and so on. 3.4.6 Bidirectional search. The idea behind bidirectional search is to run two simultaneous searches—one forward from the initial state and the other backward from the goal—hoping that the two searches meet in the middle. Bidirectional search is implemented by replacing the goal test with a check to see. whether the frontiers of the two searches intersect; if they do, a solution has been found. The check can be done when each node is generated or selected for expansion and, with a hash table, will take constant time. The space complexity is also O(b^d/2). We can reduce this by roughly half if one of the two searches is done by iterative deepening, but at least one of the frontiers must be kept in memory so that the intersection check can be done. This space requirement is the most significant weakness of bidirectional search. 3.4.7 Comparing uninformed search strategies. 3.5 Informed(Heuristic) search strategies. informed search strategy: Strategy that uses problem-specific knowledge beyond the definition of the problem itself to solve the problem. 3.5.1 Greedy best-first search. Greedy best-first search tries to expand the node that is closest to the goal, on the grounds that this is likely to lead to a solution quickly. it evaluates nodes by using just the heuristic function; that is, f(n) = h(n). the algorithm is called “greedy” because at each step it tries to get as close to the goal as it can. Greedy best-first tree search is also incomplete even in a finite state space, much like. depth-first search. The graph search version is complete in finite spaces, but not in infinite ones. The worst-case time and space complexity for the tree version is. O. (. b. m. ). O(b^m). O. (. b. m. ). ﻿. , where m is the maximum depth of the search space. With a good heuristic function, however, the complexity can be reduced substantially. The amount of the reduction depends on the particular problem and on the quality of the heuristic. 3.5.2 A* search: Minimizing the total estimated solution cost. evaluates nodes by combining g(n), the cost to reach the node, and h(n), the cost. to get from the node to the goal: f(n) = g(n) + h(n). Since g(n) gives the path cost from the start node to node n, and h(n) is the estimated cost of the cheapest path from n to the goal, we have f(n) = estimated cost of the cheapest solution through n. a reasonable thing to try first is the node with the lowest value of g(n) + h(n). It turns out that this strategy is more than just reasonable: provided that the heuristic function h(n) satisfies certain conditions, A∗ search is both complete and optimal. The algorithm is identical to UNIFORM-COST-SEARCH except that A∗ uses g + h instead of g. Conditions for optimality: Admissibility and consistency. The first condition we require for optimality is that h(n) be an admissible heuristic: aka. one that never overestimates the cost to reach the goal. Because g(n) is the actual cost to reach n along the current path, and f(n) = g(n) + h(n), we have as an immediate consequence that f(n) never overestimates the true cost of a solution along the current path through n. A second, slightly stronger condition called consistency (or sometimes monotonicity) is required only for applications of A∗ to graph search. A heuristic h(n) is consistent if, for every node n and every successor n of n generated by any action a, the estimated cost of reaching the goal from n is no greater than the step cost of getting to n plus the estimated cost of reaching the goal. from n: h(n) ≤ c(n, a, n) + h(n). A∗ has the following properties: the tree-search version of A∗ is optimal if h(n) is admissible. the graph-search version is optimal if h(n) is consistent. Completeness requires that there be only finitely many nodes with cost less than or equal to C∗, a condition that is true if all step costs exceed some finite e and if b is finite. pruning: eliminating possibilities from consideration without having to examine them. A∗ usually runs out of space before time because it keeps all generated nodes in memory. For this reason, A∗ is not practical for many large-scale problems. 3.5.3 Memory-bounded heuristic search - RBFS+SMA*. iterative-deepening A*: Adapt the idea of iterative deepening to the heuristic search context to reduce memory requirements. The main difference between IDA∗ and standard iterative deepening is that the cutoff. used is the f-cost (g +h) rather than the depth; at each iteration, the cutoff value is the smallest f-cost of any node that exceeded the cutoff on the previous iteration. IDA∗ is practical for many problems with unit step costs and avoids the substantial overhead associated with keeping a sorted queue of nodes. Unfortunately, it suffers from the same difficulties with real valued costs as does the iterative version of uniform-cost search. Recursive best-first search: a simple recursive algorithm that attempts to mimic the operation of standard best-first search, but using only linear space. Its structure is similar to that of a recursive depth-first search, but. rather than continuing indefinitely down the current path, it uses the f limit variable to keep track of the f-value of the best alternative path available from any ancestor of the current node. If the current node exceeds this limit, the recursion unwinds back to the alternative path. As the recursion unwinds, RBFS replaces the f-value of each node along the path with a backed-up value, the best f-value of its children. In this way, RBFS remembers the f-value of the best leaf in the forgotten subtree and can therefore decide whether it’s worth re-expanding the subtree at some later time. Evaluation of RBFS. RBFS is an optimal algorithm if the heuristic function h(n) is. admissible. Its space complexity is linear in the depth of the deepest optimal solution, but its time complexity is rather difficult to characterize: it depends both on the accuracy of the heuristic function and on how often the best path changes as nodes are expanded. SMA*. SMA∗ proceeds just like A∗, expanding the best leaf until memory is full. At this point, it cannot add a new node to the search tree without dropping an old one. SMA∗ always drops the worst leaf node—the one with the highest f-value. SMA∗ then backs up the value of the forgotten node to its parent. In this way, the ancestor of a forgotten subtree knows the quality of the best path in that subtree. With this information, SMA∗ regenerates the subtree only when all other paths have been shown to look worse than the path it has forgotten. To avoid selecting the same node for deletion and expansion, SMA∗ expands the newest best leaf and deletes the oldest worst leaf. These coincide when there is only one leaf, but in that case, the current search tree must be a single path from root to leaf that fills all of memory. If the leaf is not a goal node, then even if it is on an optimal solution path, that solution is not reachable with the available memory. Therefore,. the node can be discarded exactly as if it had no successors. SMA* Evaluation: SMA∗ is complete if there is any reachable solution(if d, the depth of the. shallowest goal node, is less than the memory size). It is optimal if any optimal solution is reachable; otherwise, it returns the best reachable solution. fairly robust choice for finding optimal solutions, particularly when the state. space is a graph, step costs are not uniform, and node generation is expensive compared to the overhead of maintaining the frontier and the explored set. 3.6 Heuristic Functions. 3.6.1 The effect of heuristic accuracy on performance. One way to characterize the quality of a heuristic is the effective branching factor b∗. If the. total number of nodes generated by A∗ for a particular problem is N and the solution depth is d, then b∗ is the branching factor that a uniform tree of depth d would have to have in order to contain N + 1 nodes. Thus,. if A∗ finds a solution at depth 5 using 52 nodes, then the effective branching. factor is 1.92. The effective branching factor can vary across problem instances, but usually it is fairly constant for sufficiently hard problems. 3.6.2 Generating admissible heuristics from relaxed problems. A problem with fewer restrictions on the actions is called a. relaxed problem. e the removal of restrictions creates added edges in the graph. Because the relaxed problem adds edges to the state space, any optimal solution in the. original problem is, by definition, also a solution in the relaxed problem; but the relaxed. problem may have better solutions if the added edges provide short cuts. Hence, the cost of an optimal solution to a relaxed problem is an admissible heuristic for the original problem. because the derived heuristic is an exact cost for the relaxed problem, it must. obey the triangle inequality and is therefore consistent. 3.6.3 Generating admissible heuristics from subproblems: Pattern databases. Admissible heuristics can also be derived from the solution cost of a subproblem of a given problem. fx. an 8-puzzle where we simply solve it for 4 of the numbers, eg. 1, 2, 3, 4. In order to solve it completely we will always require more moves than for the subproblem, as such it is a valid heuristic. It turns out to be more accurate than Manhattan distance in some cases. The idea behind pattern databases is to store these exact solution costs for every possible subproblem instance—in our example, every possible configuration of the four tiles and the blank. The database itself is constructed by searching back13 from the goal and recording the cost of each new pattern encountered; the expense of this search is amortized over many subsequent problem instances. The choice of 1-2-3-4 is fairly arbitrary; we could also construct databases for 5-6-7-8, for 2-4-6-8, and so on. Each database yields an admissible heuristic, and these heuristics can be combined, as explained earlier, by taking the maximum value. 3.6.4 Learning heuristics from experience. 5 Adversarial Search and Games. : Questions. Page 308, figure 5.5, stage d+e - how does A-B pruning know that the middle tree does not contain larger values than 2 when it only check the first node? -Add answer to section 5.2.3 of notes. For suboptimal. Summary. A game can be defined by the initial state (how the board is set up), the legal actions in. each state, the result of each action, a terminal test (which says when the game is over),. and a utility function that applies to terminal states to say who won and what the final. score is. In two-player, discrete, deterministic, turn-taking zero-sum games with perfect. information, the minimax algorithm can select optimal moves by a depth-first. enumeration of the game tree. The alpha–beta search algorithm computes the same optimal move as minimax, but. achieves much greater efficiency by eliminating subtrees that are provably irrelevant. Usually, it is not feasible to consider the whole game tree (even with alpha–beta), so we. need to cut the search off at some point and apply a heuristic evaluation function that. estimates the utility of a state. An alternative called Monte Carlo tree search (MCTS) evaluates states not by applying. a heuristic function, but by playing out the game all the way to the end and using the. rules of the game to see who won. Since the moves chosen during the playout may not. have been optimal moves, the process is repeated multiple times and the evaluation is. an average of the results. Many game programs precompute tables of best moves in the opening and endgame so that they can look up a move rather than search. Games of chance can be handled by expectiminimax, an extension to the minimax. algorithm that evaluates a chance node by taking the average utility of all its children,. weighted by the probability of each child. In games of imperfect information, such as Kriegspiel and poker, optimal play requires. reasoning about the current and future belief states of each player. A simple. approximation can be obtained by averaging the value of an action over each possible. configuration of missing information. Programs have soundly defeated champion human players at chess, checkers, Othello,. Go, poker, and many other games. Humans retain the edge in a few games of imperfect. information, such as bridge and Kriegspiel. In video games such as StarCraft and Dota 2,. programs are competitive with human experts, but part of their success may be due to. their ability to perform many actions very quickly. Lecture Notes. Deterministic - all possible moves are known and there are no chance elements. MiniMax. TimeComplexity is poor, but space complexity is good since we only keep the current layer and path in memory. Three stances to multi-agent environments: As an economy: Regarding agents as a whole and predicting what influence a change in the environment would have on them in general. Part of the environment: Considered a part of the environment, having no intended effect on us, but can still pose an unintended effect, such as wind, it sucks but it doesn’t intend to. Adversaries: Equiped with adversarial game-tree search techniques to actively defeat an opponent. Adversarial game-tree: A tree showing heuristic values for various steps one can make in a game. It shows some expected values based on given evaluation criterias such as captured pieces in a chess game. 5.1.1 Two-player zero-sum games. Aka. deterministic, two-player, turn-taking, perfect information(fully observable), zero-sum games(a move is good for one player and bad for the other). Move: synonym for Action. Position: synonym for state. Game defined as follows: S0: The initial state, which specifies how the game is set up at the start. TO-MOVE : The player whose turn it is to move in state. ACTIONS : The set of legal moves in state. RESULT : The transition model, which defines the state resulting from taking action. in state. IS-TERMINAL(s): A terminal test, which is true when the game is over and false otherwise. States where the game has ended are called terminal states. UTILITY(s, p): A utility function (also called an objective function or payoff function),. which defines the final numeric value to player when the game ends in terminal state s. In chess fx., the outcome is a win, loss, or draw, with values 1, 0, or 1/2. complete game tree: a tree covering all sequences of moves to a terminal state - may be infinite if not bounded. Patial game-tree: shows part(s) of all sequences to a terminal state. 5.2.0 Optimal Decisions in Games. Minimax search: two adversaries max and min fight to get a value as high or low as possible. Min wants the lowest value and max vice versa. ply: A single move made by a single player. optimal strategy. can be determined by working out the. minimax. value. of each state in the tree, which we write as MINIMAX(s). The minimax value is the. utility. (for MAX) of being in that state, assuming that both players play optimally from there to the end of the game. On Max’s turn it picks highest value, and Min picks lowest. If it is a leaf it simply has its own value, so the evaluation of a given points minimax value is: 5.2.1 The Minimax search algorithm. Minimax creates a search tree of game values, such that we can find the best route to a certain victory. It is a recursive algorithm that proceeds all the way down to the leaves of the tree and then backs up the minimax values through the tree as the recursion unwinds. Pseudo code. Performance: Performs a complete depth-first exploration of the game tree. If the maximum depth of the tree is m and there are legal moves at each point, then the time complexity of the minimax algorithm is. O. (. b. m. ). O(b^m). O. (. b. m. ). ﻿. The space complexity is. O. (. b. m. ). O(bm). O. (. bm. ). ﻿. for an. algorithm that generates all actions at once, or. O. (. m. ). O(m). O. (. m. ). ﻿. for an algorithm that generates actions one at a time. 5.2.2 Optimal decisions in multiplayer games. How to evaluate 2+ players: 1) replace the single value for each node with a vector of values. Fx. three-player game with players A,B and C =. (. v. A. ,. v. B. ,. v. C. ). (v_A ,v_B ,v_C). (. v. A. ​. ,. v. B. ​. ,. v. C. ​. ). ﻿. as each node. In terminal states, the values in the vector are from each player’s respective view fx.: (. v. a. =. 5. ,. v. b. =. 2. ,. v. c. =. 2. ). (v_a = 5, v_b = 2, v_c = 2). (. v. a. ​. =. 5. ,. v. b. ​. =. 2. ,. v. c. ​. =. 2. ). ﻿. 2) Consider the nonterminal states: for each non-terminal node, the player will choose the option which is best for itself. So the backed-up value of a node is the utility vector of the successor state with the highest value for the player choosing at. Alliances. If two players A, B are weaker than player C, they may work together to avoid being destroyed by C. This is still a selfish action to save themselves. Once C is weak, they can resume fighting. Reliability and gain from breaking alliances must be evaluated too. Explained in chapter 18 of the book. 5.2.3 Alpha-Beta Pruning. Pruning is essentially removing non-relevant sub-trees from the search queue to reduce search time. Essentially, if we have several states to choose from and the given Max or Min player has to choose, then the states which they will definitely not pick does not need to be searched. Fx. max choosing between subtrees of values 7, 3 and 2, would never choose 3 or 2 so why search them. In order to find the trees value do following: Evaluate the left subtree. Evaluate the leftmost Node(only one) in each sub-tree. If the value is higher or lower(the opposite of what a player want, since we’re examining the opponents options). Pseudo Code. The general principle is this: consider a node n somewhere in the tree such that Player has a choice of moving to n. If Player has a better choice either at the same level or at any point. higher up in the tree then Player will never move to n So once we have found out enough about (by examining some of its descendants) to reach this conclusion, we can prune it. Values of Alpha and Beta: 5.2.4 Move Ordering. effectiveness of alpha–beta pruning is highly dependent on the order in which the. states are examined. If the first leaf in a subtree is a bad representation of the subtree then it evaluates poorly. As such it might be worthwhile to try to first examine the successors that are likely to be best. Evaluation: Iterative deepening: search one ply deep and record the ranking of moves based on their evaluations. search one ply deeper, using the previous ranking to inform move ordering. repeat till last ply. Killer move heuristic : always selecting the best moves possible. Transposition: Different permutations of the move sequence so the same state arrives multiple times, like moving a piece back and forth continuously. It has a very negative effect on searches as we essentially have to search the same subtree multiple times. Transposition table: caches the heuristic value of states. If the same state is encountered again then the value is simply fetched from the table rather than searching the subtree again. It. allows us to double our search depth. Two strategies for large problems: Type A strategy. : considers all possible moves to a certain depth in the search tree, and then uses a heuristic evaluation function to estimate the utility of states at that depth. It explores a wide but shallow portion of the tree. Type B strategy. : ignores moves that look bad, and follows promising lines “as far as possible.” It explores a deep but narrow portion of the tree. 5.3 Heuristic Alpha–Beta Tree Search. heuristic evaluation function to states: treating nonterminal nodes as if they were. terminal by replacing the Utility function with Eval which estimates a state’s utility. The terminal test is also replaced by a cutoff test, which returns true for terminal states, but can otherwise cut the search anytime based on search depth and state properties. The formula for H-Minimac(s, d) for the heuristic minimax value of state s at search depth d, is as follows: 5.3.1 Evaluation functions. A heuristic evaluation function returns an estimate of the expected utility of state. to player , just as the heuristic functions(like A*) return an estimate of the distance to. the goal. For terminal states it must be that. E. v. a. l. (. s. ,. p. ). =. U. t. i. l. i. t. y. (. s. ,. p. ). Eval(s, p) = Utility(s, p). E. v. a. l. (. s. ,. p. ). =. U. t. i. l. i. t. y. (. s. ,. p. ). ﻿. and for nonterminal states the evaluation must be somewhere between a loss and a win: U. T. I. L. I. T. Y. (. l. o. s. s. ,. p. ). ≤. E. V. A. L. (. s. ,. p. ). ≤. U. T. I. L. I. T. Y. (. w. i. n. ,. p. ). UTILITY(loss, p) ≤ EVAL (s, p) ≤ UTILITY(win, p). U. T. I. L. I. T. Y. (. l. oss. ,. p. ). ≤. E. V. A. L. (. s. ,. p. ). ≤. U. T. I. L. I. T. Y. (. w. in. ,. p. ). ﻿. A good evaluation function: Has a quick computation. Is Strongly correlated to chances of winning. Essentially, through heuristics we estimate the chance of winning based on criterias such as how many possible states following the current would lead to a loss/win/tie. This is, however, hard to calculate so most evaluation functions compute separate numerical contributions from each feature and then combine them to find the total. value, a so called. weighted linear function. f indicates a given value, such as amount of pawns, and w is their weight, eg. how valuable they are in the evaluation. So by counting and weighing all pieces on a chess board we can evaluate how well a player is doing. the evaluation function should be strongly correlated with the actual chances. of winning, but it need not be linearly correlated: if state s is twice as likely to win as state s’ we don’t require that EVAL(S) be twice EVAL(S'); all we require is that EVAL(s)>EVAL(s’). In games where human experience is not available, the weights of the evaluation function(like, weight of chess pieces) can be estimated by machine learning techniques. 5.3.2 Cutting off search. Simple explanation of modification to implement it: Replace IS-TERMINAL with IS-CUTOFF(state, depth). Let IS-CUTOFF return true for depths deeper a given depth d. Add a transposition table to improve search time. modifying ALPHA-BETA-SEARCH so that it will call the heuristic EVAL function. when it is appropriate to cut off the search. replace the two lines that mention IS-TERMINAL with the following line: if game.IS-CUTOFF(state, depth) then return game.EVAL(state, player), null. Then set a fixed depth limit so that IS-CUTOFF(state, depth) returns true for all depth greater than some fixed depth (as well as for all terminal states). The depth is chosen so that a move is selected within the allocated time. A more robust approach is to apply iterative deepening. in each round of iterative deepening we keep entries in the transposition table, subsequent rounds will be faster, and we can use the evaluations to improve move ordering. These simple approaches can lead to errors due to the approximate nature of the evaluation function. It cannot look ahead and is therefore vulnerable. The evaluation function should be applied only to positions that are. quiescent. —that is,. positions in which there is no pending move that wouldwildly swing the evaluation. For nonquiescent positions the IS-CUTOFF returns false, and the. search continues until quiescent positions are reached. This extra quiescence search is. sometimes restricted to consider only certain types of moves, such as capture moves, that will quickly resolve the uncertainties in the position. horizon effect: when facing an opponent’s move that causes serious damage and is ultimately unavoidable, but is temporarily avoided by the use of delaying tactics. singular extensions: Moves that are better than all other available moves. 5.3.3 Forward Pruning. forward pruning(a Type B strategy) saves computation time, but risk failure, by pruning moves that appear to be poor moves, but might possibly be good ones. beam search: is an example of forward pruning. on each ply, consider only. a “beam” of the best moves (according to the evaluation function) rather than considering n all possible moves. Unfortunately, this approach is rather dangerous because there is no guarantee that the best move will not be pruned away. PROBCUT: PROBCUT(probabilistic cut): forward-pruning version of alpha–beta search that uses statistics gained from prior experience to lessen the chance that the best move will be pruned. It prunes nodes both provably AND probably outside the window of the current state(A, B). It searches as follows: Shallow search to compute backed-up value v of a node. Applies past experience to estimate how likely it is that a score of v at depth d. in the tree would be outside(A, B). Late move reduction. Assumes that move ordering is done well, and that the first moves in the ordering are likely the best ones. The last ones are not pruned, but their search depth is reduced - If it returns a better value it is then searched full-depth. 5.3.4 Search versus lookup. Some states may have data available and then a lookup of moves from a database is better than a search. However, as states become more uncommon, data is limited and a search is more efficient. retrograde minimax search: Essentially reverse search. Consider all possible end states and then compute moves backwards until you reach the current state. As such you will end up with a winning path. 5.4 Monte Carlo Tree Search(MCTS). Pseudo Code. MCTS does not use heuristics, the value of a state is estimated as the average utility over a number of simulations of complete games starting from the state. A simulation(. Playout. /. rollout. ) chooses moves for each player repeatedly, until reaching a terminal position. Playout policy: Used to guide the playout in a meaningful selection of moves. Game-specific heuristics can be used, such as “consider capture moves” in chess. Pure Monte Carlo Search: do N simulations starting from the current state and track which moves has the highest win percentage. Selection policy: focuses the computational resources on the important parts of the game tree by using 2 factors: Exploration of states which have had few playouts(simulations). Exploitation of states that have done well previously to evaluate their value. The MCTS does all above by maintaining a search tree and on each iteration it does: SELECTION: Go from root towards a leaf by using the selection policy. EXPANSION: Grow the search tree by generating a new child of the selected node. SIMULATION: Perform a playout from the newly generated child using the playout policy. BACK-PROPAGATION: Use child simulation results to update search tree nodes going up to the root. Steps are repeated for a given amount of iterations or within a time limit. upper confidence bounds applied to trees(UCT) policy. The policy ranks each possible move based on an upper confidence bound formula. called UCB1: For a node , the formula is: Where U(n) is the total utility of all playouts through node n, N(n) is number of playout through n and PARENT(n) is the parent of n. The. average utility of n. (the exploitation term) is. U. (. n. ). /. N. (. n. ). U(n)/N(n). U. (. n. ). /. N. (. n. ). ﻿. The term with the square root is the exploration term: it has the count in the denominator, which means the term will be high for nodes that have only been explored a few times. In the numerator it has the log of the number of times we have explored the parent of. C is a constant that balances exploitation and exploration. C may be square root of 2 or generated by neural networks for max efficiency. Evaluation. Computing a playout = linear in the depth of the tree. Monte Carlo simulation is usually better when the branching factor is very high (where. alpha–beta can’t search deep enough) or when it is difficult to define a good evaluation function. MCTS relies on aggregate values rather than computation and does not suffer from miscalculation errors. MCTS can be joined with evaluation functions by running the simulation and then truncating the playout and applying an evaluation function. MCTS can adopt aspects of Alpha-beta such as early playout termination(by stopping too long playouts, and using a heuristic evaluation function instead). It can be applied to games with no previous data as it does not rely on it. Negative factors of MCTS. pruning in Monte Carlo search means that a vital line of play might not be explored at all since the stochastic nature of Monte Carlo search means it might fail to consider it. Cannot quickly conclude “obvious” win playouts as it still needs multiple simulations to conclude anything. 5.5 Stochastic Games(Chance)(ExpectiMiniMax). Includes a random element such as the throw of a dice. we can only calculate the. expected value. of a position: the average over all possible outcomes of the chance nodes. the. expectiminimax. value for games with chance nodes, a generalization of. the minimax value for deterministic games. Terminal nodes and MAX and MIN nodes work exactly the same way as before. For chance nodes we compute the expected value, which is the sum of the value over all outcomes, weighted by the probability of each chance action: 5.5.1 Evaluation functions for games of chance. the obvious approximation to make with expectiminimax is to cut the. search off at some point and apply an evaluation function to each leaf. the presence of chance nodes means that one has to be more careful about what the values mean. the evaluation function must return values that are a positive linear transformation of the probability of winning (or of the expected utility, for games that have outcomes other than win/lose). alpha–beta pruning can be applied to game trees with chance nodes. The analysis for MIN and MAX nodes is unchanged, but we can also prune chance nodes, using a bit of ingenuity. if we put bounds on the possible values of the utility function, then we can. arrive at bounds for the average without looking at every number. 5.6 Partially Observable Games !!!INCOMPLETE FROM 5.6.2!!: Partial observability: Parts of the environment are unknows such as adversaries’ piece locations in a game. 5.7 Limitations of Game Search Algorithms. all the algorithms must make some assumptions and approximations: Alpha–beta: search uses the heuristic evaluation function as an approximation. Vulnerable to calculation errors which ruins the tree. Difficult to compensate for errors in evaluation functions because we don’t have a good model of the dependencies between the values of sibling nodes. Designed to calculate values of legal moves - when there is only one available move it will still search. Reasons on the level of individual moves - cannot abstract and see a higher goal such as trapping a chess piece but not capturing it. Poorly incorporates machine learning: Early game programs relied on human expertise to hand-craft evaluation functions, opening books, search strategies, and efficiency tricks. Monte Carlo: search computes an approximate average over a random selection of playouts. Designed to calculate values of legal moves - when there is only one available move it will still search. Reasons on the level of individual moves - cannot abstract and see a higher goal such as trapping a chess piece but not capturing it. Poorly incorporates machine learning: Early game programs relied on human expertise to hand-craft evaluation functions, opening books, search strategies, and efficiency tricks. The choice of which algorithm to use depends. in part on the features of each game: when the branching factor is high or it is difficult to. define an evaluation function, Monte Carlo search is preferred. But both algorithms suffer. from fundamental limitations. metareasoning. : (reasoning about reasoning). 6 Constraint satisfaction problems. : Lecture notes. Summary. Constraint satisfaction problems (CSPs) represent a state with a set of variable/value. pairs and represent the conditions for a solution by a set of constraints on the variables. Many important real-world problems can be described as CSPs. A number of inference techniques use the constraints to infer which variable/value pairs. are consistent and which are not. These include node, arc, path, and k-consistency. Backtracking search, a form of depth-first search, is commonly used for solving CSPs. Inference can be interwoven with search. The minimum-remaining-values and degree heuristics are domain-independent methods for deciding which variable to choose next in a backtracking search. The least constraining-value heuristic helps in deciding which value to try first for a given. variable. Backtracking occurs when no legal assignment can be found for a variable. Conflict-directed backjumping backtracks directly to the source of the problem. Local search using the min-conflicts heuristic has also been applied to constraint satisfaction problems with great success. The complexity of solving a CSP is strongly related to the structure of its constraint. graph. Tree-structured problems can be solved in linear time. Cutset conditioning can. reduce a general CSP to a tree-structured one and is quite efficient if a small cutset can. be found. Tree decomposition techniques transform the CSP into a tree of subproblems. and are efficient if the tree width of the constraint graph is small. 6.1. 6.1 Defining constraint satisfaction problems. A constraint satisfaction problem consists of three components, X, D, and C: X is a set of variables, {X1,. ,Xn}. D is a set of domains, {D1,. ,Dn}, one for each variable. C is a set of constraints that specify allowable combinations of values. Each domain Di consists of a set of allowable values, {v1,. ,vk} for variable Xi. Each constraint Ci consists of a pair scope, rel, where scope is a tuple of variables that participate in the constraint and rel is a relation that defines the values that those variables can take on. 6.2 Example. 6.3 Variations on the CSP formalism. The simplest kind of CSP involves variables that have discrete, finite domains. A discrete domain can be infinite, such as the set of integers or strings. With infinite domains, it is no longer possible to describe constraints by enumerating all allowed combinations of values. Instead, a constraint language must be used that understands constraints such as T1 + d1 ≤ T2 directly, without enumerating the set of pairs of allowable values for (T1, T2). The best-known category of continuous-domain CSPs. is that of linear programming problems, where constraints must be linear equalities or inequalities. Linear programming problems can be solved in time polynomial in the number of variables. The simplest type of constraint is the. unary constraint. , which restricts the value of a single variable. For example, in the map-coloring problem it could be the case. that South Australians won’t tolerate the color green; we can express that with the unary constraint: A. binary constraint. relates two variables. For example,. SA != NSW is a binary constraint. A binary CSP is one with only binary constraints;. A constraint involving an arbitrary number of variables is called a. global constraint. One of the most common global constraints is. Alldiff. , which says that all of the variables involved in the constraint must have different values. Constraints can be represented in a. constraint hypergraph. , such as the one shown in Figure 6.2(b). A hypergraph consists of ordinary nodes (the circles in the figure) and hypernodes (the squares), which represent n-ary constraints. two reasons why we might prefer a global constraint such as Alldiff rather than a set of binary constraints: easier and less error-prone to write the problem description using Alldiff. possible to design special-purpose inference algorithms for global constraints that are not available for a set of more primitive constraints. constraint optimization problem COP: A problem that can be solved with path-based or local optimization search methods. 6.2 Constraint propagation: Inference in CSPs. In CSPs an algorithm can search or do a specific type of inference called. constraint propagation. - using the constraints to reduce the number of legal values for a variable and thus reducing legal values for another variable. It can be done during search or as a preprocessing step before search. Local consistency: treat each variable as a node in a graph, and each binary constraint as an arc, then the process of enforcing local consistency in each part of the graph inconsistent values to be eliminated in the graph. Different types of local consistency is described in the chapters below: 6.2.1 Node consistency. node-consistent. : a variable where all its values in its domain satisfy the unary constraints. We can make a node consisting by eliminating the conflicting values from the domain of options. Like removing a color in the map issue which cannot be applied to a country. It is also possible to transform all n-ary constraints into binary ones. 6.2.2 Arc consistency. A variable in a CSP is arc-consistent if every value in its domain satisfies the variable’s. A variable X_i is. generalized arc consistent. with respect to an n-ary constraint if for every value v in the domain of X_i there exists a tuple of values that is a member of the constraint, has all its values taken from the domains of the corresponding variables, and has its X_i component equal to v. For example, if all variables have the domain {0, 1, 2, 3}, then to make the variable X consistent with the constraint X<Y <Z,. we would have to eliminate 2 and 3 from the domain of X because the constraint cannot be satisfied when X is 2 or 3. 6.2.3 Path consistency. Path consistency tightens the binary constraints by using implicit constraints that are inferred by looking at triples of variables. 6.2.4 K-consistency. A CSP is k-consistent if, for any set of k − 1 variables and for any consistent assignment to those variables, a consistent value can always be assigned to any kth variable. 1-consistency says that, given the empty set, we can make any set of one variable consistent: this is what we. called node consistency. 2-consistency is the same as arc consistency. For binary constraint networks, 3-consistency is the same as path consistency. A CSP is strongly k-consistent if it is k-consistent and is also (k − 1)-consistent, (k − 2)-consistent,. all the way down to 1-consistent. 6.2.5 Global constraints. A global constraint is one involving an arbitrary number of variables. One simple form of inconsistency detection for Alldiff constraints works as follows: if m variables are involved in the constraint, and if they have n possible distinct values altogether, and m>n, then the constraint cannot be satisfied. Simple algorithm: Remove any constraint in the constraint that has a singleton domain and delete that variable’s value from the domains of the remaining variables. Repeat as long as there are singleton variables. If at any point an empty domain is produced or there are more variables than domain values left, then an inconsistency has been detected. Resource constraint. The constraint that no more than 10 personnel are assigned to a task in total is written as. Atmost(10, P1, P2, P3, P4). We can detect an inconsistency simply by checking the sum of the minimum values of the current domains;. for example, if each variable has the domain {3, 4, 5, 6}, the Atmost constraint cannot be satisfied. - Bascially it says that we have a maximum value “10” and p1-p4 are tasks that each take some time. If they collectively take more than the maximum then the constraint can’t be satisfied. To solve it we would then remove variables from our domain, fx. 5 and 3 so we do not exceed the max value. B. ounds propagation: a CSP is bounds consistent if for every variable X, and for both the lower bound and upper-bound values of X, there exists some value of Y that satisfies the constraint between X and Y for every variable Y. This kind of bounds propagation is widely used in practical constraint problems. 6.2.6 Sudoku Example. A Sudoku puzzle can be considered a CSP with 81 variables, one for each square. We. use the variable names A1 through A9 for the top row (left to right), down to I1 through I9. for the bottom row. The empty squares have the domain {1, 2, 3, 4, 5, 6, 7, 8, 9} and the prefilled squares have a domain consisting of a single value. In addition, there are 27 different Alldiff constraints: one for each row, column, and box of 9 squares. 6.3 Backtracking Search for CSPs. A problem is. commutative. if the order of application of any given set of actions has no effect on the outcome. CSPs are commutative because when assigning values to variables, we reach the same partial assignment regardless of order. backtracking search. a depth-first search that chooses values for one variable at a time and backtracks when a variable has no legal values left to assign, shown in Figure 6.5. 6.3.1 Variable and value ordering. The backtracking algorithm contains the line. var ← SELECT-UNASSIGNED-VARIABLE(csp). The simplest strategy for SELECT-UNASSIGNED-VARIABLE is to choose the next unassigned variable in order, {X1, X2,. }. This static variable ordering seldom results in the most efficient search. minimum remaining-values(most constrained variable): choosing the variable with the fewest “legal” values. Basically if several variables needs to be assigned then start with the one with the most options. degree heuristic: attempts to reduce the branching factor on future choices by selecting the variable that is involved in the largest number of constraints on other unassigned variables. The degree of a variable. = how many constraints(options) it has. least-constraining-value heuristic: the heuristic is trying to leave the maximum flexibility for subsequent variable assignments by first selecting the variables with the least constraints(options) and assigning them a value. 6.3.2 Interleaving search and inference. every time we make a choice of a value for a variable, we can make new domain reductions on the neighboring variables - basically, if we assign a value to a variable, then we can remove it from its neighbours(in the map example). forward checking: When a variable X is assigned the forward-checking process establishes arc consistency for it - for each variable Y connected to X by a constraint, delete values that is inconsistent with the one chosen for X. Because forward checking only does arc consistency inferences, there is no reason to do forward checking if we have already done arc consistency as a preprocessing step. We can view forward checking as an efficient way to incrementally compute the information. that the. MRV(minimum remaining values). heuristic needs to do its job. Although forward checking detects many inconsistencies, but not all, it doesn’t look ahead and make all the other variables arc-consistent. MAC(Maintaining arc consistency): 6.3.3 Intelligent backtracking: looking forward. chronological backtracking: when a branch of the search fails, back up to the preceding variable and try a different. value for it. conflict set: a set of assignments that are in conflict with some value of a variable - eg. two neighbouring countries in the map example, having color 1 and 2. The two colors, 1 and 2 is then the conflict set for the third country. backjumping: backtracks to the most recent assignment in the conflict set. it accumulates the conflict set while checking for a legal value to assign. If no legal value is found, the algorithm should return the most recent element of the conflict set along with the failure indicator. forward checking can supply the conflict. set with no extra work: whenever forward checking based on an assignment X = x deletes a value from Y ’s domain, it should add X = x to Y ’s conflict set. If the last value is deleted from Y ’s domain, then the assignments in the conflict set of Y are added to the conflict set of X. Then, when we get to Y , we know immediately where to backtrack if needed. backjumping occurs when every value in a domain is in conflict with the current assignment; but forward checking detects this event and prevents the search from ever reaching such a node! In fact, it can be shown that every branch pruned by backjumping is also pruned by forward checking. Hence, simple backjumping is redundant in a forward-checking search or, indeed, in a search that uses stronger consistency checking, such as MAC. conflict-directed backjumping: A backjumping algorithm that uses conflict sets to find the last step where a conflict was moderated so we can change the assignment at that stage to resolve the conflict. When we reach a contradiction, backjumping can tell us how far to back up, so we don’t waste time changing variables that won’t fix the problem. Constraint learning. is the idea of finding a minimum set(called. no-good. ) of variables from the conflict set that causes the problem. No-goods can be effectively used by forward checking or by backjumping. 6.4 Local Search for CSPs. The point of local search is to eliminate violated constraints. Min-conflicts. : the value that results in the minimum number of conflicts with other variables when choosing a new value for a variable. tabu search. : keeping a small list of recently visited states and forbidding the algorithm to return to those states. Simulated annealing can also be used to escape from plateaux. constraint weighting: Each constraint is given a numeric weight, W_i, initially all 1. At each step of the search, the algorithm chooses a variable/value pair to change that will result in the lowest total weight of all violated constraints. The weights are then adjusted by incrementing the weight of each constraint that is violated by the current assignment. This has two benefits: Adds topography to plateaux, making sure that it is possible to improve from the current. state. Adds weight to the constraints that are proving difficult to solve. 6.5 The Structure of Problems. independent subproblems. : Parts of a problem that are not directly connected to the rest of the graph, essentially a single non-connected component is a subproblem. connected components. : directed arc consistency. : A CSP is defined to be directed arc-consistent under. topological sort: an ordering of the variables such that each variable appears after its parent in the tree. Any tree with n nodes has n−1 arcs, so we can make this graph. directed arc-consistent in O(n) steps, each of which must compare up to d possible domain. values for two variables, for a total time of O(nd^2). constraint graphs reduced to trees. two primary ways to do this: removing nodes. collapsing nodes together. The first approach involves assigning values to some variables so that the remaining variables form a tree. The general algorithm is as follows: The second approach is based on constructing a tree decomposition of the constraint graph into a set of connected subproblems. Each subproblem is solved independently, and the resulting solutions are then combined. A tree decomposition must satisfy the following three requirements: Every variable in the original problem appears in at least one of the subproblems. If two variables are connected by a constraint in the original problem, they must appear. together (along with the constraint) in at least one of the subproblems. If a variable appears in two subproblems in the tree, it must appear in every subproblem along the path connecting those subproblems. The first two conditions ensure that all the variables and constraints are represented in the. decomposition. The third condition seems rather technical, but simply reflects the constraint that any given variable must have the same value in every subproblem in which it appears;. We solve each subproblem independently; if any one has no solution, we know the entire problem has no solution. If we can solve all the subproblems, then we attempt to construct. a global solution: view each subproblem as a “mega-variable” whose domain is the set of all solutions for the subproblem. solve the constraints connecting the subproblems, using the efficient algorithm for trees given earlier. A given constraint graph admits many tree decompositions; in choosing a decomposition, the aim is to make the subproblems as small as possible. The tree width of a tree decomposition of a graph is one less than the size of the largest subproblem. the tree width of the graph itself is defined to be the minimum tree width among all its tree decompositions. value symmetry: Different combinations of assigning the same values to the same variables. symmetry-breaking constraint: A constraint which breaks value symmetry to save search space. This can be done by fx. arranging the possible values in alphabetical order so one will always be assigned before another. breaking value symmetry has proved to be important and effective on a wide range of problems. Search Heuristics and AlphaGo Zero. : Search heuristics: For heuristic swe aim to have a value as high to the actual value fx. distances or moves, that is required to reach a terminal state, however, we must be certain that it is admissible eg. cannot overestimate the heuristic value. A heuristic is dominating over another if its value is always higher or equal to the other heuristic’s values whilst remaining admissible. Relaxed problems: Essentially: Disregard some of the rules to make the game easier. This is called a relaxation. Then the problem would be easier to solve and the minimum amount of moves are then lower than the actual minimum moves for the game and therefore an admissible heuristic. 7 Logical agents. : knowledge-based agents can combine and recombine information to suit myriad purposes. Propositional logic is a factored representation; while less expressive than first-order logic which is the canonical structured representation, propositional logic illustrates all the basic. concepts of logic. It also comes with well-developed inference technologies. 7.1 Knowledge-Based Agents. The central component of a. knowledge-based agent. is its. knowledge base. consisting of a set of sentences. Each sentence is expressed in a language called a. knowledge representation language. and represents some assertion about the world. Tell. operation: adding new sentences of information to the Knowledge Base. Ask. operation: Retrieving info from the Knowledge Base. Each call to the agent it then does: Tells the current perception to the knowledge base(KB). Asks the KB what action to execute based on its perceived state. Agent informs KB what action was chosen and then performs it. A knowledge-based agent can be built simply by TELLing it what it needs to know. Starting. with an empty knowledge base, the agent designer can TELL sentences one by one until the. agent knows how to operate in its environment. This is called the. declarative approach to. system building. a successful agent often combines both declarative and procedural elements in its design, and declarative knowledge can often be compiled into more efficient procedural code. 7.2. The Wumpus World. 7.3 Logic. The sentences of the knowledge base is expressed according to the syntax of the representation language, which specifies all the sentences that are well formed. A logic must also define the semantics, or meaning, of sentences. The semantics defines the. truth of each sentence with respect to each possible world. The KB can be thought of as a set of sentences or as a single sentence that asserts all the. individual sentences. The KB is false in models that contradict what the agent knows. Model: a term for mathematical abstractions, each of which has a fixed truth. value (true or false) for every relevant sentence. It depicts possibly real environments that an agent may operate in. logical entailment. between sentences—the idea that a sentence follows logically from another sentence. to mean that the sentence entails the sentence. The formal definition of entailment is. this: if and only if, in every model in which is true, is also true. model checking. : An inference algorithm that enumerates all possible models to check that a is true in all models in which KB is true, that is: think of the set of all consequences of as a haystack and of as a needle. Entailment is like the needle being in the haystack; inference is like finding it. Sound/truthPreserving inference algorithms: An inference algorithm that derives only entailed sentences. Soundness is a highly desirable property. An unsound inference procedure essentially makes things up as it goes along. completeness: an inference algorithm is complete if it can derive any sentence that is entailed. if KB is true in the real world, then any sentence derived from KB by a sound inference procedure is also true in the real world. correspondence between world and representation is illustrated as: grounding: the connection between logical reasoning. processes and the real environment in which the agent exists. In particular, how do we know that KB is true in the real world - it is done by the agents sensors creating the connection between the sentences and real world aspects. General rules are produced by a sentence construction process called learning - such as connecting a red light with a stop action by giving it negative points for ignoring it. 7.4 Propositional Logic: A Very Simple Logic. 7.4.1 Syntax. The syntax of propositional logic defines the allowable sentences. The atomic sentences. consist of a single proposition symbol. Each such symbol stands for a proposition that can be true or false. Complex sentences. are constructed from simpler sentences, using parentheses and. operators called logical connectives. 7.4.2 Semantics. The semantics defines the rules for determining the truth of a sentence with respect to a particular model. In propositional logic, a model simply sets the truth value - true or false - for every proposition symbol. The semantics for propositional logic must specify how to compute the truth value of any sentence, given a model. This is done recursively. All sentences are constructed from atomic sentences and the five connectives;. Atomic sentences are easy: For complex sentences, we have five rules, which hold for any subsentences P and Q. (atomic or complex) in any model m. 7.4.3 A simple knowledge base. Constructing a knowledge base(based on Wumpus game): 7.4.4 A simple inference procedure. 7.5 Propositional Theorem Proving. Propositional logic. theorem proving: applying rules of inference directly to the sentences in our knowledge base to construct a proof of the desired sentence without consulting models. If the number of models is large but the length of the proof is short, then theorem proving can be more efficient than model checking. validity: A sentence is valid if it is true in all models, such as P and not P since it is always true by the LEM rule. satisfiability: A sentence is satisfiable if it is true in, or satisfied by, some model. 7.5.1 Inference and proofs. 7.5.2 Proof by resolution - INCOMPLETE. completeness: an algorithm is complete in the sense that they will find. any reachable goal, but if the available inference rules are inadequate, then the goal is not reachable—no proof exists that uses only those inference rules. complementary literals. (i.e., one is the negation of the other). 7.5.3 Horn clauses and definite clauses. definite clause. , which is a disjunction of literals of which. exactly one is positive. 7.6 Effective Propositional Model Checking. 7.6.1 A complete backtracking algorithm. Davis–Putnam algorithm(DPLL): takes a sentence and a set of clauses. It acts like a recursive depth first enumeration of possible models. Early Termination: The algorithm detects if the sentence is true or false. Pure symbol Heuristic: Symbols that always have the same variable symbol, like ‘A’ throughout all the clauses. Unit Clause Heuristic: a clause with just one literal. 7.6.2 Local search algorithms. 7.6.3 The landscape of random SAT problems. Given a source of random sentences, we can measure the probability of satisfiability. Binary Decision Diagrams. : 1 Boolean Expressions. Abstract syntax of boolean values. : when x ranges over a set of boolean variables. 2 Normal forms. DNF(Disjunctive normal form): a boolean expression which consists of a disjunction between two sets of conjunctions fx. (. a. ∧. b. ). ∨. (. c. ∧. d. ). (a \wedge b. ) \vee (c \wedge d. ). (. a. ∧. b. ). ∨. (. c. ∧. d. ). ﻿. The indexed version of writing this is: 3 Binary Decision Diagrams. Proofs and lemmas. Canonicity Lemma - There us exactly one ROBDD for any function. INF(If-then-else form): Checking if two boolean statements are the same: Construct and compare their ROBDD’s. If they are alike then the statements are the same. 4 Constructing and manipulating ROBDDs. Reducing OBDDs. Nodes are represented as 0, 1, 2. where 0 and 1 are terminal nodes. Variables in order. x. 1. <. x. 2. <. <. x. n. x_1 <x_2<. <x_n. x. 1. ​. <. x. 2. ​. <. <. x. n. ​. ﻿. are represented by their indices 1, 2,. , n. The ROBDD is stored in a table T : u (i, l, h) which maps a node u to its three attributes. i=var(u), l = low(u) and h = high(u). When the table above is generated we can compare the values for various nodes. If one or more has the same values then all but one are redundant and can be removed. Build ROBDDs algorithm. 4.3 Apply. algorithm. 5 Implementing the ROBDD operations. 6 Examples of problem solving with ROBDDs. 6.1 8-queens-problem. Essentially: check if for Arbitrary N: is it possible to place N queens on a N * N chess board without their diagonal, vertical or horizontal fields intersecting another queen. Code it using Boolean variables: Set each position on the board to be a variable, named as. x. i. x_i. x. i. ​. ﻿. j. _j. j. ​. ﻿. where i=row and j=column. Mark a variable as 1(true) when a queen is in the field. Express boolean expression: for X, no other queen is in diagonal fields, row, and column. There must be a queen in each row so for all i, it must be in. X. i. X_i. X. i. ​. ﻿. 1. ∨. _1 \vee. 1. ​. ∨. ﻿. X. i. X_i. X. i. ​. ﻿. 2. ∨. _2 \vee. 2. ​. ∨. ﻿. ﻿. ∨. X. i. \vee X_i. ∨. X. i. ​. ﻿. N. _N. N. ​. ﻿. Restrictions overview: 6.2 Correctness of Combinational Circuits. 6.3 Equivalence of Combinational Circuits. 4(4’th ed.) Search in Complex Environments (233-288). : 4.1 Local Search and Optimization Problems. Local search algorithms operate by searching from a start state to neighboring states, without keeping track of the paths, nor the set of states that have been reached so they are. not systematic. Biggest advantages: Use very littel memory. Can find solutions in large or infinite state spaces. Can solve optimization problems, by finding the best state according to an objective function. Hill climbing: searching for a global maximum. Gradient descent: searching for a global minimum. 4.1.1 Hill-climbing search algorithm - greedy local search. the algorithm keeps track of one current. state and on each iteration moves to the neighboring state with the highest value and terminates when it reaches a peak(no higher neighbour). It is greedy since it just grabs the current best solution and doesn’t think ahead. Disadvantages: May get stuck at one of the following: Local maxima - nowhere to go that is higher. Ridges - a sequence of local maxima. Plateaus - flatlines, nowhere is higher. Solutions: Allowing limited sidestepping(moving sideways on flat edges). Stochastic hill climbing: Chooses a random start position among the uphill positions and climbs to the enarest top. For each top it keeps a pointer to the largest and as such it returns the best result and is effective in large state-spaces. Random-restart hill climbing. : Conducts a series of searches from random initial states until a goal is found. Each process is run separately from the others. if there are few local maxima and plateaus, random-restart hill climbing will find a good solution very quickly. 4.1.2 Simulated annealing ////confused about this one. Steps: Random position is picked. Pick a random move, if the result is better than current, then accept the move. 4.1.3 Local Beam Search. Tracks an amount of states. Starts with random states and for each step it calculates the successors of all the states - if one of the outcomes is a goal then the alogirhtm is halted. The successors are calculated repeatedly until a goal is found. In a local beam search, useful information is passed among the parallel search threads to guide the thread in direction of better values and avoid searches in locations that are known to be poor. can suffer from a lack of diversity among the states—they can become. clustered in a small region of the state space, making the search little more than a -timesslower version of hill climbing. Stochastic beam search. instead of choosing the top successors, stochastic beam search chooses successors with probability proportional to the successor’s value, thus increasing diversity. 4.1.4 Evolutionary algorithms. Evolutionary algorithms can be seen as variants of stochastic beam search that are explicitly motivated by the metaphor of natural selection in biology.