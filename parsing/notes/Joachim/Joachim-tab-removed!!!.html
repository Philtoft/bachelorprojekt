<html>
 IAI notesIAI notes1. IntroductionSummaryThis chapter defines AI and establishes the cultural 									background against which it has developed. Some of the 									important points are as follows:Different people approach AI with different goals in mind. 										Two important questions to ask are: Are you concerned with 										thinking or behavior? Do you want to model humans or work 										from an ideal standard?In this book, we adopt the view that intelligence is 										concerned mainly with rational action. Ideally, an 										intelligent agent takes the best possible action in a 										situation. We study the problem of building agents that are 										intelligent in this sense.Philosophers (going back to 400 B.C.) made AI conceivable by 										considering the ideas that the mind is in some ways like a 										machine, that it operates on knowledge encoded in some 										internal language, and that thought can be used to choose 										what actions to take.Mathematicians provided the tools to manipulate statements 										of logical certainty as well as uncertain, probabilistic 										statements. They also set the groundwork for understanding 										computation and reasoning about algorithms.Economists formalized the problem of making decisions that 										maximize the expected outcome to the decision maker.Neuroscientists discovered some facts about how the brain 										works and the ways in which it is similar to and different 										from computers.Psychologists adopted the idea that humans and animals can 										be considered information processing machines.Linguists showed that language use fits into this model.Computer engineers provided the ever-more-powerful machines 										that make AI applications possible.Control theory deals with designing devices that act 										optimally on the basis of feedback from the environment. 										Initially, the mathematical tools of control theory were 										quite different from AI, but the fields are coming closer 										together.The history of AI has had cycles of success, misplaced 										optimism, and resulting cutbacks in enthusiasm and funding. 										There have also been cycles of introducing new creative 										approaches and systematically refining the best ones.AI has advanced more rapidly in the past decade because of 										greater use of the scientific method in experimenting with 										and comparing approaches.Recent progress in understanding the theoretical basis for 										intelligence has gone hand in hand with improvements in the 										capabilities of real systems. The subfields of AI have 										become more integrated, and AI has found common ground with 										other disciplines.1.1 What is AI1.1.1 Acting humanly: The Turing Test approachRequirements for passing the Turing test:natural language processingto enable 													it to communicate successfully in English;knowledge representationto store 													what it knows or hears;automated reasoningto use the stored 													information to answer questions and to draw new 													conclusions;machine learningto adapt to new 													circumstances and to detect and extrapolate patternsThe total Turing test:this form of the test also includes the subject to 												handle visual input and physical objects given by the 												interrogator. The computer would need:computer visionto perceive objectsroboticsto manipulate objects and 													move about1.1.2 Thinking humanly: The cognitive modeling approach1.1.3 Thinking rationally: The “laws of thought” 												approach1.1.4 Acting rationally: The rational agent approachA rational agent is one that acts so as to achieve the 												best outcome or, when there is uncertainty, the best 												expected outcome.The rational-agent approach has two advantages over the 												other approaches. First, it is more general than the 												“laws of thought” approach because correct inference is 												just one of several possible mechanisms for achieving 												rationality. Second, it is more amenable to scientific 												development than are approaches based on human behavior 												or human thought.1.2 The foundations of AI1.2.1 PhilosophyThe final element in the philosophical picture of the 												mind is the connection between knowledge and action. 												This question is vital to AI because intelligence 												requires action as well as reasoning. Moreover, only by 												understanding how actions are justified can we 												understand how to build an agent whose actions are 												justifiable (or rational).1.2.2 Mathematics1.2.3 Economics1.2.4 Neuroscience1.2.5 Psychology1.2.6 Computer engineering1.2.7 Control theory and cybernetics1.2.8 Linguistics1.3 The history of AI1.3.1 The gestation of artificial intelligence 												(1943–1955)1.3.2 The birth of artificial intelligence (1956)1.3.3 Early enthusiasm, great expectations (1952–1969)1.3.4 A dose of reality (1966–1973)1.3.5 Knowledge-based systems: The key to power? 												(1969–1979)1.3.6 AI becomes an industry (1980–present)1.3.7 The return of neural networks (1986–present)1.3.8 AI adopts the scientific method (1987–present)1.3.9 The emergence of intelligent agents (1995–present)1.3.10 The availability of very large data sets 												(2001–present)1.4 The state of the art2 Intelligent AgentsSummaryAnagentis something that perceives and 										acts in an environment. The agent function for an agent 										specifies the action taken by the agent in response to any 										percept sequence.Theperformance measureevaluates the 										behavior of the agent in an environment. A rational agent 										acts so as to maximize the expected value of the performance 										measure, given the percept sequence it has seen so far.Atask environment specificationincludes 										the performance measure, the external environment, the 										actuators, and the sensors. In designing an agent, the first 										step must always be to specify the task environment as fully 										as possible.Task environmentsvary along several 										significant dimensions. They can be fully or partially 										observable, single-agent or multiagent, deterministic or 										stochastic, episodic or sequential, static or dynamic, 										discrete or continuous, and known or unknown.The agent programimplements the agent 										function. There exists a variety of basic agent-program 										designs reflecting the kind of information made explicit and 										used in the decision process. The designs vary in 										efficiency, compactness, and flexibility. The appropriate 										design of the agent program depends on the nature of the 										environment.Simple reflex agentsrespond directly to 										percepts, whereasmodel-based reflex agentsmaintain internal 										state to track aspects of the world that are not evident in 										the current percept.Goal-based agentsact 										to achieve their goals, andutility-based agentstry to maximize their 										own expected “happiness.”All agents can improve their performance through learning.2.1 Agents and EnvironmentsAn agent is anything that can be viewed as perceiving its 									environment through sensors and acting upon that environment 									through actuators.percept sequence: the history of everything 									an agent has perceived(memories)agent function:An action mapped from a given 									input(percept) which result in a given action2.2 Good behaviour: the concept of rationalityA rational agent is one that does the right thing.Put in an environment an agent perceives it and creates a 									sequence of actions, if the actions are good as a whole then 									it has performed well.a performance measure:Evaluation of the 									desirability of a sequence of environment states.As a general rule, it is better to design performance measures 									according to what one actually wants in the environment, 									rather than according to how one thinks the agent should 									behave2.2.1 RationalityWhat is rational at any given time depends on four 												things: • The performance measure that defines the 												criterion of success. • The agent’s prior knowledge of 												the environment. • The actions that the agent can 												perform. • The agent’s percept sequence to date.definition of a rational agent:For 												each possible percept sequence, a rational agent should 												select an action that is expected to maximize its 												performance measure, given the evidence provided by the 												percept sequence and whatever built-in knowledge the 												agent hasAn agent can be called rational when its expected 												performance is at least as high as any other agent’s, 												meaning it already has optimal efficiency for its 												environment.2.2.2 Omniscience, learning and autonomyAn omniscient agent knows the actual outcome of its 												actions and can act accordingly; but omniscience is 												impossible in reality.Rationality maximizes expected performance, while 												perfection maximizes actual performance.information gathering: Doing actions in 												order to modify future percepts - essentially spending 												time to learn and improve its future performanceA priori:en environment which is 												completely knownAutonomous: Relies on its own percepts 												rather than the prior knowledge it is given - it thinks 												and reacts on its own2.3.1 specifying the task environmentTask environment(aka. PEAS (Performance, Environment, 													Actuators, Sensors)):The agent’s actuators and sensors.PEAS description: a description of the environment, 												agent actuators, performance measurement and sensors of 												the agent and its acting environment:Performance measures:Criterias for a 												rational and well performing agentEnvironment:the environment(s) in 												which the agent will operateActuators:Actions available to the 												agentSensors:Ways for the agent to perceive 												its environment2.3.2 Properties of task environmentsFully observable: The agent’s sensors 												gives it knowledge of the whole environment at all times 												and detects all relevant aspects for the choice of 												action. Agents need not keep track of its perceived 												environment as memory, since it is always available.Partially observable: The agent’s 												sensors gives it perception of a limited area at a time, 												fx. a roomba can only see right in front of it.Multiagent:Multiple agents affect each 												other in a given environment - often requires 												communication between agents for rational behaviourSingle agent:Only a single agent 												affects an environment, it is not affected by other 												agentsAgents can be competitive or cooperative, eg. chess AI 												or multiple Roombas in a room.Randomized behaviour is rational since is avoids 												predictability which can be negative.Deterministic:The next state of the 												environment is completely determined by the current 												state and the action executed by the agentStochastic:The next state of the 												environment is not determined by state and executed 												action of the agentUncertain environment:not fully 												observable or not deterministic environmentnondeterministic environment:Environments where actions are characterized by their 												possible outcomes, but no probabilities are attached to 												themEpisodic environment:Agents experience 												is divided into atomic episodes. In each episode the 												agent receives a percept and then performs a single 												action. the next episode does not depend on the actions 												taken in previous episodesSequential environment:the agent’s 												current decision could affect all future decisionsStatic environment:The environment is 												stable and does not change while the agent is 												deliberating its actionDynamic environment:The environment 												may change while the agent is deliberatingDiscrete environment:The time is based 												upon steps, like a turn based gameContinuous environment:Time is 												continuous and not affected by amount of actionsKnown/unknown environment:The agents 												knowledge of the laws of the environment such as 												physics. If it is unknown the agent must learn it by 												itself2.4.1 Agent programs2.4.2 Simple reflex agentsThese agents select actions on the basis of the current 												percept, ignoring the rest of the percept history.This type of agent is ruled bycondition–action rules:essentially if 												statements - If X happens then do YInfinite loops are often unavoidable for simple reflex 												agents operating in partially observable environments. 												Escape from infinite loops is possible if the agent can 												randomize its actions.In single-agent environments, however, randomization is 												usually not rational2.4.3 Model-based reflex agentsA Model-based agent: an agent that uses 												a model of the world(knowledge about how the environment 												works, such as traffic rules fx.)The most effective way to handle partial observability 												is for the agent to keep track of the part of the world 												it can’t see now. That is, the agent should maintain 												some sort ofinternal statethat 												depends on the percept history and thereby reflects at 												least some of the unobserved aspects of the current 												state2.4.4 Goal-based agentsthe agent needs some sort of goal information that 												describes situations that are desirable - fx. getting to 												a destination or an environmental state.Sometimes goal-based action selection is 												straightforward—for example, when goal satisfaction 												results immediately from a single action. Sometimes it 												will be more tricky—for example, when the agent has to 												consider long sequences of twists and turns in order to 												find a way to achieve the goal.Searchandplanningare subfields of AI devoted to 												finding action sequences that achieve the agent’s goals.Although the goal-based agent appears less efficient, it 												is more flexible because the knowledge that supports its 												decisions is represented explicitly and can be modified. 												If it starts to rain, the agent can update its knowledge 												of how effectively its brakes will operate; this will 												automatically cause all of the relevant behaviors to be 												altered to suit the new conditions.2.4.5 Utility-based agentsUtility:how efficient and well a goal 												is accomplished, eg. quickly, well, safely, cheap etc.utility function:An internalization of 												the performance measure.If the internal utility function and the external 												performance measure are in agreement, then an agent that 												chooses actions to maximize its utility will be rational 												according to the external performance measure.utility provides a way in which the likelihood of 												success can be weighed against the importance of the 												goals.a rational utility-based agent chooses the action that 												maximizes the expected utility of the action outcomes - 												that is, the utility the agent expects to derive, on 												average, given the probabilities and utilities of each 												outcome.any rational agent must behave as if it possesses a 												utility function whose expected value it tries to 												maximize.2.4.6 Learning agentsLearning allows the agent to operate in initially 												unknown environments and to become more competent than 												its initial knowledge alone might allow.learning element:responsible for 												making improvements. uses feedback from thecriticon how the agent is doing and 												determines how the performance element should be 												modified to do better in the future.performance element:responsible for 												selecting external actionsThe critic:tells the learning element 												how well the agent is doing with respect to a fixed 												performance standard. The critic is necessary because 												the percepts themselves provide no indication of the 												agent’s success.problem generator:responsible for 												suggesting actions that will lead to new and informative 												experiences - if the agent is willing to explore a 												little and do some perhaps suboptimal actions in the 												short run, it might discover much better actions for the 												long run2.4.7 How the components of agent programs workthe axis along which atomic, factored, and structured 												representations lie is the axis of increasing 												expressiveness. A more expressive representation can 												capture, at least as concisely, everything a less 												expressive one can capture, plus some more.atomic representation:each state of 												the world is indivisible with no internal structure.factored representation:splits up each 												state into a fixed set of variables or attributes each 												of which can have a value. two different factored states 												can share some attributes - like being in the same GPS 												location - and not others states - such as fuel level. 												this makes it much easier to work out how to turn one 												state into another. With factored representations, we 												can also represent uncertaintystructured representation:Objects and 												their various and varying relationships can be described 												explicitly.3 Solving problems by searchingSummarySearch:Before an agent can start searching for solutions, a goal 										must be identified and a well defined problem must be 										formulated.A problem consists of five parts:the initial statea set of actionsa transition model describing the results of those 												actions,a goal test function,a path cost function.The environment of the problem is represented by a state 										space. A path through the state space from the initial state 										to a goal state is a solution.Search algorithms treat states and actions as atomic: they 										do not consider any internal structure they might possess.A general TREE-SEARCH algorithm considers all possible paths 										to find a solution, whereas a GRAPH-SEARCH algorithm avoids 										consideration of redundant paths.Search algorithms are judged on the basis of completeness, 										optimality, time complexity, and space complexity. 										Complexity depends on b, the branching factor in the state 										space, and d, the depth of the shallowest solutionUninformed search methods have access only to the problem 										definition. The basic algorithms are as follows:Breadth-first search expands the shallowest nodes first; 												it is complete, optimal for unit step costs, but has 												exponential space complexity.Uniform-cost search expands the node with lowest path 												cost, g(n), and is optimal for general step costs.Depth-first search expands the deepest unexpanded node 												first. It is neither complete nor optimal, but has 												linear space complexity. Depth-limited search adds a 												depth bound.Iterative deepening search calls depth-first search with 												increasing depth limits until a goal is found. It is 												complete, optimal for unit step costs, has time 												complexity comparable to breadth-first search, and has 												linear space complexity.Bidirectional search can enormously reduce time 												complexity, but it is not always applicable and may 												require too much spaceInformed search methods may have access to a heuristic 										function h(n) that estimates the cost of a solution from n.The generic best-first search algorithm selects a node 												for expansion according to an evaluation function.Greedy best-first search expands nodes with minimal 												h(n). It is not optimal but is often efficient.A∗ search expands nodes with minimal f(n) = g(n) + h(n). 												A∗ is complete and optimal, provided that h(n) is 												admissible (for TREE-SEARCH) or consistent (for 												GRAPH-SEARCH). The space complexity of A∗ is still 												prohibitive.RBFS (recursive best-first search) and SMA∗ (simplified 												memory-bounded A∗) are robust, optimal search algorithms 												that use limited amounts of memory; given enough time, 												they can solve problems that A∗ cannot solve because it 												runs out of memoryThe performance of heuristic search algorithms depends on 										the quality of the heuristic function. One can sometimes 										construct good heuristics by relaxing the problem 										definition, by storing precomputed solution costs for 										subproblems in a pattern database, or by learning from 										experience with the problem class.Lecture NotesDiscrete states and actions: They are finite and countable, 									fx. 10 possible actions in a given stateState space:The initial state, a set of 									actions, and the transition model makes up the state space, 									essentially the route from a to bA solution is a path fromt he initial state s0 to the goal 									state gAn optimum solution is the path with minimal path costDifference between graph and tree search:Graph search has a set of explored nodesNodes are added to the set right before they are expanded in 										the graphIf a any children of the node are not in the explored or 										frontier set, then ass it to nthe frontierBig oh notation:Informed search:Best-first tree search:Maintain a queue in ascending f-value(lowest cost edge) for 									each node in the frontier.Best-first graph search:same as BFTS above + some moreMaintain a set of explored nodes and add as nodes are 										expanded.When expanding a node, add all children NOT IN Explored set, 										to the frontier set.Greedy best first search:Looks at the heuristic distance between the nodes in the 									frontier and the goal - aka. whatever is closest to the goal 									like A*. It then expands the closest node until the goal is 									reached. This is not optimal but it is quick. Also risks 									getting stuck on a dead end graph where there is a node close 									to the goal, but not connected to itA*Considers both heuristic distance from nodes to goal as well 									as the cost of reaching said nodes.3.1 Problem-Solving AgentsProblem formulationis the process of 									deciding what actions and states to consider, given a goal.In general, an agent with several immediate options of unknown 									value can decide what to do by first examining future actions 									that eventually lead to states of known valueAsearchalgorithm takes a problem as input 									and returns a solution in the form of an action sequence. Once 									a solution is found, the actions it recommends can be carried 									out. This is called the execution phase. While the agent is 									executing the solution sequence is in aopen-loop system- meaning it ignores its 									percepts when choosing an action because it knows in advance 									what they will be and breaks the loop between agent and 									environment.3.1.1 Well-defined problems and solutionsA problem can be defined formally by five components:The initial statethat the agent 													starts in.Adescription of the possible actions available to 														the agent. Given a particular state s, ACTIONS(s) returns the 													set of actions that can be executed in s. We say that 													each of these actions is applicable in s.Adescription of what each action does; 													the formal name for this is the transition model, 													specified by a function RESULT(s, a) that returns the 													state that results from doing action a in state s. We 													also use the term successor to refer to any state 													reachable from a given state by a single actionThegoal test, which determines 													whether a given state is a goal state. Sometimes there 													is an explicit set of possible goal states, and the 													test simply checks whether the given state is one of 													them.Apath cost functionthat assigns a 													numeric cost to each path. The problem-solving agent 													chooses a cost function that reflects its own 													performance measurestate space: defined by the initial 												state, actions, and transition modelA solution to a problem is an action sequence that leads 												from the initial state to a goal state. Solution quality 												is measured by the path cost function, and an optimal 												solution has the lowest path cost among all solutions.3.1.2 Formulating problemsAbstraction:The process of removing 												detail from a representationThe choice of a good abstraction thus involves removing 												as much detail as possible while retaining validity and 												ensuring that the abstract actions are easy to carry 												out. Were it not for the ability to construct useful 												abstractions, intelligent agents would be completely 												swamped by the real world3.2 Example Problemstoy problem:intended to illustrate or 									exercise various problem-solving methodsreal-world problem:one whose solutions 									people care about3.2.1 Toy problems3.2.2 Real-World problems3.3 Searching for solutionssearch algorithms work by considering various possible action 									sequences.The possible action sequences starting at the initial state 									form asearch treewith the initial state at 									the root; the branches are actions and the nodes correspond to 									states in the state space of the problem.expanding the current state:applying each 									legal action to the current state, thereby generating a new 									set of states.Frontier:the set of available leaf nodes in 									the tree at a given time3.3.1 Infrastructure for search algorithmsSearch algorithms require a data structure to keep track 												of the search tree that is being constructed. For each 												node n of the tree, we have a structure that contains 												four components:n.STATE: the state in the state space to which the 													node corresponds;n.PARENT: the node in the search tree that generated 													this node;n.ACTION: the action that was applied to the parent to 													generate the node;n.PATH-COST: the cost, traditionally denoted by g(n), 													of the path from the initial state to the node, as 													indicated by the parent pointers3.3.2 Measuring problem-solving performanceWe can evaluate an algorithm’s performance in four ways:Completeness: Is the algorithm 													guaranteed to find a solution when there is one?Optimality: Does the strategy find 													the optimal solution, as defined on page 68?Time complexity: How long does it 													take to find a solution?Space complexity: How much memory is 													needed to perform the search?complexityis expressed in terms of 												three quantities: b, the branching factor or maximum 												number of successors of any node; d, the depth of the 												shallowest goal node (i.e., the number of steps along 												the path from the root); and m, the maximum length of 												any path in the state space.Timeis often measured in terms of the 												number of nodes generated during the search, and space 												in terms of the maximum number of nodes stored in memorywe describetime and space complexityfor search on 												a tree; for a graph, the answer depends on how 												“redundant” the paths in the state space are.To assess the effectiveness of a search algorithm, we 												can consider just the search cost, which typically 												depends on the time complexity but can also include a 												term for memory usage or we can use the total cost, 												which combines the search cost and the path cost of the 												solution found.3.4 Uninformed Search Strategiesexponential-complexity search problems cannot be solved by 									uninformed methods for any but the smallest instances.3.4.1 Breadth-first searchthe root node is expanded first, then all the successors 												of the root node are expanded next, then their 												successors, and so on.all the nodes are expanded at a given depth in the 												search tree before any nodes at the next level are 												expanded.Breadth-first search is an instance of the general 												graph-search algorithm in which the shallowest 												unexpanded node is chosen for expansion. This is 												achieved very simply by using a FIFO queue for the 												frontier.breadth-first search always has the shallowest path to 												every node on the frontier.BFS evaluation:Complete: if the shallowest goal node 													is at some finite depth d, breadth-first search will 													eventually find it after generating all shallower 													nodesOptimal:if the path cost is a 													nondecreasing function of the depth of the node.Poor time complexity: it scales 													horribly as trees become deeperPoorspace complexity:every node 													generated remains in memory. There will be O(b^d−1) 													nodes in the explored set and O(b^d) nodes in the 													frontier, so the space complexity is O(b^d)3.4.2 Uniform-cost searchuniform-cost search expands the node n with the lowest 												path cost g(n). This is done by storing the frontier as 												a priority queue ordered by g.the goal test is applied to a node when it is selected 												for expansion.a test is added in case a better path is found to a node 												currently on the frontier - if there is a better path 												then the old path is discarded and the new is added 												instead.Evaluation:Completenessis guaranteed provided 													the cost of every step exceeds some small positive 													constant N.worst-case time and space complexity is:When all step costs are the same, uniform-cost search 													is similar to breadth-first search, except that the 													latter stops as soon as it generates a goal, whereas 													uniform-cost search examines all the nodes at the 													goal’s depth to see if one has a lower cost; thus 													uniform-cost search does strictly more work by 													expanding nodes at depth d unnecessarily.Uniform-cost search does not care about the number of 													steps a path has, but only about their total cost. 													Therefore, it will get stuck in an infinite loop if 													there is a path with an infinite sequence of zero-cost 													actions.3.4.3 Depth-first searchDepth-first search always expands the deepest node in 												the current frontier of the search tree.The search proceeds immediately to the deepest level of 												the search tree, where the nodes have no successors. As 												those nodes are expanded, they are dropped from the 												frontier, so then the search “backs up” to the next 												deepest node that still has unexplored successors.depth-first search uses a LIFO queue. The most recently 												generated node is chosen for expansion. This must be the 												deepest unexpanded node because it is one deeper than 												its parent—which, in turn, was the deepest unexpanded 												node when it was selected.Evaluation:graph-search version iscomplete, 													which avoids repeated states and redundant paths, is 													complete in finite state spaces because it will 													eventually expand every node.tree-search version isnot complete:In infinite state spaces, both versions fail if an 													infinite non-goal path is encounteredDoes not stop when the goal node is found previously 													in a tree-searchThe time complexity of depth-first graph search is 													bounded by the size of the state spaceA depth-first tree search may generate all of the 													O(b^m) nodes in the search tree, where m is the 													maximum depth of any node;Good space complexity: a depth-first 													tree search needs to store only a single path from the 													root to a leaf node, along with the remaining 													unexpanded sibling nodes for each node on the path. 													Once a node has been expanded, it can be removed from 													memory as soon as all its descendants have been fully 													explored.An improved variant of depth-first search calledbacktracking searchuses still less 												memory.3.4.4 Depth-limited searchdepth-first search in infinite state spaces can be 												improved by supplying depth-first search with a 												predetermined depth limit L.nodes at depth L are treated as if they have no 												successors.Unfortunately, it also introduces an additional source 												of incompleteness if we choose L&lt;d, that is, the 												shallowest goal is beyond the depth limit.Depth-limited search will also be nonoptimal if we 												choose L&gt;d.3.4.5 Iterative deepening depth-first searchIterative deepening search is a general strategy, often 												used in combination with depth-first tree search, that 												finds the best depth limit. by gradually increasing the 												limit—first 0, then 1 and so on.3.4.6 Bidirectional searchThe idea behind bidirectional search is to run two 												simultaneous searches—one forward from the initial state 												and the other backward from the goal—hoping that the two 												searches meet in the middleBidirectional search is implemented by replacing the 												goal test with a check to see whether the frontiers of 												the two searches intersect; if they do, a solution has 												been found.The check can be done when each node is generated or 												selected for expansion and, with a hash table, will take 												constant time.The space complexity is also O(b^d/2). We can reduce 												this by roughly half if one of the two searches is done 												by iterative deepening, but at least one of the 												frontiers must be kept in memory so that the 												intersection check can be done. This space requirement 												is the most significant weakness of bidirectional 												search.3.4.7 Comparing uninformed search strategies3.5 Informed(Heuristic) search strategiesinformed search strategy:Strategy that uses 									problem-specific knowledge beyond the definition of the 									problem itself to solve the problem3.5.1 Greedy best-first searchGreedy best-first search tries to expand the node that 												is closest to the goal, on the grounds that this is 												likely to lead to a solution quickly.it evaluates nodes by using just the heuristic function; 												that is, f(n) = h(n).the algorithm is called “greedy” because at each step it 												tries to get as close to the goal as it can.Greedy best-first tree search is also incomplete even in 												a finite state space, much like depth-first search.The graph search version is complete in finite spaces, 												but not in infinite ones.The worst-case time and space complexity for the tree 												version isO(bm)O(b^m)O(bm)﻿, where m is the maximum depth of the search space.With a good heuristic function, however, the complexity 												can be reduced substantially. The amount of the 												reduction depends on the particular problem and on the 												quality of the heuristic3.5.2 A* search: Minimizing the total estimated solution 												costevaluates nodes by combining g(n), the cost to reach the 												node, and h(n), the cost to get from the node to the 												goal: f(n) = g(n) + h(n) .Since g(n) gives the path cost from the start node to 												node n, and h(n) is the estimated cost of the cheapest 												path from n to the goal, we have f(n) = estimated cost 												of the cheapest solution through na reasonable thing to try first is the node with the 												lowest value of g(n) + h(n). It turns out that this 												strategy is more than just reasonable: provided that the 												heuristic function h(n) satisfies certain conditions, A∗ 												search is both complete and optimal. The algorithm is 												identical to UNIFORM-COST-SEARCH except that A∗ uses g + 												h instead of g.Conditions for optimality: Admissibility and 													consistencyThe first condition we require for optimality is that 												h(n) be an admissible heuristic: aka. one that never 												overestimates the cost to reach the goal.Because g(n) is the actual cost to reach n along the 												current path, and f(n) = g(n) + h(n), we have as an 												immediate consequence that f(n) never overestimates the 												true cost of a solution along the current path through nA second, slightly stronger condition called consistency 												(or sometimes monotonicity) is required only for 												applications of A∗ to graph search.A heuristic h(n) is consistent if, for every node n and 												every successor n of n generated by any action a, the 												estimated cost of reaching the goal from n is no greater 												than the step cost of getting to n plus the estimated 												cost of reaching the goal from n: h(n) ≤ c(n, a, n) + 												h(n)A∗ has the following properties:the tree-search version of A∗ is optimal if h(n) is 													admissiblethe graph-search version is optimal if h(n) is 													consistentCompleteness requires that there be only finitely many 													nodes with cost less than or equal to C∗, a condition 													that is true if all step costs exceed some finite e 													and if b is finite.pruning:eliminating possibilities from 												consideration without having to examine themA∗ usually runs out of space before time because it 												keeps all generated nodes in memory. For this reason, A∗ 												is not practical for many large-scale problems.3.5.3 Memory-bounded heuristic search - RBFS+SMA*iterative-deepening A*:Adapt the idea 												of iterative deepening to the heuristic search context 												to reduce memory requirements.The main difference between IDA∗ and standard iterative 												deepening is that the cutoff used is the f-cost (g +h) 												rather than the depth; at each iteration, the cutoff 												value is the smallest f-cost of any node that exceeded 												the cutoff on the previous iteration.IDA∗ is practical for many problems with unit step costs 												and avoids the substantial overhead associated with 												keeping a sorted queue of nodes. Unfortunately, it 												suffers from the same difficulties with real valued 												costs as does the iterative version of uniform-cost 												searchRecursive best-first search:a simple 												recursive algorithm that attempts to mimic the operation 												of standard best-first search, but using only linear 												space.Its structure is similar to that of a recursive 												depth-first search, but rather than continuing 												indefinitely down the current path, it uses the f limit 												variable to keep track of the f-value of the best 												alternative path available from any ancestor of the 												current node. If the current node exceeds this limit, 												the recursion unwinds back to the alternative path. As 												the recursion unwinds, RBFS replaces the f-value of each 												node along the path with a backed-up value, the best 												f-value of its children. In this way, RBFS remembers the 												f-value of the best leaf in the forgotten subtree and 												can therefore decide whether it’s worth re-expanding the 												subtree at some later time.Evaluation of RBFSRBFS is an optimal algorithm if the heuristic function 												h(n) is admissible. Its space complexity is linear in 												the depth of the deepest optimal solution, but its time 												complexity is rather difficult to characterize: it 												depends both on the accuracy of the heuristic function 												and on how often the best path changes as nodes are 												expanded.SMA*SMA∗ proceeds just like A∗, expanding the best leaf 												until memory is full. At this point, it cannot add a new 												node to the search tree without dropping an old one. 												SMA∗ always drops the worst leaf node—the one with the 												highest f-value.SMA∗ then backs up the value of the forgotten node to 												its parent. In this way, the ancestor of a forgotten 												subtree knows the quality of the best path in that 												subtree. With this information, SMA∗ regenerates the 												subtree only when all other paths have been shown to 												look worse than the path it has forgotten.To avoid selecting the same node for deletion and 												expansion, SMA∗ expands the newest best leaf and deletes 												the oldest worst leaf. These coincide when there is only 												one leaf, but in that case, the current search tree must 												be a single path from root to leaf that fills all of 												memory. If the leaf is not a goal node, then even if it 												is on an optimal solution path, that solution is not 												reachable with the available memory. Therefore, the node 												can be discarded exactly as if it had no successors.SMA* Evaluation:SMA∗ is complete if there is any reachable solution(if 													d, the depth of the shallowest goal node, is less than 													the memory size)It is optimal if any optimal solution is reachable; 													otherwise, it returns the best reachable solution.fairly robust choice for finding optimal solutions, 													particularly when the state space is a graph, step 													costs are not uniform, and node generation is 													expensive compared to the overhead of maintaining the 													frontier and the explored set.3.6 Heuristic Functions3.6.1 The effect of heuristic accuracy on performanceOne way to characterize the quality of a heuristic is 												the effective branching factor b∗. If thetotal number of nodes generated by A∗ for a particular 												problem is N and the solution depth is d, then b∗ is the 												branching factor that a uniform tree of depth d would 												have to have in order to contain N + 1 nodes. Thus,if A∗ finds a solution at depth 5 using 52 nodes, then 												the effective branching factor is 1.92. The effective 												branching factor can vary across problem instances, but 												usually it is fairly constant for sufficiently hard 												problems.3.6.2 Generating admissible heuristics from relaxed 												problemsA problem with fewer restrictions on the actions is 												called arelaxed problem.e the removal of restrictions creates added edges in the 												graph. Because the relaxed problem adds edges to the 												state space, any optimal solution in the original 												problem is, by definition, also a solution in the 												relaxed problem; but the relaxed problem may have better 												solutions if the added edges provide short cuts. Hence, 												the cost of an optimal solution to a relaxed problem is 												an admissible heuristic for the original problem.because the derived heuristic is an exact cost for the 												relaxed problem, it must obey the triangle inequality 												and is therefore consistent.3.6.3 Generating admissible heuristics from subproblems: 												Pattern databasesAdmissible heuristics can also be derived from the 												solution cost of a subproblem of a given problem. fx. an 												8-puzzle where we simply solve it for 4 of the numbers, 												eg. 1, 2, 3, 4. In order to solve it completely we will 												always require more moves than for the subproblem, as 												such it is a valid heuristic.It turns out to be more accurate than Manhattan distance 												in some cases.The idea behind pattern databases is to store these 												exact solution costs for every possible subproblem 												instance—in our example, every possible configuration of 												the four tiles and the blank.The database itself is constructed by searching back13 												from the goal and recording the cost of each new pattern 												encountered; the expense of this search is amortized 												over many subsequent problem instances. The choice of 												1-2-3-4 is fairly arbitrary; we could also construct 												databases for 5-6-7-8, for 2-4-6-8, and so on. Each 												database yields an admissible heuristic, and these 												heuristics can be combined, as explained earlier, by 												taking the maximum value.3.6.4 Learning heuristics from experience5 Adversarial Search and GamesQuestionsPage 308, figure 5.5, stage d+e - how does A-B pruning know 										that the middle tree does not contain larger values than 2 										when it only check the first node? -Add answer to section 										5.2.3 of notesFor suboptimalSummaryA game can be defined by the initial state (how the board is 										set up), the legal actions in each state, the result of each 										action, a terminal test (which says when the game is over), 										and a utility function that applies to terminal states to 										say who won and what the final score is.In two-player, discrete, deterministic, turn-taking zero-sum 										games with perfect information, the minimax algorithm can 										select optimal moves by a depth-first enumeration of the 										game tree.The alpha–beta search algorithm computes the same optimal 										move as minimax, but achieves much greater efficiency by 										eliminating subtrees that are provably irrelevant.Usually, it is not feasible to consider the whole game tree 										(even with alpha–beta), so we need to cut the search off at 										some point and apply a heuristic evaluation function that 										estimates the utility of a state.An alternative called Monte Carlo tree search (MCTS) 										evaluates states not by applying a heuristic function, but 										by playing out the game all the way to the end and using the 										rules of the game to see who won. Since the moves chosen 										during the playout may not have been optimal moves, the 										process is repeated multiple times and the evaluation is an 										average of the results.Many game programs precompute tables of best moves in the 										opening and endgame so that they can look up a move rather 										than search.Games of chance can be handled by expectiminimax, an 										extension to the minimax algorithm that evaluates a chance 										node by taking the average utility of all its children, 										weighted by the probability of each child.In games of imperfect information, such as Kriegspiel and 										poker, optimal play requires reasoning about the current and 										future belief states of each player. A simple approximation 										can be obtained by averaging the value of an action over 										each possible configuration of missing information.Programs have soundly defeated champion human players at 										chess, checkers, Othello, Go, poker, and many other games. 										Humans retain the edge in a few games of imperfect 										information, such as bridge and Kriegspiel. In video games 										such as StarCraft and Dota 2, programs are competitive with 										human experts, but part of their success may be due to their 										ability to perform many actions very quicklyLecture NotesDeterministic - all possible moves are known and there are no 									chance elementsMiniMaxTimeComplexity is poor, but space complexity is good since we 									only keep the current layer and path in memoryThree stances to multi-agent environments:As an economy: Regarding agents as a whole and predicting what 							influence a change in the environment would have on them in 							generalPart of the environment: Considered a part of the environment, 							having no intended effect on us, but can still pose an unintended 							effect, such as wind, it sucks but it doesn’t intend to.Adversaries: Equiped with adversarial game-tree search techniques 							to actively defeat an opponent.Adversarial game-tree: A tree showing heuristic values for various 						steps one can make in a game. It shows some expected values based on 						given evaluation criterias such as captured pieces in a chess game.5.1.1 Two-player zero-sum gamesAka. deterministic, two-player, turn-taking, perfect 									information(fully observable), zero-sum games(a move is good 									for one player and bad for the other).Move:synonym for ActionPosition:synonym for stateGame defined as follows:S0: The initial state, which specifies how the game is set 										up at the start.TO-MOVE : The player whose turn it is to move in state.ACTIONS : The set of legal moves in state.RESULT : The transition model, which defines the state 										resulting from taking action in state.IS-TERMINAL(s): A terminal test, which is true when the game 										is over and false otherwise. States where the game has ended 										are called terminal states.UTILITY(s, p): A utility function (also called an objective 										function or payoff function), which defines the final 										numeric value to player when the game ends in terminal state 										s. In chess fx., the outcome is a win, loss, or draw, with 										values 1, 0, or 1/2complete game tree:a tree covering all 									sequences of moves to a terminal state - may be infinite if 									not bounded.Patial game-tree:shows part(s) of all 									sequences to a terminal state5.2.0 Optimal Decisions in GamesMinimax search:two adversaries max and min fight to get a value as high or 									low as possible. Min wants the lowest value and max vice 									versa.ply:A single move made by a single playeroptimal strategycan be determined by working 									out theminimax valueof each state in the 									tree, which we write as MINIMAX(s).The minimax value is theutility(for MAX) of 									being in that state, assuming that both players play optimally 									from there to the end of the game.On Max’s turn it picks highest value, and Min picks lowest. If 									it is a leaf it simply has its own value, so the evaluation of 									a given points minimax value is:5.2.1 The Minimax search algorithmMinimax creates a search tree of game values, such that we can 									find the best route to a certain victory.It is a recursive algorithm that proceeds all the way down to 									the leaves of the tree and then backs up the minimax values 									through the tree as the recursion unwindsPseudo codePerformance:Performs a complete depth-first exploration of the game tree. 									If the maximum depth of the tree is m and there are legal 									moves at each point, then the time complexity of the minimax 									algorithm isO(bm)O(b^m)O(bm)﻿. The space complexity isO(bm)O(bm)O(bm)﻿for an algorithm that generates all actions at once, orO(m)O(m)O(m)﻿for an algorithm that generates actions one at a time5.2.2 Optimal decisions in multiplayer gamesHow to evaluate 2+ players:1) replace the single value for each node with a vector of 									values. Fx. three-player game with players A,B and C =(vA,vB,vC)(v_A ,v_B ,v_C)(vA​,vB​,vC​)﻿as each node.In terminal states, the values in the vector are from each 									player’s respective view fx.:(va=5,vb=2,vc=2)(v_a = 5, v_b = 2, v_c = 2)(va​=5,vb​=2,vc​=2)﻿.2) Consider the nonterminal states: for each non-terminal 									node, the player will choose the option which is best for 									itself.So the backed-up value of a node is the utility vector of the 									successor state with the highest value for the player choosing 									at.AlliancesIf two players A, B are weaker than player C, they may work 									together to avoid being destroyed by C. This is still a 									selfish action to save themselves. Once C is weak, they can 									resume fighting.Reliability and gain from breaking alliances must be evaluated 									too. Explained in chapter 18 of the book.5.2.3 Alpha-Beta PruningPruning is essentially removing non-relevant sub-trees from 									the search queue to reduce search time.Essentially, if we have several states to choose from and the 									given Max or Min player has to choose, then the states which 									they will definitely not pick does not need to be searched. 									Fx. max choosing between subtrees of values 7, 3 and 2, would 									never choose 3 or 2 so why search them. In order to find the 									trees value do following:Evaluate the left subtreeEvaluate the leftmost Node(only one) in each sub-tree. If 										the value is higher or lower(the opposite of what a player 										want, since we’re examining the opponents options)Pseudo CodeThe general principle is this: consider a node n somewhere in 									the tree such that Player has a choice of moving to n. If 									Player has a better choice either at the same level or at any 									point higher up in the tree then Player will never move to n 									So once we have found out enough about (by examining some of 									its descendants) to reach this conclusion, we can prune it.Values of Alpha and Beta:5.2.4 Move Orderingeffectiveness of alpha–beta pruning is highly dependent on the 									order in which the states are examined.If the first leaf in a subtree is a bad representation of the 									subtree then it evaluates poorly.As such it might be worthwhile to try to first examine the 									successors that are likely to be best.Evaluation:Iterative deepening:search one ply deep and record the ranking of moves based on 										their evaluations.search one ply deeper, using the previous ranking to inform 										move orderingrepeat till last plyKiller move heuristic :always selecting the 									best moves possible.Transposition:Different permutations of the 									move sequence so the same state arrives multiple times, like 									moving a piece back and forth continuously. It has a very 									negative effect on searches as we essentially have to search 									the same subtree multiple timesTransposition table:caches the heuristic 									value of states. If the same state is encountered again then 									the value is simply fetched from the table rather than 									searching the subtree again.Itallows us to double our search depthTwo strategies for large problems:Type A strategy: considers all possible 										moves to a certain depth in the search tree, and then uses a 										heuristic evaluation function to estimate the utility of 										states at that depth. It explores a wide but shallow portion 										of the tree.Type B strategy: ignores moves that look 										bad, and follows promising lines “as far as possible.” It 										explores a deep but narrow portion of the tree5.3 Heuristic Alpha–Beta Tree Searchheuristic evaluation function to states:treating nonterminal nodes as if they were terminal by 									replacing the Utility function with Eval which estimates a 									state’s utility. The terminal test is also replaced by a 									cutoff test, which returns true for terminal states, but can 									otherwise cut the search anytime based on search depth and 									state properties.The formula for H-Minimac(s, d) for the heuristic minimax 									value of state s at search depth d, is as follows:5.3.1 Evaluation functionsA heuristic evaluation function returns an estimate of 												the expected utility of state to player , just as the 												heuristic functions(like A*) return an estimate of the 												distance to the goal.For terminal states it must be thatEval(s,p)=Utility(s,p)Eval(s, p) = Utility(s, p)Eval(s,p)=Utility(s,p)﻿and for nonterminal states the evaluation must be 												somewhere between a loss and a win:UTILITY(loss,p)≤EVAL(s,p)≤UTILITY(win,p)UTILITY(loss, p) ≤ EVAL (s, p) ≤ 																			UTILITY(win, p)UTILITY(loss,p)≤EVAL(s,p)≤UTILITY(win,p)﻿A good evaluation function:Has a quick computationIs Strongly correlated to chances of winningEssentially, through heuristics we estimate the chance 												of winning based on criterias such as how many possible 												states following the current would lead to a 												loss/win/tieThis is, however, hard to calculate so most evaluation 												functions compute separate numerical contributions from 												each feature and then combine them to find the total 												value, a so calledweighted linear functionf indicates a given value, such as amount of pawns, and 												w is their weight, eg. how valuable they are in the 												evaluation. So by counting and weighing all pieces on a 												chess board we can evaluate how well a player is doing.the evaluation function should be strongly correlated 												with the actual chances of winning, but it need not be 												linearly correlated: if state s is twice as likely to 												win as state s’ we don’t require that EVAL(S) be twice 												EVAL(S'); all we require is that 												EVAL(s)&gt;EVAL(s’).In games where human experience is not available, the 												weights of the evaluation function(like, weight of chess 												pieces) can be estimated by machine learning techniques5.3.2 Cutting off searchSimple explanation of modification to implement it:Replace IS-TERMINAL with IS-CUTOFF(state, depth)Let IS-CUTOFF return true for depths deeper a given 													depth d.Add a transposition table to improve search timemodifying ALPHA-BETA-SEARCH so that it will call the 												heuristic EVAL function when it is appropriate to cut 												off the search.replace the two lines that mention IS-TERMINAL with the 												following line:if game.IS-CUTOFF(state, depth) then return 													game.EVAL(state, player), nullThen set a fixed depth limit so that IS-CUTOFF(state, 												depth) returns true for all depth greater than some 												fixed depth (as well as for all terminal states). The 												depth is chosen so that a move is selected within the 												allocated time. A more robust approach is to apply 												iterative deepening.in each round of iterative deepening we keep entries in 												the transposition table, subsequent rounds will be 												faster, and we can use the evaluations to improve move 												orderingThese simple approaches can lead to errors due to the 												approximate nature of the evaluation function. It cannot 												look ahead and is therefore vulnerable.The evaluation function should be applied only to 												positions that arequiescent—that is, 												positions in which there is no pending move that 												wouldwildly swing the evaluation.For nonquiescent positions the IS-CUTOFF returns false, 												and the search continues until quiescent positions are 												reached. This extra quiescence search is sometimes 												restricted to consider only certain types of moves, such 												as capture moves, that will quickly resolve the 												uncertainties in the position.horizon effect:when facing an 												opponent’s move that causes serious damage and is 												ultimately unavoidable, but is temporarily avoided by 												the use of delaying tacticssingular extensions:Moves that are 												better than all other available moves.5.3.3 Forward Pruningforward pruning(a Type B strategy) saves computation 												time, but risk failure, by pruning moves that appear to 												be poor moves, but might possibly be good ones.beam search:is an example of forward 												pruning. on each ply, consider only a “beam” of the best 												moves (according to the evaluation function) rather than 												considering n all possible moves. Unfortunately, this 												approach is rather dangerous because there is no 												guarantee that the best move will not be pruned away.PROBCUT:PROBCUT(probabilistic cut): forward-pruning version of 												alpha–beta search that uses statistics gained from prior 												experience to lessen the chance that the best move will 												be pruned. It prunes nodes both provably AND probably 												outside the window of the current state(A, B).It searches as follows:Shallow search to compute backed-up value v of a node.Applies past experience to estimate how likely it is 													that a score of v at depth d in the tree would be 													outside(A, B).Late move reductionAssumes that move ordering is done well, and that the 												first moves in the ordering are likely the best ones. 												The last ones are not pruned, but their search depth is 												reduced - If it returns a better value it is then 												searched full-depth.5.3.4 Search versus lookupSome states may have data available and then a lookup of 												moves from a database is better than a search. However, 												as states become more uncommon, data is limited and a 												search is more efficient.retrograde minimax search: Essentially reverse search. 												Consider all possible end states and then compute moves 												backwards until you reach the current state. As such you 												will end up with a winning path.5.4 Monte Carlo Tree Search(MCTS)Pseudo CodeMCTS does not use heuristics, the value of a state is 									estimated as the average utility over a number of simulations 									of complete games starting from the state.A 									simulation(Playout/rollout) 									chooses moves for each player repeatedly, until reaching a 									terminal position.Playout policy:Used to guide the playout in 									a meaningful selection of moves. Game-specific heuristics can 									be used, such as “consider capture moves” in chess.Pure Monte Carlo Search:do N simulations 									starting from the current state and track which moves has the 									highest win percentage.Selection policy:focuses the computational 									resources on the important parts of the game tree by using 2 									factors:Exploration of states which have had few 										playouts(simulations)Exploitation of states that have done well previously to 										evaluate their valueThe MCTS does all above by maintaining a search tree and on 									each iteration it does:SELECTION: Go from root towards a leaf by using the 										selection policy.EXPANSION: Grow the search tree by generating a new child of 										the selected node.SIMULATION: Perform a playout from the newly generated child 										using the playout policy.BACK-PROPAGATION: Use child simulation results to update 										search tree nodes going up to the root.Steps are repeated for a given amount of iterations or within 									a time limit.upper confidence bounds applied to trees(UCT) 										policyThe policy ranks each possible move based on an upper 									confidence bound formula called UCB1:For a node , the formula is:Where U(n) is the total utility of all playouts through node 									n, N(n) is number of playout through n and PARENT(n) is the 									parent of n.Theaverage utility of n(the exploitation 									term) isU(n)/N(n)U(n)/N(n)U(n)/N(n)﻿The term with the square root is the exploration term: it has 									the count in the denominator, which means the term will be 									high for nodes that have only been explored a few times. In 									the numerator it has the log of the number of times we have 									explored the parent of .C is a constant that balances exploitation and exploration. C 									may be square root of 2 or generated by neural networks for 									max efficiency.EvaluationComputing a playout = linear in the depth of the treeMonte Carlo simulation is usually better when the branching 									factor is very high (wherealpha–beta can’t search deep enough) or when it is difficult 									to define a good evaluation function.MCTS relies on aggregate values rather than computation and 									does not suffer from miscalculation errors.MCTS can be joined with evaluation functions by running the 									simulation and then truncating the playout and applying an 									evaluation function.MCTS can adopt aspects of Alpha-beta such as early playout 									termination(by stopping too long playouts, and using a 									heuristic evaluation function instead)It can be applied to games with no previous data as it does 									not rely on it.Negative factors of MCTSpruning in Monte Carlo search means that a vital line of play 									might not be explored at all since the stochastic nature of 									Monte Carlo search means it might fail to consider it.Cannot quickly conclude “obvious” win playouts as it still 									needs multiple simulations to conclude anything.5.5 Stochastic Games(Chance)(ExpectiMiniMax)Includes a random element such as the throw of a dice.we can only calculate theexpected valueof a 									position: the average over all possible outcomes of the chance 									nodes.theexpectiminimaxvalue for games with 									chance nodes, a generalization of the minimax value for 									deterministic games.Terminal nodes and MAX and MIN nodes work exactly the same way 									as before.For chance nodes we compute the expected value, which is the 									sum of the value over all outcomes, weighted by the 									probability of each chance action:5.5.1 Evaluation functions for games of chancethe obvious approximation to make with expectiminimax is 												to cut the search off at some point and apply an 												evaluation function to each leaf.the presence of chance nodes means that one has to be 												more careful about what the values mean.the evaluation function must return values that are a 												positive linear transformation of the probability of 												winning (or of the expected utility, for games that have 												outcomes other than win/lose).alpha–beta pruning can be applied to game trees with 												chance nodes. The analysis for MIN and MAX nodes is 												unchanged, but we can also prune chance nodes, using a 												bit of ingenuity. if we put bounds on the possible 												values of the utility function, then we can arrive at 												bounds for the average without looking at every number.5.6 Partially Observable Games !!!INCOMPLETE FROM 5.6.2!!!Partial observability: Parts of the environment are unknows 									such as adversaries’ piece locations in a game.5.7 Limitations of Game Search Algorithmsall the algorithms must make some assumptions and 									approximations:Alpha–beta:search uses the heuristic evaluation function as an 										approximation.Vulnerable to calculation errors which ruins the treeDifficult to compensate for errors in evaluation functions 										because we don’t have a good model of the dependencies 										between the values of sibling nodes.Designed to calculate values of legal moves - when there is 										only one available move it will still searchReasons on the level of individual moves - cannot abstract 										and see a higher goal such as trapping a chess piece but not 										capturing it.Poorly incorporates machine learning: Early game programs 										relied on human expertise to hand-craft evaluation 										functions, opening books, search strategies, and efficiency 										tricks.Monte Carlo:search computes an approximate average over a random 										selection of playouts.Designed to calculate values of legal moves - when there is 										only one available move it will still searchReasons on the level of individual moves - cannot abstract 										and see a higher goal such as trapping a chess piece but not 										capturing it.Poorly incorporates machine learning: Early game programs 										relied on human expertise to hand-craft evaluation 										functions, opening books, search strategies, and efficiency 										tricks.The choice of which algorithm to use depends in part on the 									features of each game: when the branching factor is high or it 									is difficult to define an evaluation function, Monte Carlo 									search is preferred. But both algorithms suffer from 									fundamental limitationsmetareasoning: (reasoning about reasoning)6 Constraint satisfaction problemsLecture notesSummaryConstraint satisfaction problems (CSPs) represent a state 										with a set of variable/value pairs and represent the 										conditions for a solution by a set of constraints on the 										variables. Many important real-world problems can be 										described as CSPs.A number of inference techniques use the constraints to 										infer which variable/value pairs are consistent and which 										are not. These include node, arc, path, and k-consistency.Backtracking search, a form of depth-first search, is 										commonly used for solving CSPs. Inference can be interwoven 										with search.The minimum-remaining-values and degree heuristics are 										domain-independent methods for deciding which variable to 										choose next in a backtracking search. The least 										constraining-value heuristic helps in deciding which value 										to try first for a given variable. Backtracking occurs when 										no legal assignment can be found for a variable. 										Conflict-directed backjumping backtracks directly to the 										source of the problem.Local search using the min-conflicts heuristic has also been 										applied to constraint satisfaction problems with great 										success.The complexity of solving a CSP is strongly related to the 										structure of its constraint graph. Tree-structured problems 										can be solved in linear time. Cutset conditioning can reduce 										a general CSP to a tree-structured one and is quite 										efficient if a small cutset can be found. Tree decomposition 										techniques transform the CSP into a tree of subproblems and 										are efficient if the tree width of the constraint graph is 										small6.16.1 Defining constraint satisfaction problemsA constraint satisfaction problem consists of three 												components, X, D, and C: X is a set of variables, 												{X1,...,Xn}. D is a set of domains, {D1,...,Dn}, one for 												each variable. C is a set of constraints that specify 												allowable combinations of values.Each domain Di consists of a set of allowable values, 												{v1,...,vk} for variable Xi. Each constraint Ci consists 												of a pair scope, rel, where scope is a tuple of 												variables that participate in the constraint and rel is 												a relation that defines the values that those variables 												can take on.6.2 Example6.3 Variations on the CSP formalismThe simplest kind of CSP involves variables that have 												discrete, finite domains.A discrete domain can be infinite, such as the set of 												integers or strings.With infinite domains, it is no longer possible to 												describe constraints by enumerating all allowed 												combinations of values. Instead, a constraint language 												must be used that understands constraints such as T1 + 												d1 ≤ T2 directly, without enumerating the set of pairs 												of allowable values for (T1, T2).The best-known category of continuous-domain 													CSPsis that of linear programming problems, where 												constraints must be linear equalities or inequalities. 												Linear programming problems can be solved in time 												polynomial in the number of variables.The simplest type of constraint is theunary constraint, which restricts the 												value of a single variable.For example, in the map-coloring problem it could be the 												case that South Australians won’t tolerate the color 												green; we can express that with the unary constraint:Abinary constraintrelates two 												variables. For example, SA != NSW is a binary 												constraint. A binary CSP is one with only binary 												constraints;A constraint involving an arbitrary number of variables 												is called aglobal constraint.One of the most common global constraints isAlldiff, which says that all of the 												variables involved in the constraint must have different 												values.Constraints can be represented in aconstraint hypergraph, such as the one 												shown in Figure 6.2(b).A hypergraph consists of ordinary nodes (the circles in 												the figure) and hypernodes (the squares), which 												represent n-ary constraints.two reasons why we might prefer a global constraint such 												as Alldiff rather than a set of binary constraints:easier and less error-prone to write the problem 													description using Alldiff .possible to design special-purpose inference 													algorithms for global constraints that are not 													available for a set of more primitive constraintsconstraint optimization problem COP:A 												problem that can be solved with path-based or local 												optimization search methods6.2 Constraint propagation: Inference in CSPsIn CSPs an algorithm can search or do a specific type of 									inference calledconstraint propagation- 									using the constraints to reduce the number of legal values for 									a variable and thus reducing legal values for another 									variable. It can be done during search or as a preprocessing 									step before search.Local consistency:treat each variable as a 									node in a graph, and each binary constraint as an arc, then 									the process of enforcing local consistency in each part of the 									graph inconsistent values to be eliminated in the graph. 									Different types of local consistency is described in the 									chapters below:6.2.1 Node consistencynode-consistent: a variable where all 												its values in its domain satisfy the unary constraints. 												We can make a node consisting by eliminating the 												conflicting values from the domain of options. Like 												removing a color in the map issue which cannot be 												applied to a country.It is also possible to transform all n-ary constraints 												into binary ones6.2.2 Arc consistencyA variable in a CSP is arc-consistent if every value in 												its domain satisfies the variable’sA variable X_i isgeneralized arc consistentwith respect 												to an n-ary constraint if for every value v in the 												domain of X_i there exists a tuple of values that is a 												member of the constraint, has all its values taken from 												the domains of the corresponding variables, and has its 												X_i component equal to vFor example, if all variables have the domain {0, 1, 2, 												3}, then to make the variable X consistent with the 												constraint X&lt;Y &lt;Z, we would have to eliminate 2 												and 3 from the domain of X because the constraint cannot 												be satisfied when X is 2 or 36.2.3 Path consistencyPath consistency tightens the binary constraints by 												using implicit constraints that are inferred by looking 												at triples of variables.6.2.4 K-consistencyA CSP is k-consistent if, for any set of k − 1 variables 												and for any consistent assignment to those variables, a 												consistent value can always be assigned to any kth 												variable.1-consistency says that, given the empty set, we can 												make any set of one variable consistent: this is what we 												called node consistency. 2-consistency is the same as 												arc consistency. For binary constraint networks, 												3-consistency is the same as path consistency.A CSP is strongly k-consistent if it is k-consistent and 												is also (k − 1)-consistent, (k − 2)-consistent, ... all 												the way down to 1-consistent.6.2.5 Global constraintsA global constraint is one involving an arbitrary number 												of variables.One simple form of inconsistency detection for Alldiff 												constraints works as follows: if m variables are 												involved in the constraint, and if they have n possible 												distinct values altogether, and m&gt;n, then the 												constraint cannot be satisfied.Simple algorithm:Remove any constraint in the constraint that has a 													singleton domain and delete that variable’s value from 													the domains of the remaining variablesRepeat as long as there are singleton variablesIf at any point an empty domain is produced or there 													are more variables than domain values left, then an 													inconsistency has been detectedResource constraintThe constraint that no more than 10 personnel are 												assigned to a task in total is written asAtmost(10, P1, P2, P3, P4)We can detect an inconsistency simply by checking the 												sum of the minimum values of the current domains; for 												example, if each variable has the domain {3, 4, 5, 6}, 												the Atmost constraint cannot be satisfied. - Bascially 												it says that we have a maximum value “10” and p1-p4 are 												tasks that each take some time. If they collectively 												take more than the maximum then the constraint can’t be 												satisfied. To solve it we would then remove variables 												from our domain, fx. 5 and 3 so we do not exceed the max 												value.Bounds propagation:a CSP is bounds consistent if for every variable X, and 												for both the lower bound and upper-bound values of X, 												there exists some value of Y that satisfies the 												constraint between X and Y for every variable Y . This 												kind of bounds propagation is widely used in practical 												constraint problems.6.2.6 Sudoku ExampleA Sudoku puzzle can be considered a CSP with 81 												variables, one for each square. We use the variable 												names A1 through A9 for the top row (left to right), 												down to I1 through I9 for the bottom row. The empty 												squares have the domain {1, 2, 3, 4, 5, 6, 7, 8, 9} and 												the prefilled squares have a domain consisting of a 												single value. In addition, there are 27 different 												Alldiff constraints: one for each row, column, and box 												of 9 squares.6.3 Backtracking Search for CSPsA problem iscommutativeif the order of 									application of any given set of actions has no effect on the 									outcome. CSPs are commutative because when assigning values to 									variables, we reach the same partial assignment regardless of 									order.backtracking searcha depth-first search that 									chooses values for one variable at a time and backtracks when 									a variable has no legal values left to assign, shown in Figure 									6.56.3.1 Variable and value orderingThe backtracking algorithm contains the line var ← 												SELECT-UNASSIGNED-VARIABLE(csp)The simplest strategy for SELECT-UNASSIGNED-VARIABLE is 												to choose the next unassigned variable in order, {X1, 												X2,...}. This static variable ordering seldom results in 												the most efficient search.minimum remaining-values(most constrained variable):choosing the variable with the fewest “legal” values. 												Basically if several variables needs to be assigned then 												start with the one with the most options.degree heuristic:attempts to reduce the branching factor on future 												choices by selecting the variable that is involved in 												the largest number of constraints on other unassigned 												variables.The degree of a variable= how many 												constraints(options) it hasleast-constraining-value heuristic:the 												heuristic is trying to leave the maximum flexibility for 												subsequent variable assignments by first selecting the 												variables with the least constraints(options) and 												assigning them a value.6.3.2 Interleaving search and inferenceevery time we make a choice of a value for a variable, 												we can make new domain reductions on the neighboring 												variables - basically, if we assign a value to a 												variable, then we can remove it from its neighbours(in 												the map example)forward checking:When a variable X is 												assigned the forward-checking process establishes arc 												consistency for it - for each variable Y connected to X 												by a constraint, delete values that is inconsistent with 												the one chosen for X.Because forward checking only does arc consistency 												inferences, there is no reason to do forward checking if 												we have already done arc consistency as a preprocessing 												step.We can view forward checking as an efficient way to 												incrementally compute the information that theMRV(minimum remaining values)heuristic 												needs to do its jobAlthough forward checking detects many inconsistencies, 												but not all, it doesn’t look ahead and make all the 												other variables arc-consistent.MAC(Maintaining arc consistency):6.3.3 Intelligent backtracking: looking forwardchronological backtracking:when a 												branch of the search fails, back up to the preceding 												variable and try a different value for it.conflict set:a set of assignments that 												are in conflict with some value of a variable - eg. two 												neighbouring countries in the map example, having color 												1 and 2. The two colors, 1 and 2 is then the conflict 												set for the third country.backjumping:backtracks to the most 												recent assignment in the conflict set. it accumulates 												the conflict set while checking for a legal value to 												assign. If no legal value is found, the algorithm should 												return the most recent element of the conflict set along 												with the failure indicator.forward checking can supply the conflict set with no 												extra work: whenever forward checking based on an 												assignment X = x deletes a value from Y ’s domain, it 												should add X = x to Y ’s conflict set. If the last value 												is deleted from Y ’s domain, then the assignments in the 												conflict set of Y are added to the conflict set of X. 												Then, when we get to Y , we know immediately where to 												backtrack if needed.backjumping occurs when every value in a domain is in 												conflict with the current assignment; but forward 												checking detects this event and prevents the search from 												ever reaching such a node! In fact, it can be shown that 												every branch pruned by backjumping is also pruned by 												forward checking. Hence, simple backjumping is redundant 												in a forward-checking search or, indeed, in a search 												that uses stronger consistency checking, such as MACconflict-directed backjumping:A 												backjumping algorithm that uses conflict sets to find 												the last step where a conflict was moderated so we can 												change the assignment at that stage to resolve the 												conflict.When we reach a contradiction, backjumping can tell us 												how far to back up, so we don’t waste time changing 												variables that won’t fix the problem.Constraint learningis the idea of 												finding a minimum set(calledno-good) 												of variables from the conflict set that causes the 												problem.No-goods can be effectively used by forward checking or 												by backjumping.6.4 Local Search for CSPsThe point of local search is to eliminate violated 									constraints.Min-conflicts: the value that results in the 									minimum number of conflicts with other variables when choosing 									a new value for a variabletabu search: keeping a small list of recently 									visited states and forbidding the algorithm to return to those 									states. Simulated annealing can also be used to escape from 									plateauxconstraint weighting:Each constraint is given a numeric weight, W_i, initially all 									1. At each step of the search, the algorithm chooses a 									variable/value pair to change that will result in the lowest 									total weight of all violated constraints. The weights are then 									adjusted by incrementing the weight of each constraint that is 									violated by the current assignment.This has two benefits:Adds topography to plateaux, making sure that it is possible 										to improve from the current stateAdds weight to the constraints that are proving difficult to 										solve.6.5 The Structure of Problemsindependent subproblems: Parts of a problem 									that are not directly connected to the rest of the graph, 									essentially a single non-connected component is a subproblemconnected components:directed arc consistency: A CSP is defined to 									be directed arc-consistent undertopological sort:an ordering of the 									variables such that each variable appears after its parent in 									the treeAny tree with n nodes has n−1 arcs, so we can make this graph 									directed arc-consistent in O(n) steps, each of which must 									compare up to d possible domain values for two variables, for 									a total time of O(nd^2).constraint graphs reduced to treestwo primary ways to do this:removing nodescollapsing nodes togetherThe first approach involves assigning values to some variables 									so that the remaining variables form a tree.The general algorithm is as follows:The second approach is based on constructing a tree 									decomposition of the constraint graph into a set of connected 									subproblems. Each subproblem is solved independently, and the 									resulting solutions are then combined.A tree decomposition must satisfy the following three 									requirements:Every variable in the original problem appears in at least 										one of the subproblems.If two variables are connected by a constraint in the 										original problem, they must appear together (along with the 										constraint) in at least one of the subproblems.If a variable appears in two subproblems in the tree, it 										must appear in every subproblem along the path connecting 										those subproblems.The first two conditions ensure that all the variables and 									constraints are represented in the decomposition. The third 									condition seems rather technical, but simply reflects the 									constraint that any given variable must have the same value in 									every subproblem in which it appears;We solve each subproblem independently; if any one has no 									solution, we know the entire problem has no solution. If we 									can solve all the subproblems, then we attempt to construct a 									global solution:view each subproblem as a “mega-variable” whose domain is 										the set of all solutions for the subproblem.solve the constraints connecting the subproblems, using the 										efficient algorithm for trees given earlierA given constraint graph admits many tree decompositions; in 									choosing a decomposition, the aim is to make the subproblems 									as small as possible.The tree width of a tree decomposition of a graph is one less 									than the size of the largest subproblem.the tree width of the graph itself is defined to be the 									minimum tree width among all its tree decompositionsvalue symmetry:Different combinations of 									assigning the same values to the same variables.symmetry-breaking constraint:A constraint 									which breaks value symmetry to save search space. This can be 									done by fx. arranging the possible values in alphabetical 									order so one will always be assigned before another.breaking value symmetry has proved to be important and 									effective on a wide range of problems.Search Heuristics and AlphaGo ZeroSearch heuristics:For heuristic swe aim to have a value as high to the actual value 						fx. distances or moves, that is required to reach a terminal state, 						however, we must be certain that it is admissible eg. cannot 						overestimate the heuristic value.A heuristic is dominating over another if its value is always higher 						or equal to the other heuristic’s values whilst remaining admissibleRelaxed problems:Essentially: Disregard some of the rules to make the game easier. 						This is called a relaxation. Then the problem would be easier to 						solve and the minimum amount of moves are then lower than the actual 						minimum moves for the game and therefore an admissible heuristic.7 Logical agentsknowledge-based agents can combine and recombine information to suit 						myriad purposesPropositional logic is a factored representation; while less 						expressive than first-order logic which is the canonical structured 						representation, propositional logic illustrates all the basic 						concepts of logic. It also comes with well-developed inference 						technologies.7.1 Knowledge-Based AgentsThe central component of aknowledge-based agentis itsknowledge baseconsisting of a set of 									sentences. Each sentence is expressed in a language called aknowledge representation languageand 									represents some assertion about the worldTelloperation: adding new sentences of 									information to the Knowledge BaseAskoperation: Retrieving info from the 									Knowledge BaseEach call to the agent it then does:Tells the current perception to the knowledge base(KB)Asks the KB what action to execute based on its perceived 										stateAgent informs KB what action was chosen and then performs itA knowledge-based agent can be built simply by TELLing it what 									it needs to know. Starting with an empty knowledge base, the 									agent designer can TELL sentences one by one until the agent 									knows how to operate in its environment. This is called thedeclarative approach to system building.a successful agent often combines both declarative and 									procedural elements in its design, and declarative knowledge 									can often be compiled into more efficient procedural code.7.2The Wumpus World7.3 LogicThe sentences of the knowledge base is expressed according to 									the syntax of the representation language, which specifies all 									the sentences that are well formedA logic must also define the semantics, or meaning, of 									sentences. The semantics defines the truth of each sentence 									with respect to each possible worldThe KB can be thought of as a set of sentences or as a single 									sentence that asserts all the individual sentences. The KB is 									false in models that contradict what the agent knowsModel:a term for mathematical abstractions, 									each of which has a fixed truth value (true or false) for 									every relevant sentence. It depicts possibly real environments 									that an agent may operate in.logical entailmentbetween sentences—the idea 									that a sentence follows logically from another sentence.to mean that the sentence entails the sentence . The formal 									definition of entailment is this: if and only if, in every 									model in which is true, is also true.model checking: An inference algorithm that 									enumerates all possible models to check that a is true in all 									models in which KB is true, that is:think of the set of all consequences of as a haystack and of 									as a needle. Entailment is like the needle being in the 									haystack; inference is like finding it.Sound/truthPreserving inference algorithms:An inference algorithm that derives only entailed sentences. 									Soundness is a highly desirable property. An unsound inference 									procedure essentially makes things up as it goes along.completeness:an inference algorithm is 									complete if it can derive any sentence that is entailed.if KB is true in the real world, then any sentence derived 									from KB by a sound inference procedure is also true in the 									real world.correspondence between world and representation is illustrated 									as:grounding:the connection between logical 									reasoning processes and the real environment in which the 									agent exists. In particular, how do we know that KB is true in 									the real world - it is done by the agents sensors creating the 									connection between the sentences and real world aspects.General rules are produced by a sentence construction process 									called learning - such as connecting a red light with a stop 									action by giving it negative points for ignoring it.7.4 Propositional Logic: A Very Simple Logic7.4.1 SyntaxThe syntax of propositional logic defines the allowable 												sentences. The atomic sentences consist of a single 												proposition symbol. Each such symbol stands for a 												proposition that can be true or false.Complex sentencesare constructed from 												simpler sentences, using parentheses and operators 												called logical connectives7.4.2 SemanticsThe semantics defines the rules for determining the 												truth of a sentence with respect to a particular model. 												In propositional logic, a model simply sets the truth 												value - true or false - for every proposition symbol.The semantics for propositional logic must specify how 												to compute the truth value of any sentence, given a 												model. This is done recursively. All sentences are 												constructed from atomic sentences and the five 												connectives;Atomic sentences are easy:For complex sentences, we have five rules, which hold 												for any subsentences P and Q (atomic or complex) in any 												model m7.4.3 A simple knowledge baseConstructing a knowledge base(based on Wumpus game):7.4.4 A simple inference procedure7.5 Propositional Theorem ProvingPropositional logictheorem proving:applying rules of 												inference directly to the sentences in our knowledge 												base to construct a proof of the desired sentence 												without consulting models. If the number of models is 												large but the length of the proof is short, then theorem 												proving can be more efficient than model checking.validity:A sentence is valid if it is 												true in all models, such as P and not P since it is 												always true by the LEM rule.satisfiability:A sentence is 												satisfiable if it is true in, or satisfied by, some 												model.7.5.1 Inference and proofs7.5.2 Proof by resolution - INCOMPLETEcompleteness: an algorithm is complete in the sense that 												they will find any reachable goal, but if the available 												inference rules are inadequate, then the goal is not 												reachable—no proof exists that uses only those inference 												rulescomplementary literals(i.e., one is 												the negation of the other)7.5.3 Horn clauses and definite clausesdefinite clause, which is a disjunction 												of literals of which exactly one is positive7.6 Effective Propositional Model Checking7.6.1 A complete backtracking algorithmDavis–Putnam algorithm(DPLL): takes a sentence and a set 												of clauses. It acts like a recursive depth first 												enumeration of possible models.Early Termination: The algorithm detects if the 													sentence is true or falsePure symbol Heuristic: Symbols that always have the 													same variable symbol, like ‘A’ throughout all the 													clausesUnit Clause Heuristic: a clause with just one literal.7.6.2 Local search algorithms7.6.3 The landscape of random SAT problemsGiven a source of random sentences, we can measure the 												probability of satisfiabilityBinary Decision Diagrams1 Boolean ExpressionsAbstract syntax of boolean values: when x 									ranges over a set of boolean variables.2 Normal formsDNF(Disjunctive normal form): a boolean expression which 									consists of a disjunction between two sets of conjunctions fx.(a∧b...)∨(c∧d...)(a \wedge b...) \vee (c \wedge d 																...)(a∧b...)∨(c∧d...)﻿The indexed version of writing this is:3 Binary Decision DiagramsProofs and lemmasCanonicity Lemma - There us exactly one ROBDD for 															any functionINF(If-then-else form):Checking if two boolean statements are the same:Construct and compare their ROBDD’s. If they are alike then 									the statements are the same.4 Constructing and manipulating ROBDDsReducing OBDDsNodes are represented as 0, 1, 2 ... where 0 and 1 are 												terminal nodes.Variables in orderx1&lt;x2&lt;...&lt;xnx_1 &lt;x_2&lt;...&lt;x_nx1​&lt;x2​&lt;...&lt;xn​﻿are represented by their indices 1, 2, ... , nThe ROBDD is stored in a table T : u (i, l, h) which 												maps a node u to its three attributes. i=var(u), l = 												low(u) and h = high(u)When the table above is generated we can compare the 												values for various nodes. If one or more has the same 												values then all but one are redundant and can be 												removed.Build ROBDDs algorithm4.3 Applyalgorithm5 Implementing the ROBDD operations6 Examples of problem solving with ROBDDs6.1 8-queens-problemEssentially: check if for Arbitrary N: is it possible to 												place N queens on a N * N chess board without their 												diagonal, vertical or horizontal fields intersecting 												another queen.Code it using Boolean variables:Set each position on the board to be a variable, named 													asxix_ixi​﻿j_jj​﻿where i=row and j=columnMark a variable as 1(true) when a queen is in the 													fieldExpress boolean expression: for X, no other queen is 													in diagonal fields, row, and columnThere must be a queen in each row so for all i, it 													must be inXiX_iXi​﻿1∨_1 \vee1​∨﻿XiX_iXi​﻿2∨_2 \vee2​∨﻿.........﻿∨Xi\vee X_i∨Xi​﻿N_NN​﻿Restrictions overview:6.2 Correctness of Combinational Circuits6.3 Equivalence of Combinational Circuits4(4’th ed.) Search in Complex Environments (233-288)4.1 Local Search and Optimization ProblemsLocal search algorithms operate by searching from a start 									state to neighboring states, without keeping track of the 									paths, nor the set of states that have been reached so they 									arenot systematic.Biggest advantages:Use very littel memoryCan find solutions in large or infinite state spacesCan solve optimization problems, by finding the best state 										according to an objective functionHill climbing: searching for a global maximumGradient descent: searching for a global minimum4.1.1 Hill-climbing search algorithm - greedy local searchthe algorithm keeps track of one current state and on each 									iteration moves to the neighboring state with the highest 									value and terminates when it reaches a peak(no higher 									neighbour). It is greedy since it just grabs the current best 									solution and doesn’t think ahead.Disadvantages:May get stuck at one of the following:Local maxima - nowhere to go that is higherRidges - a sequence of local maximaPlateaus - flatlines, nowhere is higherSolutions:Allowing limited sidestepping(moving sideways on flat edges)Stochastic hill climbing:Chooses a random start position among the uphill positions and 									climbs to the enarest top. For each top it keeps a pointer to 									the largest and as such it returns the best result and is 									effective in large state-spacesRandom-restart hill climbing:Conducts a series of searches from random initial states until 									a goal is found.Each process is run separately from the others.if there are few local maxima and plateaus, random-restart 									hill climbing will find a good solution very quickly.4.1.2 Simulated annealing ////confused about this oneSteps:Random position is pickedPick a random move, if the result is better than current, 										then accept the move4.1.3 Local Beam SearchTracks an amount of states.Starts with random states and for each step it calculates the 									successors of all the states - if one of the outcomes is a 									goal then the alogirhtm is halted.The successors are calculated repeatedly until a goal is foundIn a local beam search, useful information is passed among the 									parallel search threads to guide the thread in direction of 									better values and avoid searches in locations that are known 									to be poor.can suffer from a lack of diversity among the states—they can 									become clustered in a small region of the state space, making 									the search little more than a -timesslower version of hill 									climbingStochastic beam searchinstead of choosing 									the top successors, stochastic beam search chooses successors 									with probability proportional to the successor’s value, thus 									increasing diversity.4.1.4 Evolutionary algorithmsEvolutionary algorithms can be seen as variants of stochastic 									beam search that are explicitly motivated by the metaphor of 									natural selection in biology
</html>
