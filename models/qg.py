import sys
from transformers import (
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    T5TokenizerFast,
    T5ForConditionalGeneration
)
from evaluate import load
from datasets import load_dataset, Dataset
from transformers.utils import PaddingStrategy
from parsing.settings_parser import DataTrainingArguments
from preprocessing.data_collator import T2TDataCollator
import torch
import wandb
import os
import logging

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s %(levelname)s: %(message)s', datefmt="%Y-%m-%d - %H:%M:%S")

file_handler = logging.FileHandler('qg.log', mode='a')
file_handler.setLevel(logging.INFO)
file_handler.setFormatter(formatter)

ch = logging.StreamHandler(sys.stdout)
ch.setLevel(logging.INFO)
ch.setFormatter(formatter)

logger.addHandler(file_handler)
logger.addHandler(ch)


_MODEL_MAX_LENGTH = 512
_GENERATOR_ARGS = {
    "max_length": _MODEL_MAX_LENGTH,
    "num_beams": 4,
    "length_penalty": 1.5,
    "no_repeat_ngram_size": 3,
    "early_stopping": True,
}


class QG:
    """Question Generation model based on Google's `T5` model."""

    def __init__(self, model: str, tokenizer: str):
        """Initializes the `QG` model."""

        self._device = "cuda" if torch.cuda.is_available() else "cpu"
        self._model: T5ForConditionalGeneration = AutoModelForSeq2SeqLM.from_pretrained(model).to(self._device)
        self._tokenizer: T5TokenizerFast = AutoTokenizer.from_pretrained(tokenizer, model_max_length=_MODEL_MAX_LENGTH)

        self._tokenizer.add_tokens(['<sep>'])
        self._model.resize_token_embeddings(len(self._tokenizer))


    def __call__(self, context: str, limit: int = 15) -> list[dict]:
        """Generates questions based on the given context and formats it as a dictionary."""
        
        logger.info(f"Received context of length: {len(context)}")
        logger.info(f"Generation limit set to: {limit}")

        # Split the context into chunks of the maximum length
        context_chunks = self.split_text(context)
        logger.info(f"Split context in {len(context_chunks)} chunks")

        context_and_questions = []

        # Generate questions for each chunk
        for i, context_chunk in enumerate(context_chunks):
            if i > limit:
                break

            logger.info(f"Generating questions for chunk {i}/{limit}")

            input_string = "generate questions: " + context_chunk + " </s>"

            # Encode input string
            input_ids = self._tokenizer.encode(input_string, return_tensors="pt").to(self._device)

            # Let the model generate questions from the encoded input
            result = self._model.generate(input_ids, **_GENERATOR_ARGS)

            # Decode the questions generated by the model
            questions = self._tokenizer.decode(result[0], skip_special_tokens=True)

            # Split each question by the separator token
            questions = questions.split("<sep>")

            # If there are multiple '?' in a question, split it into multiple questions
            for question in questions:
                if question.count('?') > 1:
                    questions.remove(question)
                    split_questions = question.split('?')
                    split_questions = [
                        newQuestion + '?' for newQuestion in split_questions if newQuestion != '']
                    questions.extend(split_questions)

            # Remove leading and trailing white space, remove last empty element from results
            questions = [question.strip() for question in questions]

            # Remove empty questions
            questions = [question for question in questions if question != '']

            context_and_questions.append({
                "context": context_chunk,
                "questions": questions
            })

        return context_and_questions


    def split_text(self, text: str, max_length: int = 100) -> list:
        """Splits the given text into chunks of the given maximum length."""

        sentences = text.split(".")
        chunks = []
        current_chunk = ""

        for sentence in sentences:
            tokenized_sentence = self._tokenizer(sentence)

            if len(current_chunk.split()) + len(tokenized_sentence) < max_length:
                current_chunk += f"{sentence}."
            else:
                chunks.append(current_chunk.strip())
                current_chunk = f"{sentence}."

        if current_chunk.strip():
            chunks.append(current_chunk.strip())

        return chunks
    
    def evaluate(self):
        """Evaluates `QG` in terms of `BLEU` and `METEOR` on the `validation` split of `SQuAD`."""

        squad: Dataset = load_dataset("the-coorporation/the_squad_qg")
        validation = squad['validation']
        bleu = load("bleu")
        meteor = load("meteor")

        references: list[str] = [entry['questions'].replace(" {sep_token}", "") for entry in squad['validation']]
        predictions: list[str] = []

        device = "cuda" if torch.cuda.is_available() else "cpu"

        for i, entry in enumerate(validation):
            logger.info(f"Evaluating on entry {i + 1}/{len(validation)}...")

            input_string: str = "generate questions: " + entry['context'] + " </s>"
            input_encodings = self._tokenizer(input_string, padding='longest', truncation=True, return_tensors='pt').to(device)
            results = self._model.generate(**input_encodings, **_GENERATOR_ARGS).to(device)
            questions = self._tokenizer.decode(results[0], skip_special_tokens=True)
            predictions.append(questions)
        
        logger.info(bleu.compute(references=references, predictions=predictions))
        logger.info(meteor.compute(references=references, predictions=predictions))


    def train(self, training_args: TrainingArguments, data_args: DataTrainingArguments, wandb_key: str):
        """Start training the `QG` model. Once completed it will be pushed to the HuggingFace Hub along with the `tokenizer`."""

        # Set WANDB project name and login
        os.environ['WANDB_PROJECT'] = data_args.wandb_project_name
        wandb.login(key=wandb_key)

        # Load preprocessed datasets
        train = torch.load(data_args.training_file_path)
        validation = torch.load(data_args.validation_file_path)

        # Whether to apply Smart Batching and Mixed Precision Training
        padding_strategy = PaddingStrategy.LONGEST if data_args.optimized_training else PaddingStrategy.MAX_LENGTH
        training_args.group_by_length = data_args.optimized_training
        training_args.fp16 = True if torch.cuda.is_available() and data_args.optimized_training else False
        training_args.run_name = "With Smart Bacthing & Mixed Precision Training" if data_args.optimized_training else training_args.run_name

        # Trainer whether to upload to hub
        training_args.push_to_hub = data_args.upload_to_hub

        trainer = Trainer(
            model=self._model,
            args=training_args,
            train_dataset=train,
            eval_dataset=validation,
            data_collator=T2TDataCollator(self._model, self._tokenizer, padding_strategy)
        )

        trainer.train()
        wandb.finish()

        if data_args.upload_to_hub:
            trainer.push_to_hub(blocking=True)
            self._tokenizer.push_to_hub(training_args.hub_model_id)
