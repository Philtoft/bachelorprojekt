{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QGAR - A Flashcard Generating NLP Model\n",
    "\n",
    "This notebook shows how to load and use the `QGAR` model.\n",
    "\n",
    "Please read the [README](./readme.md) before continueing!\n",
    "\n",
    "**Table of Contents:**\n",
    "1. [Load QGAR](#load-qgar)\n",
    "2. [Download and Preprocess SQuAD Dataset](#download-and-preprocess-squad-dataset)\n",
    "3. [Run QGAR](#run-qgar)\n",
    "4. [Train QGAR](#4-train-qgar)\n",
    "\n",
    "</br>\n",
    "\n",
    "---\n",
    "\n",
    "</br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load QGAR\n",
    "First, we must load the `QGAR` model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Your token has been saved in your configured git credential helpers (osxkeychain).\n",
      "Your token has been saved to /Users/philiphyltoft/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from huggingface_hub import login\n",
    "from main import load_model_and_tokenizer, get_hg_token\n",
    "\n",
    "login(get_hg_token(), add_to_git_credential=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, tokenizer = load_model_and_tokenizer(\"the-coorporation/t5-qgar\", device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download and Preprocess SQuAD Dataset\n",
    "\n",
    "First, we download and preprocess the modified `SQuAD` dataset, adding separator (`<sep>`) and end of sequence tokens (`<\\s>`) to each entry.\n",
    "The preprocessed file is split in two datasets, `training` and `validation`, and the sets are saved in the `data` directory in `PyTorch` format under the names:\n",
    "* [training_data.pt](./data/training_data.pt)\n",
    "* [validation_data.pt](./data/validation_data.pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading SQuAD dataset...\n",
      "Found cached dataset squad_processor (/Users/philiphyltoft/.cache/huggingface/datasets/squad_processor/plain_text/1.0.0/173b8305efd9aeaed82e2f74eb48fff367a5b6036cbf7fca6cd0deb4d4bb4f95)\n",
      "100%|██████████| 2/2 [00:00<00:00, 1104.49it/s]\n",
      "Download complete.\n",
      "Preprocessing SQuAD dataset...\n",
      "Loading cached processed dataset at /Users/philiphyltoft/.cache/huggingface/datasets/squad_processor/plain_text/1.0.0/173b8305efd9aeaed82e2f74eb48fff367a5b6036cbf7fca6cd0deb4d4bb4f95/cache-bd6bdb3f24c6a457.arrow\n",
      "Loading cached processed dataset at /Users/philiphyltoft/.cache/huggingface/datasets/squad_processor/plain_text/1.0.0/173b8305efd9aeaed82e2f74eb48fff367a5b6036cbf7fca6cd0deb4d4bb4f95/cache-b8d3c00b5355c76b.arrow\n",
      "Loading cached processed dataset at /Users/philiphyltoft/.cache/huggingface/datasets/squad_processor/plain_text/1.0.0/173b8305efd9aeaed82e2f74eb48fff367a5b6036cbf7fca6cd0deb4d4bb4f95/cache-170928d4e07c8d49.arrow\n",
      "Loading cached processed dataset at /Users/philiphyltoft/.cache/huggingface/datasets/squad_processor/plain_text/1.0.0/173b8305efd9aeaed82e2f74eb48fff367a5b6036cbf7fca6cd0deb4d4bb4f95/cache-c53b6afe5389b59a.arrow\n",
      "Loading cached processed dataset at /Users/philiphyltoft/.cache/huggingface/datasets/squad_processor/plain_text/1.0.0/173b8305efd9aeaed82e2f74eb48fff367a5b6036cbf7fca6cd0deb4d4bb4f95/cache-e299d5eada15f7a6.arrow\n",
      "Loading cached processed dataset at /Users/philiphyltoft/.cache/huggingface/datasets/squad_processor/plain_text/1.0.0/173b8305efd9aeaed82e2f74eb48fff367a5b6036cbf7fca6cd0deb4d4bb4f95/cache-9ddeacb2c6a6c68a.arrow\n",
      "Done.\n",
      "Saving processed dataset to directory './data/'...\n",
      "Successfully saved processed datasets.\n"
     ]
    }
   ],
   "source": [
    "from preprocessing.preprocessor import Preprocessor\n",
    "\n",
    "preprocessor = Preprocessor(model, tokenizer)\n",
    "preprocessor.preprocess_dataset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run QGAR\n",
    "Next, we import `QGAR` and set up a pipeline.\n",
    "Now, we can simply pass a context to the model to generate questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'Historical Fiction is one of those sub-genres of literature that takes many forms. It's most important feature, though, is that it's set in the past, with every element of the story conforming to the norms of the day. Here's how we define Historical Fiction, a look at its origins, and some popular types.'\n",
      "Results: tensor([[    0,   363,    19,    80,    13,     8,   769,    18,   729,    60,\n",
      "             7,    13,  6678,    24,  1217,   186,  2807,    58, 32100,   363,\n",
      "            19,     8,   167,   359,  1451,    13, 17885, 24525,    58, 32100,\n",
      "          2840,    19, 17885, 24525,   356,    16,     8,   657,    58, 32100,\n",
      "             1]])\n",
      "Output: \n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "from main import run_model\n",
    "\n",
    "input_text = \"Historical Fiction is one of those sub-genres of literature that takes many forms. It's most important feature, though, is that it's set in the past, with every element of the story conforming to the norms of the day. Here's how we define Historical Fiction, a look at its origins, and some popular types.\"\n",
    "run_model(\"the-coorporation/t5-qgar\", device, input_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train QGAR\n",
    "To train `QGAR`, we first parse `settings.json` to get the training arguments.\n",
    "\n",
    "We then make an instance of `QGARTrainer` and call `train` which will train the model and push it to `The Coorporation`'s Huggingface Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ModelArguments.__init__() missing 1 required positional argument: 'model_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m train_file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./data/training_data.pt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m validation_file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./data/validation_data.pt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m model_args, data_args, train_args \u001b[39m=\u001b[39m parse_settings()\n\u001b[1;32m      9\u001b[0m trainer \u001b[39m=\u001b[39m QGARTrainer(model, train_file, validation_file, train_args)\n\u001b[1;32m     10\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/MEGA/ITU/6sem/bachelorproject/bachelorprojekt/main.py:117\u001b[0m, in \u001b[0;36mparse_settings\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m mergedEntries \u001b[39m=\u001b[39m settings[\u001b[39m'\u001b[39m\u001b[39mmodel_arguments\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m|\u001b[39m settings[\u001b[39m'\u001b[39m\u001b[39mdata_training_arguments\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m|\u001b[39m settings[\u001b[39m'\u001b[39m\u001b[39mtraining_arguments\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    116\u001b[0m parser \u001b[39m=\u001b[39m HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n\u001b[0;32m--> 117\u001b[0m \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mparse_dict(mergedEntries)\n",
      "File \u001b[0;32m~/miniconda3/envs/hg_training/lib/python3.10/site-packages/transformers/hf_argparser.py:367\u001b[0m, in \u001b[0;36mHfArgumentParser.parse_dict\u001b[0;34m(self, args, allow_extra_keys)\u001b[0m\n\u001b[1;32m    365\u001b[0m     inputs \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m args\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m keys}\n\u001b[1;32m    366\u001b[0m     unused_keys\u001b[39m.\u001b[39mdifference_update(inputs\u001b[39m.\u001b[39mkeys())\n\u001b[0;32m--> 367\u001b[0m     obj \u001b[39m=\u001b[39m dtype(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m    368\u001b[0m     outputs\u001b[39m.\u001b[39mappend(obj)\n\u001b[1;32m    369\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_extra_keys \u001b[39mand\u001b[39;00m unused_keys:\n",
      "\u001b[0;31mTypeError\u001b[0m: ModelArguments.__init__() missing 1 required positional argument: 'model_type'"
     ]
    }
   ],
   "source": [
    "from main import parse_settings\n",
    "from training.trainer import QGARTrainer\n",
    "\n",
    "train_file = \"./data/training_data.pt\"\n",
    "validation_file = \"./data/validation_data.pt\"\n",
    "\n",
    "model_args, data_args, train_args = parse_settings()\n",
    "\n",
    "trainer = QGARTrainer(model, train_file, validation_file, train_args)\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae2ba95b8bc8fbec5d0ccff067e6f5791542bd91f96bfdb3bb5972317be5ea2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
