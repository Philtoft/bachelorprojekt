{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QGAR - A Flashcard Generating NLP Model\n",
    "\n",
    "This notebook shows how to load and use the `QGAR` model.\n",
    "\n",
    "Please read the [README](./README.md) before continuing!\n",
    "\n",
    "---\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "0. [Dependencies](#0-dependencies)\n",
    "1. [Load QG](#1-load-qg)\n",
    "2. [Run QG](#2-run-qg)\n",
    "3. [Train QG](#3-train-qg)\n",
    "4. [Answer Generator](#4-answer-generator)\n",
    "5. [Output to Anki](#5-output-to-anki)\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Dependencies\n",
    "\n",
    "The project uses the following dependencies.\n",
    "\n",
    "Make sure to install them in your `Virtual Environment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -q\n",
    "%pip install transformers -q\n",
    "%pip install datasets -q\n",
    "%pip install wandb -q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load QG\n",
    "\n",
    "To load our `Question Generator` model, we use the `QG class` from the `models` module.\n",
    "\n",
    "From the model, we can extract the actual loaded `model` and `tokenizer` via its fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from models.qg import QG\n",
    "\n",
    "qg = QG(\"t5-small\", \"t5-small\")\n",
    "model = qg._model\n",
    "tokenizer = qg._tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run QG\n",
    "\n",
    "To run the `QG` model on some example `context`, we use the `__call__` method of the class.\n",
    "\n",
    "This will return a dictionary containing the `context` and a list of `questions` generated for the provided `context`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "\n",
    "qg = QG(\"t5-small\", \"t5-small\")\n",
    "context = \"Historical Fiction is one of those sub-genres of literature that takes many forms. It's most important feature, though, is that it's set in the past, with every element of the story conforming to the norms of the day. Here's how we define Historical Fiction, a look at its origins, and some popular types.\"\n",
    "\n",
    "questions = qg(context)\n",
    "print(json.dumps(questions, indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train QG\n",
    "\n",
    "To train `QG`, we first parse the `settings.json` file to get the `TrainingArguments` and `DataTrainingArguments`.\n",
    "\n",
    "We can then call `train` on the `QG` instance, which will train the model and push it to `The Coorporation`'s Huggingface Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from main import get_local_file\n",
    "from parsing.settings_parser import parse_settings\n",
    "\n",
    "qg = QG(\"t5-small\", \"t5-small\")\n",
    "\n",
    "_, data_args, train_args = parse_settings()\n",
    "qg.train(train_args, data_args, get_local_file(\"wandb_token.txt\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Answer Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "from transformers import pipeline\n",
    "question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n",
    "\n",
    "question_answers = []\n",
    "\n",
    "print(questions[\"questions\"])\n",
    "\n",
    "for question in questions[\"questions\"]:\n",
    "    print(question)\n",
    "    result = question_answerer(question=question, context=context)\n",
    "    if result[\"score\"] > 0.5:\n",
    "        question_answers.append({ \"question\": question, \"answer\": result[\"answer\"] })\n",
    "        # question_answers.append({ \"question\": question, \"answer\": result[\"answer\"], \"score\": result[\"score\"] })\n",
    "\n",
    "print(question_answers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Output to Anki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output format: Front, Back\n",
    "# Front: Question\n",
    "# Back: Answer\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(question_answers, columns=[\"question\", \"answer\"])\n",
    "df.to_csv(\"anki-output.csv\", index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae2ba95b8bc8fbec5d0ccff067e6f5791542bd91f96bfdb3bb5972317be5ea2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
