{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QG - A Question Generating NLP Model\n",
    "\n",
    "This notebook shows how to load and use the `QGAR` model.\n",
    "\n",
    "Please read the [README](./README.md) before continuing!\n",
    "\n",
    "---\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "0. [Dependencies](#0-dependencies)\n",
    "1. [Load QG](#1-load-qg)\n",
    "2. [Train QG](#2-train-qg)\n",
    "3. [Run QG](#3-run-qg)\n",
    "4. [Answer Generator](#4-answer-generator)\n",
    "5. [Output to Anki](#5-output-to-anki)\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Dependencies\n",
    "\n",
    "The project uses the following dependencies.\n",
    "\n",
    "Make sure to install them in your `Virtual Environment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -q\n",
    "%pip install transformers -q\n",
    "%pip install datasets -q\n",
    "%pip install wandb -q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load QG\n",
    "\n",
    "To load our `Question Generation` model, we use the `QG` class from the `models` module.\n",
    "\n",
    "From the model, we can extract the actual loaded `model` and `tokenizer` via its fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from models.qg import QG\n",
    "\n",
    "qg = QG(\"t5-small\", \"t5-small\")\n",
    "model = qg._model\n",
    "tokenizer = qg._tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train QG\n",
    "\n",
    "To train `QG`, we first parse the `settings.json` file to get the `TrainingArguments` and `DataTrainingArguments`.\n",
    "\n",
    "We can then call `train` on the `QG` instance, which will train the model and push it to `The Coorporation` organization on Huggingface Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from main import get_local_file\n",
    "from models.qg import QG\n",
    "from parsing.settings_parser import parse_settings\n",
    "\n",
    "qg = QG(\"t5-small\", \"t5-small\")\n",
    "\n",
    "_, data_args, train_args = parse_settings()\n",
    "qg.train(train_args, data_args, get_local_file(\"wandb_token.txt\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run QG\n",
    "\n",
    "To run the `QG` model on some example `context`, we use the `__call__` method of the class.\n",
    "\n",
    "This will return a dictionary containing the `context` and a list of `questions` generated for the provided `context`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from models.qg import QG\n",
    "import json\n",
    "\n",
    "qg = QG(\"t5-small\", \"t5-small\")\n",
    "context = \"Historical Fiction is one of those sub-genres of literature that takes many forms. It's most important feature, though, is that it's set in the past, with every element of the story conforming to the norms of the day. Here's how we define Historical Fiction, a look at its origins, and some popular types.\"\n",
    "\n",
    "questions = qg(context)\n",
    "print(json.dumps(questions, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "_MODEL_MAX_LENGTH = 512\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"the-coorporation/t5-small-qg\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\", model_max_length=_MODEL_MAX_LENGTH)\n",
    "\n",
    "tokenizer.add_tokens(['<sep>'])\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Historical Fiction is one of those sub-genres of literature that takes many forms. It's most important feature, though, is that it's set in the past, with every element of the story conforming to the norms of the day. Here's how we define Historical Fiction, a look at its origins, and some popular types.\"\n",
    "\n",
    "generator_args = {\n",
    "            \"max_length\": 256,\n",
    "            \"num_beams\": 4,\n",
    "            \"length_penalty\": 1.5,\n",
    "            \"no_repeat_ngram_size\": 3,\n",
    "            \"early_stopping\": True,\n",
    "        }\n",
    "\n",
    "input_string = \"generate questions: \" + context + \" </s>\"\n",
    "\n",
    "# Encode input string\n",
    "inputs = tokenizer.encode(\n",
    "    input_string, \n",
    "    add_special_tokens=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\", \n",
    ").to(device)\n",
    "\n",
    "# Let the model generate questions from the encoded input\n",
    "result = model.generate(inputs, **generator_args)\n",
    "\n",
    "# Decode the questions generated by the model\n",
    "questions = tokenizer.decode(result[0], skip_special_tokens=True)\n",
    "print(questions)\n",
    "\n",
    "# # Split each question by the separator token\n",
    "# questions = questions.split(\"<sep>\")\n",
    "\n",
    "# # Remove leading and trailing white space, remove last empty element from results\n",
    "# questions = [question.strip() for question in questions[:-1]]\n",
    "\n",
    "# output = {\n",
    "#     \"context\": context,\n",
    "#     \"questions\": questions\n",
    "# }\n",
    "\n",
    "# output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Answer Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "from transformers import pipeline\n",
    "question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n",
    "\n",
    "question_answers = []\n",
    "\n",
    "print(questions[\"questions\"])\n",
    "\n",
    "for question in questions[\"questions\"]:\n",
    "    print(question)\n",
    "    result = question_answerer(question=question, context=context)\n",
    "    if result[\"score\"] > 0.5:\n",
    "        question_answers.append({ \"question\": question, \"answer\": result[\"answer\"] })\n",
    "        # question_answers.append({ \"question\": question, \"answer\": result[\"answer\"], \"score\": result[\"score\"] })\n",
    "\n",
    "print(question_answers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Output to Anki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output format: Front, Back\n",
    "# Front: Question\n",
    "# Back: Answer\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(question_answers, columns=[\"question\", \"answer\"])\n",
    "df.to_csv(\"anki-output.csv\", index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "5f5ac68a19970ac19728ecfdb27ce90af7d423a161df21ca370564cfbde878e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
