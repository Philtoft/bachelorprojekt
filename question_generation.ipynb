{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QGAR - A Flashcard Generating NLP Model\n",
    "\n",
    "This notebook shows how to load and use the `QGAR` model.\n",
    "\n",
    "Please read the [README](./readme.md) before continueing!\n",
    "\n",
    "**Table of Contents:**\n",
    "1. [Load QGAR](#load-qgar)\n",
    "2. [Download and Preprocess SQuAD Dataset](#download-and-preprocess-squad-dataset)\n",
    "3. [Run QGAR](#run-qgar)\n",
    "4. [Train QGAR](#4-train-qgar)\n",
    "\n",
    "</br>\n",
    "\n",
    "---\n",
    "\n",
    "</br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Used Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (1.13.1)\n",
      "Requirement already satisfied: torchvision in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (0.14.1)\n",
      "Requirement already satisfied: torchaudio in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (0.13.1)\n",
      "Requirement already satisfied: typing_extensions in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: numpy in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: requests in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from requests->torchvision) (1.26.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (4.26.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: filelock in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: requests in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from requests->transformers) (1.26.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: datasets in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (2.10.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from datasets) (8.0.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: multiprocess in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from datasets) (1.5.2)\n",
      "Requirement already satisfied: responses<0.19 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: aiohttp in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: packaging in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from datasets) (22.0)\n",
      "Requirement already satisfied: xxhash in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from datasets) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: filelock in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: six in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from responses<0.19->datasets) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from pandas->datasets) (2022.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wandb in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (0.14.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: PyYAML in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from wandb) (3.1.31)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from wandb) (4.22.0)\n",
      "Requirement already satisfied: setproctitle in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: pathtools in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from wandb) (1.17.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from wandb) (2.28.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: setuptools in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from wandb) (65.6.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "%pip install transformers\n",
    "%pip install datasets\n",
    "%pip install wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load QG\n",
    "First, we must load the `QG` model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved in your configured git credential helpers (osxkeychain).\n",
      "Your token has been saved to /Users/philiphyltoft/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from models.qg import QG\n",
    "\n",
    "qg = QG(\"the-coorporation/t5-qgar\", \"t5-small\")\n",
    "model = qg._model\n",
    "tokenizer = qg._tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download and Preprocess SQuAD Dataset\n",
    "\n",
    "First, we download and preprocess the modified `SQuAD` dataset, adding separator (`<sep>`) and end of sequence tokens (`<\\s>`) to each entry.\n",
    "The preprocessed file is split in two datasets, `training` and `validation`, and the sets are saved in the `data` directory in `PyTorch` format under the names:\n",
    "* [training_data.pt](./data/training_data.pt)\n",
    "* [validation_data.pt](./data/validation_data.pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading SQuAD dataset...\n",
      "Found cached dataset squad_processor (/Users/philiphyltoft/.cache/huggingface/datasets/squad_processor/plain_text/1.0.0/173b8305efd9aeaed82e2f74eb48fff367a5b6036cbf7fca6cd0deb4d4bb4f95)\n",
      "100%|██████████| 2/2 [00:00<00:00, 299.58it/s]\n",
      "Download complete.\n",
      "Preprocessing SQuAD dataset...\n",
      "Loading cached processed dataset at /Users/philiphyltoft/.cache/huggingface/datasets/squad_processor/plain_text/1.0.0/173b8305efd9aeaed82e2f74eb48fff367a5b6036cbf7fca6cd0deb4d4bb4f95/cache-92c17265b08f1edc.arrow\n",
      "Loading cached processed dataset at /Users/philiphyltoft/.cache/huggingface/datasets/squad_processor/plain_text/1.0.0/173b8305efd9aeaed82e2f74eb48fff367a5b6036cbf7fca6cd0deb4d4bb4f95/cache-387cbdf968909eb4.arrow\n",
      "Loading cached processed dataset at /Users/philiphyltoft/.cache/huggingface/datasets/squad_processor/plain_text/1.0.0/173b8305efd9aeaed82e2f74eb48fff367a5b6036cbf7fca6cd0deb4d4bb4f95/cache-b9172de254d7c7c7.arrow\n",
      "Loading cached processed dataset at /Users/philiphyltoft/.cache/huggingface/datasets/squad_processor/plain_text/1.0.0/173b8305efd9aeaed82e2f74eb48fff367a5b6036cbf7fca6cd0deb4d4bb4f95/cache-18e8d390c4563c2c.arrow\n",
      "Loading cached processed dataset at /Users/philiphyltoft/.cache/huggingface/datasets/squad_processor/plain_text/1.0.0/173b8305efd9aeaed82e2f74eb48fff367a5b6036cbf7fca6cd0deb4d4bb4f95/cache-7b7bcfbe1cc4b6cb.arrow\n",
      "Loading cached processed dataset at /Users/philiphyltoft/.cache/huggingface/datasets/squad_processor/plain_text/1.0.0/173b8305efd9aeaed82e2f74eb48fff367a5b6036cbf7fca6cd0deb4d4bb4f95/cache-329286cc5d1e96d2.arrow\n",
      "Done.\n",
      "Saving processed dataset to directory './data/'...\n",
      "Successfully saved processed datasets.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from preprocessing.preprocessor import SquadPreprocessor\n",
    "\n",
    "preprocessor = SquadPreprocessor(model, tokenizer)\n",
    "preprocessor.preprocess_dataset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run QG\n",
    "Next, we import `QG` and set up a pipeline.\n",
    "Now, we can simply pass a context to the model to generate questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "\n",
    "context = \"Historical Fiction is one of those sub-genres of literature that takes many forms. It's most important feature, though, is that it's set in the past, with every element of the story conforming to the norms of the day. Here's how we define Historical Fiction, a look at its origins, and some popular types.\"\n",
    "\n",
    "questions = qt(context)\n",
    "print(json.dumps(questions, indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train QG\n",
    "To train `QG`, we first parse `settings.json` to get the training arguments.\n",
    "\n",
    "We then call `train` which will train the model and push it to `The Coorporation`'s Huggingface Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "env: WANDB_PROJECT=t5-qg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
      "Cloning https://huggingface.co/the-coorporation/t5-qg into local empty directory.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/740 : < :, Epoch 0.01/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     key \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[1;32m     14\u001b[0m qt \u001b[39m=\u001b[39m QG(\u001b[39m\"\u001b[39m\u001b[39mt5-small\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mt5-small\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m qt\u001b[39m.\u001b[39;49mtrain(train_args, data_args, key)\n",
      "File \u001b[0;32m~/bachelorprojekt/models/qg.py:72\u001b[0m, in \u001b[0;36mQG.train\u001b[0;34m(self, training_args, data_args, wandb_key)\u001b[0m\n\u001b[1;32m     62\u001b[0m validation \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(data_args\u001b[39m.\u001b[39mvalidation_file_path)\n\u001b[1;32m     64\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     65\u001b[0m     model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model,\n\u001b[1;32m     66\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m     data_collator\u001b[39m=\u001b[39mT2TDataCollator()\n\u001b[1;32m     70\u001b[0m )\n\u001b[0;32m---> 72\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     73\u001b[0m wandb\u001b[39m.\u001b[39mfinish()\n\u001b[1;32m     74\u001b[0m trainer\u001b[39m.\u001b[39mpush_to_hub()\n",
      "File \u001b[0;32m~/miniconda3/envs/hugging/lib/python3.10/site-packages/transformers/trainer.py:1633\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1630\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1631\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1632\u001b[0m )\n\u001b[0;32m-> 1633\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1634\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1635\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1636\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1637\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1638\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/hugging/lib/python3.10/site-packages/transformers/trainer.py:1902\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1900\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1901\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1902\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1904\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1905\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1906\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1907\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1908\u001b[0m ):\n\u001b[1;32m   1909\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1910\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/hugging/lib/python3.10/site-packages/transformers/trainer.py:2663\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2661\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39mbackward(loss)\n\u001b[1;32m   2662\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2663\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m   2665\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/miniconda3/envs/hugging/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/hugging/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "from main import parse_settings, get_wandb_token\n",
    "\n",
    "# %env WANDB_PROJECT=t5-qg\n",
    "\n",
    "model_args, data_args, train_args = parse_settings()\n",
    "qg.train(train_args, data_args, get_wandb_token())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Answer Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "['What is one of the sub-genres of literature that takes many forms?', 'What is the most important feature of Historical Fiction?', 'Where is Historical Fiction set in the past?']\n",
      "What is one of the sub-genres of literature that takes many forms?\n",
      "What is the most important feature of Historical Fiction?\n",
      "Where is Historical Fiction set in the past?\n",
      "[{'question': 'What is one of the sub-genres of literature that takes many forms?', 'answer': 'Historical Fiction'}, {'question': 'What is the most important feature of Historical Fiction?', 'answer': \"it's set in the past\"}]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "from transformers import pipeline\n",
    "question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n",
    "\n",
    "question_answers = []\n",
    "\n",
    "print(questions[\"questions\"])\n",
    "\n",
    "for question in questions[\"questions\"]:\n",
    "    print(question)\n",
    "    result = question_answerer(question=question, context=input_text)\n",
    "    if result[\"score\"] > 0.5:\n",
    "        question_answers.append({ \"question\": question, \"answer\": result[\"answer\"] })\n",
    "        # question_answers.append({ \"question\": question, \"answer\": result[\"answer\"], \"score\": result[\"score\"] })\n",
    "\n",
    "print(question_answers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Output to Anki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output format: Front, Back\n",
    "# Front: Question\n",
    "# Back: Answer\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(question_answers, columns=[\"question\", \"answer\"])\n",
    "df.to_csv(\"anki-output.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae2ba95b8bc8fbec5d0ccff067e6f5791542bd91f96bfdb3bb5972317be5ea2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
