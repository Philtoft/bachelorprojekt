{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QGAR - A Flashcard Generating NLP Model\n",
    "\n",
    "This notebook shows how to load and use the `QGAR` model.\n",
    "\n",
    "Please read the [README](./readme.md) before continueing!\n",
    "\n",
    "**Table of Contents:**\n",
    "1. [Load QGAR](#load-qgar)\n",
    "2. [Download and Preprocess SQuAD Dataset](#download-and-preprocess-squad-dataset)\n",
    "3. [Run QGAR](#run-qgar)\n",
    "4. [Train QGAR](#4-train-qgar)\n",
    "\n",
    "</br>\n",
    "\n",
    "---\n",
    "\n",
    "</br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Used Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "%pip install transformers\n",
    "%pip install datasets\n",
    "%pip install wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load QGAR\n",
    "First, we must load the `QGAR` model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from huggingface_hub import login\n",
    "from main import load_model_and_tokenizer, get_hg_token\n",
    "\n",
    "login(get_hg_token(), add_to_git_credential=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, tokenizer = load_model_and_tokenizer(\"t5-small\", device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download and Preprocess SQuAD Dataset\n",
    "\n",
    "First, we download and preprocess the modified `SQuAD` dataset, adding separator (`<sep>`) and end of sequence tokens (`<\\s>`) to each entry.\n",
    "The preprocessed file is split in two datasets, `training` and `validation`, and the sets are saved in the `data` directory in `PyTorch` format under the names:\n",
    "* [training_data.pt](./data/training_data.pt)\n",
    "* [validation_data.pt](./data/validation_data.pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.preprocessor import Preprocessor\n",
    "\n",
    "preprocessor = Preprocessor(model, tokenizer)\n",
    "preprocessor.preprocess_dataset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run QGAR\n",
    "Next, we import `QGAR` and set up a pipeline.\n",
    "Now, we can simply pass a context to the model to generate questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "from models.qg import QG\n",
    "import json\n",
    "\n",
    "qt = QG(\"the-coorporation/t5-qgar\", \"t5-small\")\n",
    "questions = qt(\"Historical Fiction is one of those sub-genres of literature that takes many forms. It's most important feature, though, is that it's set in the past, with every element of the story conforming to the norms of the day. Here's how we define Historical Fiction, a look at its origins, and some popular types.\")\n",
    "print(json.dumps(questions, indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train QGAR\n",
    "To train `QGAR`, we first parse `settings.json` to get the training arguments.\n",
    "\n",
    "We then make an instance of `QGARTrainer` and call `train` which will train the model and push it to `The Coorporation`'s Huggingface Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from main import parse_settings\n",
    "from models.qg import QG\n",
    "\n",
    "%env WANDB_PROJECT=t5-qg\n",
    "\n",
    "model_args, data_args, train_args = parse_settings()\n",
    "\n",
    "with open(\"./.local/wandb_token.txt\", \"r\") as f:\n",
    "    key = f.read()\n",
    "\n",
    "qt = QG(\"t5-small\", \"t5-small\")\n",
    "qt.train(train_args, data_args, key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Answer Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "from transformers import pipeline\n",
    "question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n",
    "\n",
    "question_answers = []\n",
    "\n",
    "print(questions)\n",
    "\n",
    "for question in questions:\n",
    "    result = question_answerer(question=question, context=input_text)\n",
    "    question_answers.append({ \"question\": question, \"answer\": result[\"answer\"], \"score\": result[\"score\"] })\n",
    "\n",
    "print(question_answers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae2ba95b8bc8fbec5d0ccff067e6f5791542bd91f96bfdb3bb5972317be5ea2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
