{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QG - A Question Generating NLP Model\n",
    "\n",
    "This notebook shows how to load and use the `QG` model.\n",
    "\n",
    "Please read the [README](./README.md) before continuing!\n",
    "\n",
    "---\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "0. [Dependencies](#0-dependencies)\n",
    "1. [Load QG](#1-load-qg)\n",
    "2. [Train QG](#2-train-qg)\n",
    "3. [Run QG](#3-run-qg)\n",
    "4. [Answer Generator](#4-answer-generator)\n",
    "5. [Output to Anki](#5-output-to-anki)\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Dependencies\n",
    "\n",
    "The project uses the following dependencies.\n",
    "\n",
    "Make sure to install them in your `Virtual Environment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -q\n",
    "%pip install transformers -q\n",
    "%pip install datasets -q\n",
    "%pip install wandb -q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load QG\n",
    "\n",
    "To load our `Question Generation` model, we use the `QG` class from the `models` module.\n",
    "\n",
    "From the model, we can extract the actual loaded `model` and `tokenizer` via its fields."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philiphyltoft/miniconda3/envs/hg_training/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> main
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from models.qg import QG\n",
    "\n",
    "qg = QG(\"t5-small\", \"t5-small\")\n",
    "model = qg._model\n",
    "tokenizer = qg._tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train QG\n",
    "\n",
    "To train `QG`, we first parse the `settings.json` file to get the `TrainingArguments` and `DataTrainingArguments`.\n",
    "\n",
    "We can then call `train` on the `QG` instance, which will train the model and push it to `The Coorporation` organization on Huggingface Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from main import get_local_file\n",
    "from models.qg import QG\n",
    "from parsing.settings_parser import parse_settings\n",
    "\n",
    "qg = QG(\"t5-small\", \"t5-small\")\n",
    "\n",
    "_, data_args, train_args = parse_settings()\n",
    "qg.train(train_args, data_args, get_local_file(\"wandb_token.txt\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run QG\n",
    "\n",
    "To run the `QG` model on some example `context`, we use the `__call__` method of the class.\n",
    "\n",
    "This will return a dictionary containing the `context` and a list of `questions` generated for the provided `context`."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "{\n",
      "    \"context\": \"Historical Fiction is one of those sub-genres of literature that takes many forms. It's most important feature, though, is that it's set in the past, with every element of the story conforming to the norms of the day. Here's how we define Historical Fiction, a look at its origins, and some popular types.\",\n",
      "    \"questions\": [\n",
      "        \"What is one of the sub-genres of literature that takes many forms?\",\n",
      "        \"What is the most important feature of Historical Fiction?\",\n",
      "        \"Where is Historical Fiction set in the past?\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> main
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from models.qg import QG\n",
    "import json\n",
    "\n",
    "qg = QG(\"the-coorporation/t5-small-qg\", \"the-coorporation/t5-small-qg\")\n",
    "context = \"Historical Fiction is one of those sub-genres of literature that takes many forms. It's most important feature, though, is that it's set in the past, with every element of the story conforming to the norms of the day. Here's how we define Historical Fiction, a look at its origins, and some popular types.\"\n",
    "\n",
    "questions = qg(context)\n",
    "print(json.dumps(questions, indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Answer Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "from transformers import pipeline\n",
    "question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n",
    "\n",
    "question_answers = []\n",
    "\n",
    "print(questions[\"questions\"])\n",
    "\n",
    "for question in questions[\"questions\"]:\n",
    "    print(question)\n",
    "    result = question_answerer(question=question, context=context)\n",
    "    if result[\"score\"] > 0.5:\n",
    "        question_answers.append({ \"question\": question, \"answer\": result[\"answer\"] })\n",
    "        # question_answers.append({ \"question\": question, \"answer\": result[\"answer\"], \"score\": result[\"score\"] })\n",
    "\n",
    "print(question_answers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Output to Anki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output format: Front, Back\n",
    "# Front: Question\n",
    "# Back: Answer\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(question_answers, columns=[\"question\", \"answer\"])\n",
    "df.to_csv(\"anki-output.csv\", index=False, header=False)"
   ]
<<<<<<< HEAD
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install evaluate -q\n",
    "%pip install scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 4.31k/4.31k [00:00<00:00, 1.80MB/s]\n",
      "Downloading metadata: 100%|██████████| 2.17k/2.17k [00:00<00:00, 878kB/s]\n",
      "Downloading readme: 100%|██████████| 7.59k/7.59k [00:00<00:00, 2.29MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imdb/plain_text to /Users/philiphyltoft/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 84.1M/84.1M [02:44<00:00, 513kB/s]   \n",
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imdb downloaded and prepared to /Users/philiphyltoft/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /Users/philiphyltoft/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Thu Feb 23 13:33:23 2023) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m pipe \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39m\u001b[39mtext-classification\u001b[39m\u001b[39m\"\u001b[39m, model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlvwerra/distilbert-imdb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m data \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39m\u001b[39mimdb\u001b[39m\u001b[39m\"\u001b[39m, split\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mshuffle()\u001b[39m.\u001b[39mselect(\u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m))\n\u001b[0;32m----> 8\u001b[0m metric \u001b[39m=\u001b[39m evaluate\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/hg_training/lib/python3.10/site-packages/evaluate/loading.py:734\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, config_name, module_type, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, **init_kwargs)\u001b[0m\n\u001b[1;32m    730\u001b[0m download_mode \u001b[39m=\u001b[39m DownloadMode(download_mode \u001b[39mor\u001b[39;00m DownloadMode\u001b[39m.\u001b[39mREUSE_DATASET_IF_EXISTS)\n\u001b[1;32m    731\u001b[0m evaluation_module \u001b[39m=\u001b[39m evaluation_module_factory(\n\u001b[1;32m    732\u001b[0m     path, module_type\u001b[39m=\u001b[39mmodule_type, revision\u001b[39m=\u001b[39mrevision, download_config\u001b[39m=\u001b[39mdownload_config, download_mode\u001b[39m=\u001b[39mdownload_mode\n\u001b[1;32m    733\u001b[0m )\n\u001b[0;32m--> 734\u001b[0m evaluation_cls \u001b[39m=\u001b[39m import_main_class(evaluation_module\u001b[39m.\u001b[39;49mmodule_path)\n\u001b[1;32m    735\u001b[0m evaluation_instance \u001b[39m=\u001b[39m evaluation_cls(\n\u001b[1;32m    736\u001b[0m     config_name\u001b[39m=\u001b[39mconfig_name,\n\u001b[1;32m    737\u001b[0m     process_id\u001b[39m=\u001b[39mprocess_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minit_kwargs,\n\u001b[1;32m    744\u001b[0m )\n\u001b[1;32m    746\u001b[0m \u001b[39mif\u001b[39;00m module_type \u001b[39mand\u001b[39;00m module_type \u001b[39m!=\u001b[39m evaluation_instance\u001b[39m.\u001b[39mmodule_type:\n",
      "File \u001b[0;32m~/miniconda3/envs/hg_training/lib/python3.10/site-packages/evaluate/loading.py:77\u001b[0m, in \u001b[0;36mimport_main_class\u001b[0;34m(module_path)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimport_main_class\u001b[39m(module_path) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Union[Type[DatasetBuilder], Type[EvaluationModule]]]:\n\u001b[1;32m     76\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Import a module at module_path and return its main class, a Metric by default\"\"\"\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     module \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(module_path)\n\u001b[1;32m     78\u001b[0m     main_cls_type \u001b[39m=\u001b[39m EvaluationModule\n\u001b[1;32m     80\u001b[0m     \u001b[39m# Find the main class in our imported module\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hg_training/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14/accuracy.py:17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m\"\"\"Accuracy metric.\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdatasets\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n\u001b[1;32m     19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mevaluate\u001b[39;00m\n\u001b[1;32m     22\u001b[0m _DESCRIPTION \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[39mAccuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[39mAccuracy = (TP + TN) / (TP + TN + FP + FN)\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mFN: False negative\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[39m\"\"\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from evaluate import evaluator\n",
    "import evaluate\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"lvwerra/distilbert-imdb\")\n",
    "data = load_dataset(\"imdb\", split=\"test\").shuffle().select(range(1000))\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
=======
>>>>>>> main
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "5f5ac68a19970ac19728ecfdb27ce90af7d423a161df21ca370564cfbde878e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
