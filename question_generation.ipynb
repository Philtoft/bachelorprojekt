{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QGAR - A Flashcard Generating NLP Model\n",
    "\n",
    "This notebook shows how to load and use the `QGAR` model.\n",
    "\n",
    "Please read the [README](./README.md) before continuing!\n",
    "\n",
    "**Table of Contents:**\n",
    "\n",
    "0. [Dependencies](#0-dependencies)\n",
    "1. [Load QG](#1-load-qg)\n",
    "2. [Download and Preprocess SQuAD Dataset](#2-download-and-preprocess-squad-dataset)\n",
    "3. [Run QG](#3-run-qg)\n",
    "4. [Train QG](#4-train-qg)\n",
    "5. [Answer Generator](#5-answer-generator)\n",
    "6. [Output to Anki](#6-output-to-anki)\n",
    "\n",
    "</br>\n",
    "\n",
    "---\n",
    "\n",
    "</br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Dependencies\n",
    "\n",
    "The project uses the following dependencies.\n",
    "\n",
    "Make sure to install them in your `Virtual Environment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -q\n",
    "%pip install transformers -q\n",
    "%pip install datasets -q\n",
    "%pip install wandb -q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load QG\n",
    "\n",
    "To load our `Question Generator` model, we use the `QG class` from the `models` module.\n",
    "\n",
    "From the model, we can extract the actual loaded `model` and `tokenizer` via its fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from models.qg import QG\n",
    "\n",
    "qg = QG(\"t5-small\", \"t5-small\")\n",
    "model = qg._model\n",
    "tokenizer = qg._tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download and Preprocess SQuAD Dataset\n",
    "\n",
    "If we want to train the `QG` model, we must download and preprocess our modified `SQuAD 2.0` dataset from `Hugging Face`.\n",
    "\n",
    "This is done with the `SquadPreProcessor class` from the `Preprocessing` module.\n",
    "\n",
    "The `SquadPreProcessor` will download and preprocess the modified `SQuAD` dataset by adding separator (`<sep>`), `end of sequence tokens` (`<\\s>`) and encode both `contexts` and `questions` for each entry in the dataset.\n",
    "\n",
    "The preprocessed dataset is then split in two datasets, `training` and `validation`. The sets can be saved in the specified `save_dir` directory in `PyTorch` format under the names:\n",
    "* `training_data.pt`\n",
    "* `validation_data.pt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from preprocessing.squad_preprocessor import SquadPreprocessor\n",
    "\n",
    "preprocessor = SquadPreprocessor(tokenizer)\n",
    "train, validation = preprocessor.preprocess(\"the-coorporation/the_squad_v2\", \"data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run QG\n",
    "\n",
    "To run the `QG` model on some example `context`, we use the `__call__` method of the class.\n",
    "\n",
    "This will return a dictionary containing the `context` and a list of `questions` generated for the provided `context`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "\n",
    "qg = QG(\"t5-small\", \"t5-small\")\n",
    "context = \"Historical Fiction is one of those sub-genres of literature that takes many forms. It's most important feature, though, is that it's set in the past, with every element of the story conforming to the norms of the day. Here's how we define Historical Fiction, a look at its origins, and some popular types.\"\n",
    "\n",
    "questions = qg(context)\n",
    "print(json.dumps(questions, indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train QG\n",
    "\n",
    "To train `QG`, we first parse the `settings.json` file to get the `TrainingArguments` and `DataTrainingArguments`.\n",
    "\n",
    "We can then call `train` on the `QG` instance, which will train the model and push it to `The Coorporation`'s Huggingface Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from main import get_local_file\n",
    "from parsing.settings_parser import parse_settings\n",
    "\n",
    "qg = QG(\"t5-small\", \"t5-small\")\n",
    "\n",
    "_, data_args, train_args = parse_settings()\n",
    "qg.train(train_args, data_args, get_local_file(\"wandb_token.txt\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Answer Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "from transformers import pipeline\n",
    "question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n",
    "\n",
    "question_answers = []\n",
    "\n",
    "print(questions[\"questions\"])\n",
    "\n",
    "for question in questions[\"questions\"]:\n",
    "    print(question)\n",
    "    result = question_answerer(question=question, context=context)\n",
    "    if result[\"score\"] > 0.5:\n",
    "        question_answers.append({ \"question\": question, \"answer\": result[\"answer\"] })\n",
    "        # question_answers.append({ \"question\": question, \"answer\": result[\"answer\"], \"score\": result[\"score\"] })\n",
    "\n",
    "print(question_answers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Output to Anki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output format: Front, Back\n",
    "# Front: Question\n",
    "# Back: Answer\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(question_answers, columns=[\"question\", \"answer\"])\n",
    "df.to_csv(\"anki-output.csv\", index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae2ba95b8bc8fbec5d0ccff067e6f5791542bd91f96bfdb3bb5972317be5ea2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
