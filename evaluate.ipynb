{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from evaluate.module import EvaluationModule\n",
    "from datasets import load_dataset, Dataset\n",
    "from models.qg import QG\n",
    "import torch\n",
    "\n",
    "qg = QG(\"the-coorporation/t5-small-qg-2.0\", \"the-coorporation/t5-small-qg-2.0\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu: EvaluationModule = load(\"bleu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METEOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/philiphyltoft/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/philiphyltoft/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/philiphyltoft/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "meteor: EvaluationModule = load(\"meteor\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROGUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: the_squad_qg/v2\n",
      "Found cached dataset the_squad_qg (/Users/philiphyltoft/.cache/huggingface/datasets/the-coorporation___the_squad_qg/v2/2.0.0/cd5e3386e9e3124c24114539fb5124093fa58d99c1a7292451b8801a94a2aca5)\n",
      "100%|██████████| 2/2 [00:00<00:00, 353.86it/s]\n"
     ]
    }
   ],
   "source": [
    "rouge: EvaluationModule = load(\"rouge\")\n",
    "\n",
    "squad: Dataset = load_dataset(\"the-coorporation/the_squad_qg\")\n",
    "validation = squad['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "references: list[dict] = []\n",
    "\n",
    "for i, entry in enumerate(validation):\n",
    "    if i == 5:\n",
    "        break\n",
    "    \n",
    "    context: str = entry['context']\n",
    "    questions: list[str] = [question.strip() for question in entry['questions'].split(\"{sep_token}\") if len(question.strip()) > 0 ]\n",
    "    \n",
    "    reference = {\n",
    "        \"context\": context,\n",
    "        \"questions\": questions\n",
    "    }\n",
    "    \n",
    "    references.append(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions: list[dict] = []\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "generator_args = {\n",
    "            \"max_length\": 512,\n",
    "            \"num_beams\": 4,\n",
    "            \"length_penalty\": 1.5,\n",
    "            \"no_repeat_ngram_size\": 3,\n",
    "            \"early_stopping\": True,\n",
    "        }\n",
    "\n",
    "for i, entry in enumerate(validation):\n",
    "    if i == 5:\n",
    "        break\n",
    "    \n",
    "    context: str = entry['context']\n",
    "    input_string = \"generate questions: \" + context + \" </s>\"\n",
    "\n",
    "    input_ids = qg._tokenizer.encode(input_string, return_tensors=\"pt\").to(device)\n",
    "    result = qg._model.generate(input_ids, **generator_args).to(device)\n",
    "    questions = qg._tokenizer.decode(result[0], skip_special_tokens=True)\n",
    "\n",
    "    questions: list[str] = [question.strip() for question in questions.split(\"<sep>\") if len(question.strip()) > 0 ]\n",
    "\n",
    "    prediction = {\n",
    "        \"context\": context,\n",
    "        \"questions\": questions\n",
    "    }\n",
    "\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mismatch in the number of predictions (19) and references (23)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m p \u001b[39m=\u001b[39m [question \u001b[39mfor\u001b[39;00m prediction \u001b[39min\u001b[39;00m predictions \u001b[39mfor\u001b[39;00m question \u001b[39min\u001b[39;00m prediction[\u001b[39m'\u001b[39m\u001b[39mquestions\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m      8\u001b[0m r \u001b[39m=\u001b[39m [question \u001b[39mfor\u001b[39;00m reference \u001b[39min\u001b[39;00m references \u001b[39mfor\u001b[39;00m question \u001b[39min\u001b[39;00m reference[\u001b[39m'\u001b[39m\u001b[39mquestions\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m----> 9\u001b[0m rouge\u001b[39m.\u001b[39;49mcompute(references\u001b[39m=\u001b[39;49mr, predictions\u001b[39m=\u001b[39;49mp)\n",
      "File \u001b[0;32m~/miniconda3/envs/hg_training/lib/python3.10/site-packages/evaluate/module.py:432\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m compute_kwargs \u001b[39m=\u001b[39m {k: kwargs[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m kwargs \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feature_names()}\n\u001b[1;32m    431\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m--> 432\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_batch(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m    433\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_finalize()\n\u001b[1;32m    435\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_file_name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hg_training/lib/python3.10/site-packages/evaluate/module.py:512\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     error_msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    507\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPredictions and/or references don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt match the expected format.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected format: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselected_feature_format\u001b[39m \u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    509\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput predictions: \u001b[39m\u001b[39m{\u001b[39;00msummarize_if_long_list(predictions)\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    510\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput references: \u001b[39m\u001b[39m{\u001b[39;00msummarize_if_long_list(references)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    511\u001b[0m     )\n\u001b[0;32m--> 512\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(error_msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Mismatch in the number of predictions (19) and references (23)"
     ]
    }
   ],
   "source": [
    "# p = predictions\n",
    "# r = references\n",
    "# flat_list = [item for sublist in nested_list for item in sublist]\n",
    "# p = [question for questions in predictions['questions'] for question in questions]\n",
    "# p = [questions for questions in predictions['questions'] for prediction in predictions]\n",
    "# p = [question for question in predictions['questions']]\n",
    "p = [question for prediction in predictions for question in prediction['questions']]\n",
    "r = [question for reference in references for question in reference['questions']]\n",
    "rouge.compute(references=r, predictions=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hg_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
